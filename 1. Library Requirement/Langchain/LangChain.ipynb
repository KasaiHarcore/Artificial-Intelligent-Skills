{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fa68b51",
   "metadata": {},
   "source": [
    "# What is LangChain?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2e489f",
   "metadata": {},
   "source": [
    "**LangChain** is a framework that helps you build AI applications using Large Language Models (LLMs) in a fast, stable and structured way.\n",
    "\n",
    "*In short*: LangChain = A toolkit that helps you connect LLMs with data + logic + external tools to create a complete AI App."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08fb9d8",
   "metadata": {},
   "source": [
    "# When to use LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c26b785",
   "metadata": {},
   "source": [
    "Large Language Models (LLMs), despite their impressive capabilities, fundamentally behave as **“text → text” functions**. By themselves, they cannot:\n",
    "- access external data,\n",
    "- store long-term memory,\n",
    "- execute multi-step workflows,\n",
    "- call tools or APIs,\n",
    "- maintain state, or\n",
    "- carry out reliable, complex logic.\n",
    "\n",
    "Engineering teams quickly discovered that pure LLM usage is enough for demos, but **insufficient for real products** such as enterprise chatbots, document-grounded Q&A systems, data-analysis assistants, or automated workflows.\n",
    "\n",
    "This gap is precisely where **LangChain** emerged: a framework designed to extend LLMs into practical, production-grade systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23804623",
   "metadata": {},
   "source": [
    "### Connecting LLMs to External Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ce282c",
   "metadata": {},
   "source": [
    "LangChain acts as a middleware layer between the model and the real world. It allows LLMs to read:\n",
    "- documents\n",
    "- databases\n",
    "- APIs\n",
    "- vector stores\n",
    "- computational tools\n",
    "\n",
    "This capability is essential for *RAG (Retrieval-Augmented Generation)*, where applications must provide accurate, up-to-date, contextual information instead of relying on the model’s internal guesses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df7bab6",
   "metadata": {},
   "source": [
    "### Multi-Step Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3fec67",
   "metadata": {},
   "source": [
    "LLMs do not inherently understand step-by-step procedures or stateful tasks. LangChain provides:\n",
    "- Chains for simple multi-step logic\n",
    "- LangGraph for complex workflows such as:\n",
    "    - branching\n",
    "    - looping\n",
    "    - retrying\n",
    "    - validation\n",
    "    - deterministic state machines\n",
    "\n",
    "These are the building blocks of AI pipelines, document processing systems, and multi-stage reasoning assistants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22e40b1",
   "metadata": {},
   "source": [
    "### Agents and Tool-Use Capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c2c62f",
   "metadata": {},
   "source": [
    "LLMs cannot decide on their own: when to use a tool, which tool to select, or how to integrate tool outputs.\n",
    "\n",
    "LangChain’s Agents introduce this capability. It can help the model interact with multiple type of tool or even creating an working environment that can simulate user computer.\n",
    "\n",
    "Whenever you want an AI assistant that can act, not just respond, agents become essential."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0744d099",
   "metadata": {},
   "source": [
    "# Getting Started with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4936e7ed",
   "metadata": {},
   "source": [
    "In this example, we will introduce LangChain, building a simple LLM-powered assistant. We'll provide examples for both OpenAI's `gpt-4o-mini` *and* Meta's `llama3.2` via Ollama!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ac99b0",
   "metadata": {},
   "source": [
    "### Initializing OpenAI's gpt-4o-mini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2211b9b9",
   "metadata": {},
   "source": [
    "We start by initializing our LLM. We will use OpenAI's `gpt-4o-mini` model, if you need an API key you can get one from [OpenAI's website](https://platform.openai.com/settings/organization/api-keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb7d4661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from os import getenv\n",
    "\n",
    "\n",
    "openai_model_auth = \"openai/gpt-4o-mini\"\n",
    "\n",
    "# Free one from OpenRouter\n",
    "openai_model = \"openai/gpt-oss-120b:free\"\n",
    "os.environ[\"OPENROUTER_API_KEY\"] = os.getenv(\"OPENROUTER_API_KEY\") or getpass(\n",
    "    \"Enter OpenRouter API Key: \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59eb3e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Working/WORKING/github/Artificial-Intelligent-Skills/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# For normal accurate responses\n",
    "llm = ChatOpenAI(temperature = 0.0, model = openai_model, api_key = getenv(\"OPENROUTER_API_KEY\"), base_url = \"https://openrouter.ai/api/v1\")\n",
    "\n",
    "# For unique creative responses\n",
    "creative_llm = ChatOpenAI(temperature = 0.9, model = openai_model, api_key = getenv(\"OPENROUTER_API_KEY\"), base_url = \"https://openrouter.ai/api/v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e7636f",
   "metadata": {},
   "source": [
    "We will be taking an `article` _draft_ and using LangChain to generate various useful items around this article. We'll be creating:\n",
    "\n",
    "1. An article title\n",
    "2. An article description\n",
    "3. Editor advice where we will insert an additional paragraph in the article\n",
    "4. A thumbnail / hero image for our article."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9ede9d",
   "metadata": {},
   "source": [
    "Here we input our article to start with. Currently this is using an article from the Aurelio AI learning page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5170cbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"\"\"\n",
    "\\\n",
    "We believe AI's short—to mid-term future belongs to agents and that the long-term future of *AGI* may evolve from agentic systems. Our definition of agents covers any neuro-symbolic system in which we merge neural AI (such as an LLM) with semi-traditional software.\n",
    "\n",
    "With agents, we allow LLMs to integrate with code — allowing AI to search the web, perform math, and essentially integrate into anything we can build with code. It should be clear the scope of use cases is phenomenal where AI can integrate with the broader world of software.\n",
    "\n",
    "In this introduction to AI agents, we will cover the essential concepts that make them what they are and why that will make them the core of real-world AI in the years to come.\n",
    "\n",
    "---\n",
    "\n",
    "## Neuro-Symbolic Systems\n",
    "\n",
    "Neuro-symbolic systems consist of both neural and symbolic computation, where:\n",
    "\n",
    "- Neural refers to LLMs, embedding models, or other neural network-based models.\n",
    "- Symbolic refers to logic containing symbolic logic, such as code.\n",
    "\n",
    "Both neural and symbolic AI originate from the early philosophical approaches to AI: connectionism (now neural) and symbolism. Symbolic AI is the more traditional AI. Diehard symbolists believed they could achieve true AGI via written rules, ontologies, and other logical functions.\n",
    "\n",
    "The other camp were the connectionists. Connectionism emerged in 1943 with a theoretical neural circuit but truly kicked off with Rosenblatt's perceptron paper in 1958 [1][2]. Both of these approaches to AI are fascinating but deserve more time than we can give them here, so we will leave further exploration of these concepts for a future chapter.\n",
    "\n",
    "Most important to us is understanding where symbolic logic outperforms neural-based compute and vice-versa.\n",
    "\n",
    "| Neural | Symbolic |\n",
    "| --- | --- |\n",
    "| Flexible, learned logic that can cover a huge range of potential scenarios. | Mostly hand-written rules which can be very granular and fine-tuned but hard to scale. |\n",
    "| Hard to interpret why a neural system does what it does. Very difficult or even impossible to predict behavior. | Rules are written and can be understood. When unsure why a particular ouput was produced we can look at the rules / logic to understand. |\n",
    "| Requires huge amount of data and compute to train state-of-the-art neural models, making it hard to add new abilities or update with new information. | Code is relatively cheap to write, it can be updated with new features easily, and latest information can often be added often instantaneously. |\n",
    "| When trained on broad datasets can often lack performance when exposed to unique scenarios that are not well represented in the training data. | Easily customized to unique scenarios. |\n",
    "| Struggles with complex computations such as mathematical operations. | Perform complex computations very quickly and accurately. |\n",
    "\n",
    "Pure neural architectures struggle with many seemingly simple tasks. For example, an LLM *cannot* provide an accurate answer if we ask it for today's date.\n",
    "\n",
    "Retrieval Augmented Generation (RAG) is commonly used to provide LLMs with up-to-date knowledge on a particular subject or access to proprietary knowledge.\n",
    "\n",
    "### Giving LLMs Superpowers\n",
    "\n",
    "By 2020, it was becoming clear that neural AI systems could not perform tasks symbolic systems typically excelled in, such as arithmetic, accessing structured DB data, or making API calls. These tasks require discrete input parameters that allow us to process them reliably according to strict written logic.\n",
    "\n",
    "In 2022, researchers at AI21 developed Jurassic-X, an LLM-based \"neuro-symbolic architecture.\" Neuro-symbolic refers to merging the \"neural computation\" of large language models (LLMs) with more traditional (i.e. symbolic) computation of code.\n",
    "\n",
    "Jurassic-X used the Modular Reasoning, Knowledge, and Language (MRKL) system [3]. The researchers developed MRKL to solve the limitations of LLMs, namely:\n",
    "\n",
    "- Lack of up-to-date knowledge, whether that is the latest in AI or something as simple as today's date.\n",
    "- Lack of proprietary knowledge, such as internal company docs or your calendar bookings.\n",
    "- Lack of reasoning, i.e. the inability to perform operations that traditional software is good at, like running complex mathematical operations.\n",
    "- Lack of ability to generalize. Back in 2022, most LLMs had to be fine-tuned to perform well in a specific domain. This problem is still present today but far less prominent as the SotA models generalize much better and, in the case of MRKL, are able to use tools relatively well (although we could certainly take the MRKL solution to improve tool use performance even today).\n",
    "\n",
    "MRKL represents one of the earliest forms of what we would now call an agent; it is an LLM (neural computation) paired with executable code (symbolic computation).\n",
    "\n",
    "## ReAct and Tools\n",
    "\n",
    "There is a misconception in the broader industry that an AI agent is an LLM contained within some looping logic that can generate inputs for and execute code functions. This definition of agents originates from the huge popularity of the ReAct agent framework and the adoption of a similar structure with function/tool calling by LLM providers such as OpenAI, Anthropic, and Ollama.\n",
    "\n",
    "![ReAct agent flow with the Reasoning-Action loop [4]. When the action chosen specifies to use a normal tool, the tool is used and the observation returned for another iteration through the Reasoning-Action loop. To return a final answer to the user the LLM must choose action \"answer\" and provide the natural language response, finishing the loop.](/images/posts/ai-agents/ai-agents-00.png)\n",
    "\n",
    "<small>ReAct agent flow with the Reasoning-Action loop [4]. When the action chosen specifies to use a normal tool, the tool is used and the observation returned for another iteration through the Reasoning-Action loop. To return a final answer to the user the LLM must choose action \"answer\" and provide the natural language response, finishing the loop.</small>\n",
    "\n",
    "Our \"neuro-symbolic\" definition is much broader but certainly does include ReAct agents and LLMs paired with tools. This agent type is the most common for now, so it's worth understanding the basic concept behind it.\n",
    "\n",
    "The **Re**ason **Act**ion (ReAct) method encourages LLMs to generate iterative *reasoning* and *action* steps. During *reasoning,* the LLM describes what steps are to be taken to answer the user's query. Then, the LLM generates an *action,* which we parse into an input to some executable code, which we typically describe as a tool/function call.\n",
    "\n",
    "![ReAct method. Each iteration includes a Reasoning step followed by an Action (tool call) step. The Observation is the output from the previous tool call. During the final iteration the agent calls the answer tool, meaning we generate the final answer for the user.](/images/posts/ai-agents/ai-agents-01.png)\n",
    "\n",
    "<small>ReAct method. Each iteration includes a Reasoning step followed by an Action (tool call) step. The Observation is the output from the previous tool call. During the final iteration the agent calls the answer tool, meaning we generate the final answer for the user.</small>\n",
    "\n",
    "Following the reason and action steps, our action tool call returns an observation. The logic returns the observation to the LLM, which is then used to generate subsequent reasoning and action steps.\n",
    "\n",
    "The ReAct loop continues until the LLM has enough information to answer the original input. Once the LLM reaches this state, it calls a special *answer* action with the generated answer for the user.\n",
    "\n",
    "## Not only LLMs and Tool Calls\n",
    "\n",
    "LLMs paired with tool calling are powerful but far from the only approach to building agents. Using the definition of neuro-symbolic, we cover architectures such as:\n",
    "\n",
    "- Multi-agent workflows that involve multiple LLM-tool (or other agent structure) combinations.\n",
    "- More deterministic workflows where we may have set neural model-tool paths that may fork or merge as the use case requires.\n",
    "- Embedding models that can detect user intents and decide tool-use or LLM selection-based selection in vector space.\n",
    "\n",
    "These are just a few high-level examples of alternative agent structures. Far from being designed for niche use cases, we find these alternative options to frequently perform better than the more common ReAct or Tool agents. We will cover all of these examples and more in future chapters.\n",
    "\n",
    "---\n",
    "\n",
    "Agents are fundamental to the future of AI, but that doesn't mean we should expect that future to come from agents in their most popular form today. ReAct and Tool agents are great and handle many simple use cases well, but the scope of agents is much broader, and we believe thinking beyond ReAct and Tools is key to building future AI.\n",
    "\n",
    "---\n",
    "\n",
    "You can sign up for the [Aurelio AI newsletter](https://b0fcw9ec53w.typeform.com/to/w2BDHVK7) to stay updated on future releases in our comprehensive course on agents.\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "[1] The curious case of Connectionism (2019) [https://www.degruyter.com/document/doi/10.1515/opphil-2019-0018/html](https://www.degruyter.com/document/doi/10.1515/opphil-2019-0018/html)\n",
    "\n",
    "[2] F. Rosenblatt, [The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain](https://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf) (1958), Psychological Review\n",
    "\n",
    "[3] E. Karpas et al. [MRKL Systems: A Modular, Neuro-Symbolic Architecture That Combines Large Language Models, External Knowledge Sources and Discrete Reasoning](https://arxiv.org/abs/2205.00445) (2022), AI21 Labs\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680ef2c4",
   "metadata": {},
   "source": [
    "### Preparing our Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff10c31",
   "metadata": {},
   "source": [
    "LangChain comes with several prompt classes and methods for organizing or constructing our prompts. We will cover these in more detail in later examples, but for now we'll cover the essentials that we need here.\n",
    "\n",
    "Prompts for chat agents are at a minimum broken up into three components, those are:\n",
    "\n",
    "* System prompt: this provides the instructions to our LLM on how it must behave, what it's objective is, etc.\n",
    "\n",
    "* User prompt: this is a user written input.\n",
    "\n",
    "* AI prompt: this is the AI generated output. When representing a conversation, previous generations will be inserted back into the next prompt and become part of the broader _chat history_.\n",
    "\n",
    "```\n",
    "You are a helpful AI assistant, you will do XYZ.    | SYSTEM PROMPT\n",
    "\n",
    "User: Hi, what is the capital of Australia?         | USER PROMPT\n",
    "AI: It is Canberra                                  | AI PROMPT\n",
    "User: When is the best time to visit?               | USER PROMPT\n",
    "```\n",
    "\n",
    "LangChain provides us with _templates_ for each of these prompt types. By using templates we can insert different inputs to the template, modifying the prompt based on the provided inputs.\n",
    "\n",
    "Let's initialize our system and user prompt first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c68a4bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "\n",
    "# Defining the system prompt (how the AI should act)\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are an AI assistant that helps generate article titles.\"\n",
    ")\n",
    "\n",
    "# the user prompt is provided by the user, in this case however the only dynamic\n",
    "# input is the article\n",
    "user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"\"\"You are tasked with creating a name for a article.\n",
    "The article is here for you to examine: \n",
    "\n",
    "---\n",
    "\n",
    "{article}\n",
    "\n",
    "---\n",
    "\n",
    "The name should be based of the context of the article.\n",
    "Be creative, but make sure the names are clear, catchy,\n",
    "and relevant to the theme of the article.\n",
    "\n",
    "Only output the article name, no other explanation or\n",
    "text can be provided.\"\"\",\n",
    "    input_variables = [\"article\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d10a8bb",
   "metadata": {},
   "source": [
    "We can display what our formatted human prompt would look like after inserting a value into the `article` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7d03bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessage(content='You are tasked with creating a name for a article.\\nThe article is here for you to examine: \\n\\n---\\n\\nTEST STRING\\n\\n---\\n\\nThe name should be based of the context of the article.\\nBe creative, but make sure the names are clear, catchy,\\nand relevant to the theme of the article.\\n\\nOnly output the article name, no other explanation or\\ntext can be provided.', additional_kwargs={}, response_metadata={})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_prompt.format(article = \"TEST STRING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a217922",
   "metadata": {},
   "source": [
    "We have our system and user prompts, we can merge both into our full chat prompt using the `ChatPromptTemplate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15800c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_prompt = ChatPromptTemplate.from_messages([system_prompt, user_prompt])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747d49ad",
   "metadata": {},
   "source": [
    "By default, the `ChatPromptTemplate` will read the `input_variables` from each of the prompt templates inserted and allow us to use those input variables when formatting the full chat prompt template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a01aaed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are an AI assistant that helps generate article titles.\n",
      "Human: You are tasked with creating a name for a article.\n",
      "The article is here for you to examine: \n",
      "\n",
      "---\n",
      "\n",
      "TEST STRING\n",
      "\n",
      "---\n",
      "\n",
      "The name should be based of the context of the article.\n",
      "Be creative, but make sure the names are clear, catchy,\n",
      "and relevant to the theme of the article.\n",
      "\n",
      "Only output the article name, no other explanation or\n",
      "text can be provided.\n"
     ]
    }
   ],
   "source": [
    "print(first_prompt.format(article = \"TEST STRING\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10acd0d",
   "metadata": {},
   "source": [
    "`ChatPromptTemplate` also prefixes each individual message with it's role, ie `System:`, `Human:`, or `AI:`.\n",
    "\n",
    "We can chain together our `first_prompt` template and the `llm` object we defined earlier to create a simple LLM chain. This chain will perform the steps **prompt formatting > llm generation > get output**.\n",
    "\n",
    "We'll be using **L**ang**C**hain **E**xpression **L**anguage (LCEL) to construct our chain. This syntax can look a little strange but we will cover it in detail later in the course. For now, all we need to know is that we define our inputs with the first dictionary segment (ie `{\"article\": lambda x: x[\"article\"]}`) and then we use the pipe operator (`|`) to say that the output from the left of the pipe will be fed into the input to the right of the pipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8822e502",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_one = (\n",
    "    {\"article\": lambda x: x[\"article\"]}\n",
    "    | first_prompt\n",
    "    | creative_llm\n",
    "    | {\"article_title\": lambda x: x.content}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3716bd1a",
   "metadata": {},
   "source": [
    "Our first chain creates the article title, note: we can run all of these individually..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45b8dcb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article_title': 'Neuro‑Symbolic Agents: Building the Next Generation of AI Intelligence'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_title_msg = chain_one.invoke({\"article\": article})\n",
    "article_title_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03f59ef",
   "metadata": {},
   "source": [
    "But we will actually chain this step with multiple other `LLMChain` steps. So, to continue, our next step is to summarize the article using both the `article` and newly generated `article_title` values, from which we will output a new `summary` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "776a08ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"\"\"You are tasked with creating a description for\n",
    "the article. The article is here for you to examine:\n",
    "\n",
    "---\n",
    "\n",
    "{article}\n",
    "\n",
    "---\n",
    "\n",
    "Here is the article title '{article_title}'.\n",
    "\n",
    "Output the SEO friendly article description. Do not output\n",
    "anything other than the description.\"\"\",\n",
    "    input_variables = [\"article\", \"article_title\"]\n",
    ")\n",
    "\n",
    "second_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_prompt,\n",
    "    second_user_prompt\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3409776d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_two = (\n",
    "    {\n",
    "        \"article\": lambda x: x[\"article\"],\n",
    "        \"article_title\": lambda x: x[\"article_title\"]\n",
    "    }\n",
    "    | second_prompt\n",
    "    | llm\n",
    "    | {\"summary\": lambda x: x.content}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb153043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'Discover how neuro‑symbolic agents combine powerful LLMs with traditional code to create versatile, up‑to‑date AI systems. Learn the fundamentals of neuro‑symbolic architecture, the ReAct reasoning‑action loop, tool‑calling techniques, and emerging multi‑agent workflows that are shaping the future of real‑world AI. Perfect for developers, researchers, and AI enthusiasts eager to build the next generation of intelligent agents.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_description_msg = chain_two.invoke({\n",
    "    \"article\": article,\n",
    "    \"article_title\": article_title_msg[\"article_title\"]\n",
    "})\n",
    "article_description_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddd4bfc",
   "metadata": {},
   "source": [
    "The third step will consume our first `article` variable and provide several output fields, focusing on helping the user improve a part of their writing. As we are outputting multiple fields we can specify for the LLM to use structured outputs, keeping the generated fields aligned with our requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "042c0a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "third_user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"\"\"You are tasked with creating a new paragraph for the\n",
    "article. The article is here for you to examine:\n",
    "\n",
    "---\n",
    "\n",
    "{article}\n",
    "\n",
    "---\n",
    "\n",
    "Choose one paragraph to review and edit. During your edit\n",
    "ensure you provide constructive feedback to the user so they\n",
    "can learn where to improve their own writing.\n",
    "\n",
    "Return your response as a JSON object with the following fields:\n",
    "- original_paragraph: The original paragraph you selected\n",
    "- edited_paragraph: Your improved version of the paragraph\n",
    "- feedback: Constructive feedback explaining the changes\"\"\",\n",
    "    input_variables = [\"article\"]\n",
    ")\n",
    "\n",
    "# prompt template 3: creating a new paragraph for the article\n",
    "third_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_prompt,\n",
    "    third_user_prompt\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa117b8e",
   "metadata": {},
   "source": [
    "We create a pydantic object describing the output format we need. This format description is then passed to our model using the `with_structured_output` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7069960d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Paragraph(BaseModel):\n",
    "    original_paragraph: str = Field(description = \"The original paragraph\")\n",
    "    edited_paragraph: str = Field(description = \"The improved edited paragraph\")\n",
    "    feedback: str = Field(description = (\n",
    "        \"Constructive feedback on the original paragraph\"\n",
    "    ))\n",
    "\n",
    "structured_llm = creative_llm.with_structured_output(Paragraph, method = \"json_schema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a2c673",
   "metadata": {},
   "source": [
    "Now we put all of this together in another chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7339359b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain 3: inputs: article / output: article_para\n",
    "chain_three = (\n",
    "    {\"article\": lambda x: x[\"article\"]}\n",
    "    | third_prompt\n",
    "    | structured_llm\n",
    "    | {\n",
    "        \"original_paragraph\": lambda x: x.original_paragraph,\n",
    "        \"edited_paragraph\": lambda x: x.edited_paragraph,\n",
    "        \"feedback\": lambda x: x.feedback\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4468448f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original_paragraph': \"Pure neural architectures struggle with many seemingly simple tasks. For example, an LLM *cannot* provide an accurate answer if we ask it for today's date.\",\n",
       " 'edited_paragraph': 'Pure neural architectures often falter on tasks that appear trivial to humans. For instance, a vanilla LLM will typically answer \"I don\\'t know\" or generate an outdated date when asked for the current day, because it lacks direct access to real‑time information and relies solely on patterns learned during training.',\n",
       " 'feedback': '### What was improved\\n1. **Specificity** – The revised sentence clarifies *why* the model fails (no real‑time access, reliance on training data) rather than just stating it \"cannot\". This gives readers a concrete understanding of the limitation.\\n2. **Tone and Formality** – Replacing the informal asterisk emphasis with a more academic phrasing keeps the article’s voice consistent.\\n3. **Clarity** – Adding \"vanilla\" and \"real‑time information\" distinguishes between a base LLM and one that might be augmented with retrieval mechanisms, which aligns with the surrounding discussion of RAG.\\n4. **Flow** – The two sentences are merged into a single, smoother statement, improving readability.\\n\\n### Why the changes matter\\n- **Explain the cause**: Readers benefit from knowing the underlying reason for the failure, which sets up the motivation for neuro‑symbolic solutions later in the article.\\n- **Avoid absolutes**: Saying an LLM \"cannot\" is too strong; even with tool‑calling it *can* provide the date. The edit acknowledges the limitation of *pure* models while leaving room for extensions.\\n- **Maintain consistency**: Using terminology already introduced (e.g., \"real‑time information\") reinforces key concepts without introducing new jargon.\\n\\n### Tips for future writing\\n- When highlighting a limitation, briefly explain the mechanism behind it.\\n- Prefer precise language over absolute statements; qualify with phrases like \"typically\" or \"without augmentation\".\\n- Keep the narrative tight by combining related ideas into a single, well‑structured sentence where possible.\\n- If you reference a concrete example (e.g., date retrieval), consider linking it to a later section that discusses how RAG or tool‑calling resolves the issue.'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = chain_three.invoke({\"article\": article})\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd0753b",
   "metadata": {},
   "source": [
    "### Generate Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7a64998",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "image_prompt = PromptTemplate(\n",
    "    input_variables = [\"article\"],\n",
    "    template = (\n",
    "        \"Generate a prompt with less then 500 characters to generate an image \"\n",
    "        \"based on the following article: {article}\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7ef5da",
   "metadata": {},
   "source": [
    "The `generate_and_display` function will generate the article image once we have the prompt from our image prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6690450c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def generate_and_display_image(image_prompt):\n",
    "    print(image_prompt)\n",
    "    image_url = DallEAPIWrapper(model = \"dall-e-3\").run(image_prompt)\n",
    "    image_data = io.imread(image_url)\n",
    "\n",
    "    # And update the display code to:\n",
    "    plt.imshow(image_data)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# we wrap this in a RunnableLambda for use with LCEL\n",
    "image_gen_runnable = RunnableLambda(generate_and_display_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66913021",
   "metadata": {},
   "source": [
    "We have all of our image generation components ready, we chain them together again with LCEL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "49cd2933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain 4: inputs: article, article_para / outputs: new_suggestion_article\n",
    "chain_four = (\n",
    "    {\"article\": lambda x: x[\"article\"]}\n",
    "    | image_prompt\n",
    "    | llm\n",
    "    | (lambda x: x.content)\n",
    "    | image_gen_runnable\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea324f1",
   "metadata": {},
   "source": [
    "And now, we `invoke` our final chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d050bb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_four.invoke({\"article\": article})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d054de8",
   "metadata": {},
   "source": [
    "# Chat Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a527a6f",
   "metadata": {},
   "source": [
    "Memory Buffer is a mechanism that stores the conversation history between the user and the AI assistant. It acts as a short-term memory, allowing the language model to access previous messages in the conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f2a99c",
   "metadata": {},
   "source": [
    "There is 4 main type of Memory:\n",
    "- Buffer Memory: Storing all conversation in raw text\n",
    "- Summary Memory: Storing an overall context of a conversation\n",
    "- Window Memory: Storing N closest chat\n",
    "- Entity Memory: Storing key memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3799b7a9",
   "metadata": {},
   "source": [
    "### Buffer Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05884e5c",
   "metadata": {},
   "source": [
    "`ConversationBufferMemory` is the simplest form of conversational memory, it is literally just a place that we store messages, and then use to feed messages into our LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04bcc75",
   "metadata": {},
   "source": [
    "Let's start with LangChain's original `ConversationBufferMemory` object, we are setting `return_messages = True` to return the messages as a list of `ChatMessage` objects — unless using a non-chat model we would always set this to `True` as without it the messages are passed as a direct string which can lead to unexpected behavior from chat LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "402cd459",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1108/29041069.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(return_messages = True)\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79154e05",
   "metadata": {},
   "source": [
    "There are several ways that we can add messages to our memory, using the `save_context` method we can add a user query (via the `input` key) and the AI's response (via the `output` key)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30eaceaa",
   "metadata": {},
   "source": [
    "For example, we will create a conversation and save it directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c66678d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context(\n",
    "    {\"input\": \"Hi, my name is Dat\"},  # user message\n",
    "    {\"output\": \"Hey Dat, what's up? I'm an AI model.\"}  # AI response\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"I'm researching the different types of conversational memory.\"},  # user message\n",
    "    {\"output\": \"That's interesting, what type do you want to see?\"}  # AI response\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\"},  # user message\n",
    "    {\"output\": \"That's interesting, ConversationBufferMemory is the simplest form of conversational memory in LangChain. It stores the entire conversation history as a buffer, allowing the LLM to access all previous messages for context. It is useful for chatbots and agents that need to remember the full conversation. Beside The Window version will limited to what model need to remember instead\"}  # AI response\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"Buffer memory just stores the entire conversation, right?\"},  # user message\n",
    "    {\"output\": \"Yes\"}  # AI response\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"Buffer window memory stores the last k messages, dropping the rest.\"},  # user message\n",
    "    {\"output\": \"Also right!\"}  # AI response\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9808907b",
   "metadata": {},
   "source": [
    "Before using the memory, we need to load in any variables for that memory type — in this case, there are none, so we just pass an empty dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c247981c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi, my name is Dat', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hey Dat, what's up? I'm an AI model.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what type do you want to see?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, ConversationBufferMemory is the simplest form of conversational memory in LangChain. It stores the entire conversation history as a buffer, allowing the LLM to access all previous messages for context. It is useful for chatbots and agents that need to remember the full conversation. Beside The Window version will limited to what model need to remember instead\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Yes', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Also right!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa45149",
   "metadata": {},
   "source": [
    "With that, we've created our buffer memory. Before feeding it into our LLM let's quickly view the alternative method for adding messages to our memory. With this other method, we pass individual user and AI messages via the add_user_message and add_ai_message methods. To reproduce what we did above, we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "67f081eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hello, my name is Dat.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hey Dat, what's up? I'm an AI model.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what type do you want to see?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, ConversationBufferMemory is the simplest form of conversational memory in LangChain. It stores the entire conversation history as a buffer, allowing the LLM to access all previous messages for context. It is useful for chatbots and agents that need to remember the full conversation. Beside The Window version will limited to what model need to remember instead\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Yes', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Also right!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "memory.chat_memory.add_user_message(\"Hello, my name is Dat.\")\n",
    "memory.chat_memory.add_ai_message(\"Hey Dat, what's up? I'm an AI model.\")\n",
    "memory.chat_memory.add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "memory.chat_memory.add_ai_message(\"That's interesting, what type do you want to see?\")\n",
    "memory.chat_memory.add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "memory.chat_memory.add_ai_message(\"That's interesting, ConversationBufferMemory is the simplest form of conversational memory in LangChain. It stores the entire conversation history as a buffer, allowing the LLM to access all previous messages for context. It is useful for chatbots and agents that need to remember the full conversation. Beside The Window version will limited to what model need to remember instead\")\n",
    "memory.chat_memory.add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "memory.chat_memory.add_ai_message(\"Yes\")\n",
    "memory.chat_memory.add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "memory.chat_memory.add_ai_message(\"Also right!\")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e038f999",
   "metadata": {},
   "source": [
    "The outcome is exactly the same in either case. To pass this onto our LLM, we need to create a `ConversationChain` object — which is already deprecated in favor of the `RunnableWithMessageHistory` class, which we will cover in a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c744cfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1108/3315403223.py:3: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  chain = ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "chain = ConversationChain(\n",
    "    llm = llm,\n",
    "    memory = memory,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8498cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='Hello, my name is Dat.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hey Dat, what's up? I'm an AI model.\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what type do you want to see?\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, ConversationBufferMemory is the simplest form of conversational memory in LangChain. It stores the entire conversation history as a buffer, allowing the LLM to access all previous messages for context. It is useful for chatbots and agents that need to remember the full conversation. Beside The Window version will limited to what model need to remember instead\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}), AIMessage(content='Yes', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='Also right!', additional_kwargs={}, response_metadata={})]\n",
      "Human: what is my name again?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is my name again?',\n",
       " 'history': [HumanMessage(content='Hello, my name is Dat.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hey Dat, what's up? I'm an AI model.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what type do you want to see?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, ConversationBufferMemory is the simplest form of conversational memory in LangChain. It stores the entire conversation history as a buffer, allowing the LLM to access all previous messages for context. It is useful for chatbots and agents that need to remember the full conversation. Beside The Window version will limited to what model need to remember instead\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Yes', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Also right!', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='what is my name again?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Your name is\\u202fDat! 😊 (You mentioned it right at the start of our chat.)', additional_kwargs={}, response_metadata={})],\n",
       " 'response': 'Your name is\\u202fDat! 😊 (You mentioned it right at the start of our chat.)'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"what is my name again?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7606e7ed",
   "metadata": {},
   "source": [
    "##### `RunnableWithMessageHistory` in Buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a47c103",
   "metadata": {},
   "source": [
    "The `ConversationBufferMemory` type is due for deprecation. Instead, we can use the `RunnableWithMessageHistory` class to implement the same functionality.\n",
    "\n",
    "When implementing `RunnableWithMessageHistory` we will use LangChain Expression Language (LCEL) and for this we need to define our prompt template and LLM components. Our llm has already been defined, so now we just define a `ChatPromptTemplate` object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ddc51326",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import MessagesPlaceholder\n",
    "\n",
    "system_prompt = \"You are a helpful AI assistant.\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "    MessagesPlaceholder(variable_name = \"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f79726",
   "metadata": {},
   "source": [
    "We can link our `prompt_template` and our `llm` together to create a pipeline via LCEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a9c8563f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = prompt_template | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2adf46",
   "metadata": {},
   "source": [
    "Our `RunnableWithMessageHistory` requires our pipeline to be wrapped in a `RunnableWithMessageHistory` object. This object requires a few input parameters. One of those is `get_session_history`, which requires a function that returns a `ChatMessageHistory` object based on a session ID. We define this function ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c5812600",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "chat_map = {}\n",
    "def get_chat_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = InMemoryChatMessageHistory()\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a8765b",
   "metadata": {},
   "source": [
    "We also need to tell our runnable which variable name to use for the chat history (ie `history`) and which to use for the user's query (ie `query`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "67c97c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history = get_chat_history,\n",
    "    input_messages_key = \"query\",\n",
    "    history_messages_key = \"history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e648e22b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello, Dat! Nice to meet you. How can I help you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 86, 'total_tokens': 134, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 22, 'rejected_prediction_tokens': None, 'image_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0, 'video_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_name': 'openai/gpt-oss-120b:free', 'system_fingerprint': None, 'id': 'gen-1766227892-6yMu9P33iz3csodVbd2k', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--25b7c0b5-0988-40b5-9f48-d245f30811b4-0', usage_metadata={'input_tokens': 86, 'output_tokens': 48, 'total_tokens': 134, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'reasoning': 22}})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is Dat\"},\n",
    "    config = {\"session_id\": \"id_123\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "80590c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Dat. 😊', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 56, 'prompt_tokens': 116, 'total_tokens': 172, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 41, 'rejected_prediction_tokens': None, 'image_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0, 'video_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_name': 'openai/gpt-oss-120b:free', 'system_fingerprint': None, 'id': 'gen-1766227902-RtIbQD70eV10OUzXrueu', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--41a5fd2d-977c-49d2-a07f-3c286514d85a-0', usage_metadata={'input_tokens': 116, 'output_tokens': 56, 'total_tokens': 172, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'reasoning': 41}})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"What is my name again?\"},\n",
    "    config = {\"session_id\": \"id_123\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5be16a3",
   "metadata": {},
   "source": [
    "### Window Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb21402c",
   "metadata": {},
   "source": [
    "The `ConversationBufferWindowMemory` type is similar to `ConversationBufferMemory`, but only keeps track of the last `k` messages. There are a few reasons why we would want to keep only the last `k` messages:\n",
    "- More messages mean more tokens are sent with each request, more tokens increases latency and cost.\n",
    "- LLMs tend to perform worse when given more tokens, making them more likely to deviate from instructions, hallucinate, or \"forget\" information provided to them. Conciseness is key to high performing LLMs.\n",
    "- If we keep all messages we will eventually hit the LLM's context window limit, by adding a window size `k` we can ensure we never hit this limit.\n",
    "\n",
    "The buffer window solves many problems that we encounter with the standard buffer memory, while still being a very simple and intuitive form of conversational memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "88c6d2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1108/1794397535.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(k = 4, return_messages = True)\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k = 4, return_messages = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "28f22401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what type do you want to see?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, ConversationBufferMemory is the simplest form of conversational memory in LangChain. It stores the entire conversation history as a buffer, allowing the LLM to access all previous messages for context. It is useful for chatbots and agents that need to remember the full conversation. Beside The Window version will limited to what model need to remember instead\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Yes', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Also right!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.chat_memory.add_user_message(\"Hello, my name is Dat.\")\n",
    "memory.chat_memory.add_ai_message(\"Hey Dat, what's up? I'm an AI model.\")\n",
    "memory.chat_memory.add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "memory.chat_memory.add_ai_message(\"That's interesting, what type do you want to see?\")\n",
    "memory.chat_memory.add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "memory.chat_memory.add_ai_message(\"That's interesting, ConversationBufferMemory is the simplest form of conversational memory in LangChain. It stores the entire conversation history as a buffer, allowing the LLM to access all previous messages for context. It is useful for chatbots and agents that need to remember the full conversation. Beside The Window version will limited to what model need to remember instead\")\n",
    "memory.chat_memory.add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "memory.chat_memory.add_ai_message(\"Yes\")\n",
    "memory.chat_memory.add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "memory.chat_memory.add_ai_message(\"Also right!\")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15e5149",
   "metadata": {},
   "source": [
    "As before, we use the `ConversationChain` object (again, this is deprecated and we will rewrite it with `RunnableWithMessageHistory` in a moment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bd0054b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = ConversationChain(\n",
    "    llm = llm,\n",
    "    memory = memory,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "06e77341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what type do you want to see?\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, ConversationBufferMemory is the simplest form of conversational memory in LangChain. It stores the entire conversation history as a buffer, allowing the LLM to access all previous messages for context. It is useful for chatbots and agents that need to remember the full conversation. Beside The Window version will limited to what model need to remember instead\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}), AIMessage(content='Yes', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='Also right!', additional_kwargs={}, response_metadata={})]\n",
      "Human: what is my name again?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is my name again?',\n",
       " 'history': [HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what type do you want to see?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, ConversationBufferMemory is the simplest form of conversational memory in LangChain. It stores the entire conversation history as a buffer, allowing the LLM to access all previous messages for context. It is useful for chatbots and agents that need to remember the full conversation. Beside The Window version will limited to what model need to remember instead\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Yes', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Also right!', additional_kwargs={}, response_metadata={})],\n",
       " 'response': 'I don’t actually know your name— you haven’t mentioned it in our conversation yet. If you’d like to share it, I’ll be happy to remember it for the rest of our chat!'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"what is my name again?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6ba0fb",
   "metadata": {},
   "source": [
    "The reason our LLM can no longer remember our name is because we have set the `k` parameter to 4, meaning that only the last messages are stored in memory, as we can see above this does not include the first message where we introduced ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f367d29",
   "metadata": {},
   "source": [
    "Based on the agent forgetting our name, we might wonder why we would ever use this memory type compared to the standard buffer memory. Well, as with most things in AI, it is always a trade-off. Here we are able to support much longer conversations, use less tokens, and improve latency — but these come at the cost of forgetting non-recent messages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633320c6",
   "metadata": {},
   "source": [
    "##### `RunnableWithMessageHistory` in Window Buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296f42fd",
   "metadata": {},
   "source": [
    "To implement this memory type using the `RunnableWithMessageHistory` class, we can use the same approach as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5128dfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class BufferWindowMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    messages: list[BaseMessage] = Field(default_factory=list)\n",
    "    k: int = Field(default_factory = int)\n",
    "\n",
    "    def __init__(self, k: int):\n",
    "        super().__init__(k = k)\n",
    "        print(f\"Initializing BufferWindowMessageHistory with k={k}\")\n",
    "\n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        \"\"\"Add messages to the history, removing any messages beyond\n",
    "        the last `k` messages.\n",
    "        \"\"\"\n",
    "        self.messages.extend(messages)\n",
    "        self.messages = self.messages[-self.k:]\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear the history.\"\"\"\n",
    "        self.messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0c09a1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_map = {}\n",
    "def get_chat_history(session_id: str, k: int = 4) -> BufferWindowMessageHistory:\n",
    "    print(f\"get_chat_history called with session_id={session_id} and k={k}\")\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = BufferWindowMessageHistory(k = k)\n",
    "    # remove anything beyond the last\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2b05fb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import ConfigurableFieldSpec\n",
    "\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history = get_chat_history,\n",
    "    input_messages_key = \"query\",\n",
    "    history_messages_key = \"history\",\n",
    "    history_factory_config = [\n",
    "        ConfigurableFieldSpec(\n",
    "            id = \"session_id\",\n",
    "            annotation = str,\n",
    "            name = \"Session ID\",\n",
    "            description = \"The session ID to use for the chat history\",\n",
    "            default = \"id_default\",\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id = \"k\",\n",
    "            annotation = int,\n",
    "            name = \"k\",\n",
    "            description = \"The number of messages to keep in the history\",\n",
    "            default = 4,\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58fdee7",
   "metadata": {},
   "source": [
    "Now we invoke our runnable, this time passing a `k` parameter via the config parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b9cd6d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session_id=id_k4 and k=4\n",
      "Initializing BufferWindowMessageHistory with k=4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello, Dat! Nice to meet you. How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 55, 'prompt_tokens': 86, 'total_tokens': 141, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 29, 'rejected_prediction_tokens': None, 'image_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0, 'video_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_name': 'openai/gpt-oss-120b:free', 'system_fingerprint': None, 'id': 'gen-1766227951-BPAkEBlsJE6goBkbYaVw', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--dcfc2097-5329-41f4-b3a1-8fe4b076073c-0', usage_metadata={'input_tokens': 86, 'output_tokens': 55, 'total_tokens': 141, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'reasoning': 29}})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is Dat\"},\n",
    "    config = {\"configurable\": {\"session_id\": \"id_k4\", \"k\": 4}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee11904",
   "metadata": {},
   "source": [
    "We can also modify the messages that are stored in memory by modifying the records inside the chat_map dictionary directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5f4ceff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Yes', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Also right!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"id_k4\"].clear()  # clear the history\n",
    "\n",
    "chat_map[\"id_k4\"].add_user_message(\"Hello, my name is Dat.\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"Hey Dat, what's up? I'm an AI model.\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"That's interesting, what type do you want to see?\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"That's interesting, ConversationBufferMemory is the simplest form of conversational memory in LangChain. It stores the entire conversation history as a buffer, allowing the LLM to access all previous messages for context. It is useful for chatbots and agents that need to remember the full conversation. Beside The Window version will limited to what model need to remember instead\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"Yes\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"Also right!\")\n",
    "\n",
    "chat_map[\"id_k4\"].messages  # should contain only the last 4 messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1b38aa53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session_id=id_k4 and k=4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I’m not sure— I don’t have a record of your name from our earlier messages. Could you let me know what you’d like me to call you?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 95, 'prompt_tokens': 129, 'total_tokens': 224, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 61, 'rejected_prediction_tokens': None, 'image_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0, 'video_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_name': 'openai/gpt-oss-120b:free', 'system_fingerprint': None, 'id': 'gen-1766227958-cW7Aq67I8RU59hksJ6NK', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--23bb24ba-7191-410d-b486-7da1ecae4bb5-0', usage_metadata={'input_tokens': 129, 'output_tokens': 95, 'total_tokens': 224, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'reasoning': 61}})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test again\n",
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"what is my name again?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"id_k4\", \"k\": 4}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2398b983",
   "metadata": {},
   "source": [
    "Now let's initialize a new session with further `k`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "16ec6281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session_id=id_k14 and k=14\n",
      "Initializing BufferWindowMessageHistory with k=14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello James! Nice to meet you. How can I help you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 47, 'prompt_tokens': 86, 'total_tokens': 133, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 22, 'rejected_prediction_tokens': None, 'image_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0, 'video_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_name': 'openai/gpt-oss-120b:free', 'system_fingerprint': None, 'id': 'gen-1766227962-Eb3exVQn5H8hZL77AeXL', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--6172d667-0b7d-4e88-a49e-b0a3dc0e5aad-0', usage_metadata={'input_tokens': 86, 'output_tokens': 47, 'total_tokens': 133, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'reasoning': 22}})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is James\"},\n",
    "    config = {\"session_id\": \"id_k14\", \"k\": 14}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5e4dbe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_map[\"id_k4\"].add_user_message(\"Hello, my name is Dat.\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"Hey Dat, what's up? I'm an AI model.\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"That's interesting, what type do you want to see?\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"That's interesting, ConversationBufferMemory is the simplest form of conversational memory in LangChain. It stores the entire conversation history as a buffer, allowing the LLM to access all previous messages for context. It is useful for chatbots and agents that need to remember the full conversation. Beside The Window version will limited to what model need to remember instead\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"Yes\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"Also right!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "36dbd49d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Hello James! Nice to meet you. How can I help you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 47, 'prompt_tokens': 86, 'total_tokens': 133, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 22, 'rejected_prediction_tokens': None, 'image_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0, 'video_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_name': 'openai/gpt-oss-120b:free', 'system_fingerprint': None, 'id': 'gen-1766227962-Eb3exVQn5H8hZL77AeXL', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--6172d667-0b7d-4e88-a49e-b0a3dc0e5aad-0', usage_metadata={'input_tokens': 86, 'output_tokens': 47, 'total_tokens': 133, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'reasoning': 22}})]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"id_k14\"].messages  # should contain all messages since k = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6606a1a",
   "metadata": {},
   "source": [
    "### Summary Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8cb507",
   "metadata": {},
   "source": [
    "This memory type keeps track of a summary of the conversation rather than the entire conversation. This is useful for long conversations where we don't need to keep track of the entire conversation, but we do want to keep some thread of the full conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9af298b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1108/1770911873.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(llm = llm)\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "memory = ConversationSummaryMemory(llm = llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b948247",
   "metadata": {},
   "source": [
    "Unlike with the previous memory types, we need to provide an llm to initialize `ConversationSummaryMemory`. The reason for this is that we need an LLM to generate the conversation summaries.\n",
    "\n",
    "Beyond this small tweak, using `ConversationSummaryMemory` is the same as with our previous memory types when using the deprecated `ConversationChain` object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "96d3fd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = ConversationChain(\n",
    "    llm = llm,\n",
    "    memory = memory,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "50cd4dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: hello there my name is Dat\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human greets the AI and introduces themselves as “Dat.” The AI replies with a friendly hello, introduces itself as ChatGPT (GPT‑4, knowledge up to June 2024), outlines its abilities—multilingual text handling, creative writing assistance, technical explanations, and everyday help like recipes or coding—and then asks Dat about their hobbies or interests.\n",
      "Human: i am researching different types of AI model\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human greets the AI and introduces themselves as “Dat.” The AI replies with a friendly hello, introduces itself as ChatGPT (GPT‑4, knowledge up to June 2024), outlines its abilities—multilingual text handling, creative writing assistance, technical explanations, and everyday help like recipes or coding—and asks Dat about their hobbies or interests.  \n",
      "\n",
      "Dat then says they are researching different types of AI models. The AI provides a comprehensive tour of AI model families, covering:\n",
      "\n",
      "* **Rule‑based / symbolic AI** – explicit if‑then systems, interpretable but brittle.  \n",
      "* **Statistical / classical machine‑learning** – linear models, tree‑based ensembles (XGBoost, LightGBM, CatBoost), kernel methods (SVM, Kernel PCA), probabilistic models (Naïve Bayes, HMM, GMM); good for tabular or modest‑size data, often more interpretable than deep nets.  \n",
      "* **Shallow neural networks** – one‑ or two‑layer perceptrons, early pattern‑recognition tools that were superseded by deeper architectures.  \n",
      "* **Deep learning** –  \n",
      "  * **CNNs** for images/video (LeNet, AlexNet, ResNet, EfficientNet, ConvNeXt, YOLO, etc.).  \n",
      "  * **RNNs / LSTMs / GRUs** for sequential data (speech, language modeling pre‑transformer).  \n",
      "  * **Transformers** (self‑attention) and their major families: encoder‑only (BERT, RoBERTa), decoder‑only (GPT‑1/2/3/3.5/4, LLaMA, Falcon, Mistral, Claude, Gemini), encoder‑decoder (T5, BART, mBART), multimodal (CLIP, DALL·E, Stable Diffusion, Flamingo, Gemini‑V).  \n",
      "  * **Diffusion models** (DDPM, Stable Diffusion, Imagen, Midjourney) for high‑fidelity image/audio generation.  \n",
      "  * **Large Language Models (LLMs)** – scaling from millions to trillions of parameters, trained with pre‑training, instruction‑tuning, RLHF; notable families include GPT‑4, Claude, Gemini, LLaMA 2/3, Mistral‑7B, Falcon‑180B, etc.  \n",
      "  * **Reinforcement‑learning agents** (DQN, AlphaGo/AlphaZero, MuZero, AutoGPT‑style tool‑using agents).  \n",
      "* **Hybrid & emerging paradigms** – neuro‑symbolic AI, mixture‑of‑experts (Switch‑Transformer, GLaM), retrieval‑augmented generation (RAG), foundation models for non‑text modalities (Whisper, Codex, Point‑E).  \n",
      "* **Model deployment & “tiny” variants** – quantization, pruning, distillation (DistilBERT, TinyLlama), on‑device inference frameworks (Core ML, TensorFlow Lite, ONNX Runtime).  \n",
      "\n",
      "The AI also supplies a quick cheat‑sheet linking common goals (tabular classification, object detection, translation, creative image generation, code writing, web‑browsing chatbots, robot control) to the most suitable model families, suggests next steps (pick a domain, choose benchmarks, explore open‑source repos, read model cards), and shares fun tidbits about scaling laws, emergent abilities, and the energy footprint of large‑scale training.\n",
      "Human: what have we talked about so far?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what have we talked about so far?',\n",
       " 'history': 'The human greets the AI and introduces themselves as “Dat.” The AI replies with a friendly hello, introduces itself as ChatGPT (GPT‑4, knowledge up to June\\u202f2024), outlines its abilities—multilingual text handling, creative writing assistance, technical explanations, and everyday help like recipes or coding—and asks Dat about their hobbies or interests.  \\n\\nDat then says they are researching different types of AI models. The AI provides a comprehensive tour of AI model families, covering:\\n\\n* **Rule‑based / symbolic AI** – explicit if‑then systems, interpretable but brittle.  \\n* **Statistical / classical machine‑learning** – linear models, tree‑based ensembles (XGBoost, LightGBM, CatBoost), kernel methods (SVM, Kernel PCA), probabilistic models (Naïve Bayes, HMM, GMM); good for tabular or modest‑size data, often more interpretable than deep nets.  \\n* **Shallow neural networks** – one‑ or two‑layer perceptrons, early pattern‑recognition tools that were superseded by deeper architectures.  \\n* **Deep learning** –  \\n  * **CNNs** for images/video (LeNet, AlexNet, ResNet, EfficientNet, ConvNeXt, YOLO, etc.).  \\n  * **RNNs / LSTMs / GRUs** for sequential data (speech, language modeling pre‑transformer).  \\n  * **Transformers** (self‑attention) and their major families: encoder‑only (BERT, RoBERTa), decoder‑only (GPT‑1/2/3/3.5/4, LLaMA, Falcon, Mistral, Claude, Gemini), encoder‑decoder (T5, BART, mBART), multimodal (CLIP, DALL·E, Stable Diffusion, Flamingo, Gemini‑V).  \\n  * **Diffusion models** (DDPM, Stable Diffusion, Imagen, Midjourney) for high‑fidelity image/audio generation.  \\n  * **Large Language Models (LLMs)** – scaling from millions to trillions of parameters, trained with pre‑training, instruction‑tuning, RLHF; notable families include GPT‑4, Claude, Gemini, LLaMA\\u202f2/3, Mistral‑7B, Falcon‑180B, etc.  \\n  * **Reinforcement‑learning agents** (DQN, AlphaGo/AlphaZero, MuZero, AutoGPT‑style tool‑using agents).  \\n* **Hybrid & emerging paradigms** – neuro‑symbolic AI, mixture‑of‑experts (Switch‑Transformer, GLaM), retrieval‑augmented generation (RAG), foundation models for non‑text modalities (Whisper, Codex, Point‑E).  \\n* **Model deployment & “tiny” variants** – quantization, pruning, distillation (DistilBERT, TinyLlama), on‑device inference frameworks (Core\\u202fML, TensorFlow Lite, ONNX Runtime).  \\n\\nThe AI also supplies a quick cheat‑sheet linking common goals (tabular classification, object detection, translation, creative image generation, code writing, web‑browsing chatbots, robot control) to the most suitable model families, suggests next steps (pick a domain, choose benchmarks, explore open‑source repos, read model cards), and shares fun tidbits about scaling laws, emergent abilities, and the energy footprint of large‑scale training.',\n",
       " 'response': 'Sure thing, Dat! Here’s a quick recap of our chat so far:\\n\\n1. **Greetings & introductions**  \\n   * You said hi and introduced yourself as **Dat**.  \\n   * I replied with a friendly hello, introduced myself as **ChatGPT (GPT‑4, knowledge up to June\\u202f2024)**, and gave you a short rundown of what I can help with (multilingual text, creative writing, technical explanations, recipes, coding, etc.). I also asked what you like to do in your free time.\\n\\n2. **Your research focus**  \\n   * You mentioned you’re looking into the different types of AI models.\\n\\n3. **My deep‑dive on AI model families**  \\n   * **Rule‑based / symbolic AI** – explicit if‑then systems, very interpretable but brittle.  \\n   * **Statistical / classical ML** – linear models, tree ensembles (XGBoost, LightGBM, CatBoost), kernel methods (SVM, Kernel\\u202fPCA), probabilistic models (Naïve Bayes, HMM, GMM). Great for tabular or modest‑size data and often more explainable than deep nets.  \\n   * **Shallow neural networks** – one‑ or two‑layer perceptrons, the early “deep‑learning‑precursors.”  \\n   * **Deep learning**  \\n     * **CNNs** – image/video workhorses (LeNet, AlexNet, ResNet, EfficientNet, ConvNeXt, YOLO, etc.).  \\n     * **RNNs / LSTMs / GRUs** – sequential data before transformers took over.  \\n     * **Transformers** – self‑attention revolution, broken down into:  \\n       * Encoder‑only (BERT, RoBERTa)  \\n       * Decoder‑only (GPT‑1/2/3/3.5/4, LLaMA, Falcon, Mistral, Claude, Gemini)  \\n       * Encoder‑decoder (T5, BART, mBART)  \\n       * Multimodal (CLIP, DALL·E, Stable Diffusion, Flamingo, Gemini‑V)  \\n     * **Diffusion models** – DDPM, Stable Diffusion, Imagen, Midjourney for high‑fidelity image/audio generation.  \\n     * **Large Language Models (LLMs)** – scaling from millions to trillions of parameters, trained with pre‑training → instruction‑tuning → RLHF. Notable families: GPT‑4, Claude, Gemini, LLaMA\\u202f2/3, Mistral‑7B, Falcon‑180B, etc.  \\n     * **Reinforcement‑learning agents** – DQN, AlphaGo/AlphaZero, MuZero, tool‑using agents like AutoGPT.  \\n   * **Hybrid & emerging paradigms** – neuro‑symbolic AI, mixture‑of‑experts (Switch‑Transformer, GLaM), retrieval‑augmented generation (RAG), foundation models for non‑text (Whisper, Codex, Point‑E).  \\n   * **Model deployment & “tiny” variants** – quantization, pruning, distillation (DistilBERT, TinyLlama), on‑device runtimes (Core\\u202fML, TensorFlow\\u202fLite, ONNX Runtime).\\n\\n4. **Practical cheat‑sheet & next steps**  \\n   * Mapped common tasks (tabular classification, object detection, translation, creative image generation, code writing, web‑browsing chatbots, robot control) to the model families that usually work best.  \\n   * Suggested a workflow: pick a domain → choose benchmark datasets → explore open‑source repos & model cards → read the relevant papers.  \\n\\n5. **Fun tidbits**  \\n   * Mentioned scaling laws (performance roughly follows a power‑law with model size, data, and compute).  \\n   * Talked about emergent abilities (e.g., in‑context learning, chain‑of‑thought reasoning) that appear once models cross certain size thresholds.  \\n   * Briefly noted the energy footprint of training massive models and why efficiency research (quantization, sparsity, better hardware) matters.\\n\\nThat’s the gist of what we’ve covered so far. Anything you’d like to dive deeper into?'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"hello there my name is Dat\"})\n",
    "chain.invoke({\"input\": \"i am researching different types of AI model\"})\n",
    "chain.invoke({\"input\": \"what have we talked about so far?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fa77c26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human greeted the AI and introduced themselves as “Dat,” and the AI replied with a friendly hello, introduced itself as ChatGPT (GPT‑4, knowledge up to June 2024), listed its multilingual text, creative‑writing, technical‑explanation, recipe‑and‑coding assistance abilities, and asked about Dat’s hobbies. Dat said they were researching different types of AI models, so the AI gave a comprehensive tour of AI model families—including rule‑based/symbolic systems; statistical/classical machine‑learning (linear models, tree ensembles, kernel methods, probabilistic models); shallow neural networks; deep learning (CNNs for vision, RNN/LSTM/GRU for sequences, transformers broken into encoder‑only, decoder‑only, encoder‑decoder, and multimodal variants; diffusion models for high‑fidelity image/audio generation; large language models with scaling, instruction‑tuning, RLHF; reinforcement‑learning agents such as DQN, AlphaGo/AlphaZero, tool‑using agents); hybrid and emerging paradigms (neuro‑symbolic AI, mixture‑of‑experts, retrieval‑augmented generation, foundation models for speech, code, 3‑D); and model‑deployment tricks (quantization, pruning, distillation, on‑device runtimes). The AI also supplied a quick cheat‑sheet linking common tasks (tabular classification, object detection, translation, creative image generation, code writing, web‑browsing chatbots, robot control) to the most suitable model families, suggested a workflow (pick a domain, choose benchmarks, explore repos and model cards, read papers), and shared fun tidbits about scaling laws, emergent abilities, and the energy footprint of large‑scale training. Afterward, the human asked “what have we talked about so far?” and the AI responded with a concise recap that reiterated all of the above points in a numbered list, confirming the greetings, research focus, detailed AI‑model overview, practical cheat‑sheet and next‑step suggestions, and the scaling‑law/energy‑footprint observations.\n",
      "Human: What is my name again?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is my name again?',\n",
       " 'history': 'The human greeted the AI and introduced themselves as “Dat,” and the AI replied with a friendly hello, introduced itself as ChatGPT (GPT‑4, knowledge up to June\\u202f2024), listed its multilingual text, creative‑writing, technical‑explanation, recipe‑and‑coding assistance abilities, and asked about Dat’s hobbies.\\u202fDat said they were researching different types of AI models, so the AI gave a comprehensive tour of AI model families—including rule‑based/symbolic systems; statistical/classical machine‑learning (linear models, tree ensembles, kernel methods, probabilistic models); shallow neural networks; deep learning (CNNs for vision, RNN/LSTM/GRU for sequences, transformers broken into encoder‑only, decoder‑only, encoder‑decoder, and multimodal variants; diffusion models for high‑fidelity image/audio generation; large language models with scaling, instruction‑tuning, RLHF; reinforcement‑learning agents such as DQN, AlphaGo/AlphaZero, tool‑using agents); hybrid and emerging paradigms (neuro‑symbolic AI, mixture‑of‑experts, retrieval‑augmented generation, foundation models for speech, code, 3‑D); and model‑deployment tricks (quantization, pruning, distillation, on‑device runtimes).\\u202fThe AI also supplied a quick cheat‑sheet linking common tasks (tabular classification, object detection, translation, creative image generation, code writing, web‑browsing chatbots, robot control) to the most suitable model families, suggested a workflow (pick a domain, choose benchmarks, explore repos and model cards, read papers), and shared fun tidbits about scaling laws, emergent abilities, and the energy footprint of large‑scale training.\\u202fAfterward, the human asked “what have we talked about so far?” and the AI responded with a concise recap that reiterated all of the above points in a numbered list, confirming the greetings, research focus, detailed AI‑model overview, practical cheat‑sheet and next‑step suggestions, and the scaling‑law/energy‑footprint observations.',\n",
       " 'response': 'Your name is **Dat**.'}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"What is my name again?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ccdd46",
   "metadata": {},
   "source": [
    "As this information was stored in the summary the LLM successfully recalled our name. This may not always be the case, by summarizing the conversation we inevitably compress the full amount of information and so we may lose key details occasionally. Nonetheless, this is a great memory type for long conversations while retaining some key information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b431464f",
   "metadata": {},
   "source": [
    "##### `RunnableWithMessageHistory` in Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d187e17",
   "metadata": {},
   "source": [
    "As with the window buffer memory, we need to define a custom implementation of the `InMemoryChatMessageHistory` class. We'll call this one `ConversationSummaryMessageHistory`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "550c1f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "\n",
    "class ConversationSummaryMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    messages: list[BaseMessage] = Field(default_factory=list)\n",
    "    llm: ChatOpenAI = Field(default_factory=ChatOpenAI)\n",
    "\n",
    "    def __init__(self, llm: ChatOpenAI):\n",
    "        super().__init__(llm=llm)\n",
    "\n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        \"\"\"Add messages to the history, removing any messages beyond\n",
    "        the last `k` messages.\n",
    "        \"\"\"\n",
    "        self.messages.extend(messages)\n",
    "        # construct the summary chat messages\n",
    "        summary_prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(\n",
    "                \"Given the existing conversation summary and the new messages, \"\n",
    "                \"generate a new summary of the conversation. Ensuring to maintain \"\n",
    "                \"as much relevant information as possible.\"\n",
    "            ),\n",
    "            HumanMessagePromptTemplate.from_template(\n",
    "                \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
    "                \"New messages:\\n{messages}\"\n",
    "            )\n",
    "        ])\n",
    "        # format the messages and invoke the LLM\n",
    "        new_summary = self.llm.invoke(\n",
    "            summary_prompt.format_messages(\n",
    "                existing_summary=self.messages.content,\n",
    "                messages=[x.content for x in messages]\n",
    "            )\n",
    "        )\n",
    "        # replace the existing history with a single system summary message\n",
    "        self.messages = [SystemMessage(content=new_summary.content)]\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear the history.\"\"\"\n",
    "        self.messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5e554417",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_map = {}\n",
    "def get_chat_history(session_id: str, llm: ChatOpenAI) -> ConversationSummaryMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = ConversationSummaryMessageHistory(llm=llm)\n",
    "    # return the chat history\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6d19a9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Session ID\",\n",
    "            description=\"The session ID to use for the chat history\",\n",
    "            default=\"id_default\",\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"llm\",\n",
    "            annotation=ChatOpenAI,\n",
    "            name=\"LLM\",\n",
    "            description=\"The LLM to use for the conversation summary\",\n",
    "            default=llm,\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc23fb9",
   "metadata": {},
   "source": [
    "Now we invoke our runnable, this time passing a `llm` parameter via the `config` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "92db5dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: AttributeError(\"'list' object has no attribute 'content'\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello, Dat! Nice to meet you. How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 55, 'prompt_tokens': 86, 'total_tokens': 141, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 29, 'rejected_prediction_tokens': None, 'image_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0, 'video_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_name': 'openai/gpt-oss-120b:free', 'system_fingerprint': None, 'id': 'gen-1766228326-N1OwE2oIHjwTux0gpoCk', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--cded2bb4-af02-4efc-804a-a31ad80c8540-0', usage_metadata={'input_tokens': 86, 'output_tokens': 55, 'total_tokens': 141, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'reasoning': 29}})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is Dat\"},\n",
    "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8ffed2c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi, my name is Dat', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Hello, Dat! Nice to meet you. How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 55, 'prompt_tokens': 86, 'total_tokens': 141, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 29, 'rejected_prediction_tokens': None, 'image_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0, 'video_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_name': 'openai/gpt-oss-120b:free', 'system_fingerprint': None, 'id': 'gen-1766228326-N1OwE2oIHjwTux0gpoCk', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--cded2bb4-af02-4efc-804a-a31ad80c8540-0', usage_metadata={'input_tokens': 86, 'output_tokens': 55, 'total_tokens': 141, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'reasoning': 29}})]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"id_123\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "01013df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: AttributeError(\"'list' object has no attribute 'content'\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi, my name is Dat', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Hello, Dat! Nice to meet you. How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 55, 'prompt_tokens': 86, 'total_tokens': 141, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 29, 'rejected_prediction_tokens': None, 'image_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0, 'video_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_name': 'openai/gpt-oss-120b:free', 'system_fingerprint': None, 'id': 'gen-1766228326-N1OwE2oIHjwTux0gpoCk', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--cded2bb4-af02-4efc-804a-a31ad80c8540-0', usage_metadata={'input_tokens': 86, 'output_tokens': 55, 'total_tokens': 141, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'reasoning': 29}}),\n",
       " HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Below is a compact yet comprehensive guide to the **different types of conversational memory**—both from a **human‑cognitive** perspective and from the **AI/LLM** side of things. I’ve organized the material into three sections:\\n\\n1. **Human memory systems that shape conversation**  \\n2. **Memory architectures used in conversational AI**  \\n3. **How the two worlds intersect & practical take‑aways for research**\\n\\nFeel free to let me know which part you’d like to dive deeper into (e.g., technical implementations, psychological studies, evaluation metrics, etc.).\\n\\n---\\n\\n## 1. Human Memory Systems that Shape Conversation\\n\\n| Memory Type | Core Characteristics | Role in Conversation | Typical Capacity / Duration | Key References |\\n|-------------|----------------------|----------------------|-----------------------------|----------------|\\n| **Sensory Memory** | Brief (≤\\u202f1\\u202fs) storage of raw sensory input (iconic, echoic) | Allows us to “catch” the last word or tone before we parse it | ~4–12\\u202fbits per modality | Sperling (1960) |\\n| **Short‑Term / Working Memory** | Actively maintained, limited (≈\\u202f4\\u202f±\\u202f1 “chunks”) | Holds the current turn, recent utterances, and any mental “scratchpad” for planning the next response | ~10–30\\u202fs without rehearsal | Baddeley & Hitch (1974) |\\n| **Episodic Memory (Explicit Long‑Term)** | Autobiographical events with context (time, place, emotions) | Enables us to refer back to past interactions (“Remember when you told me…”) | Potentially lifelong, but retrieval is cue‑dependent | Tulving (1972) |\\n| **Semantic Memory (Explicit Long‑Term)** | General world knowledge, facts, concepts | Supplies background knowledge (“Paris is the capital of France”) | Lifelong, highly organized | Anderson (1976) |\\n| **Procedural / Implicit Memory** | Skills, habits, conditioned responses | Drives conversational fluency (e.g., turn‑taking, politeness formulas) without conscious effort | Lifelong, often inaccessible to introspection | Reber (1993) |\\n| **Prospective Memory** | Remembering to do something in the future (e.g., “Ask me about the report tomorrow”) | Supports follow‑up questions, reminders, commitments | Can be triggered by internal or external cues | Einstein & McDaniel (1990) |\\n| **Meta‑Memory** | Awareness of what we know/forget | Guides self‑monitoring (“I’m not sure I understood that”) | Variable; often inaccurate | Koriat (2007) |\\n\\n### Why These Matter for Conversational Research\\n- **Turn‑taking** relies on working‑memory buffers that keep the last few utterances online.  \\n- **Coherence** (maintaining topic, referents, and discourse relations) draws on episodic recall of the dialogue history.  \\n- **Politeness & Pragmatics** are largely procedural—learned patterns that run automatically.  \\n- **Error recovery** often involves meta‑memory (“Did I just say that?”) and prospective memory (“I need to ask a clarification later”).\\n\\n---\\n\\n## 2. Memory Architectures in Conversational AI\\n\\n| Architecture | What It Stores | Typical Scope (tokens/records) | Retrieval Mechanism | Strengths | Weaknesses / Open Challenges |\\n|--------------|----------------|-------------------------------|---------------------|----------|------------------------------|\\n| **Stateless / Context‑Window Only** | The most recent *N* tokens (e.g., 4\\u202fk‑token window) | 2–8\\u202fk tokens (≈\\u202f3–5\\u202fk words) | Sliding window; no explicit retrieval | Simplicity; low latency | Forgetting beyond window; no long‑term personalization |\\n| **Session‑Level Memory** | All turns of a single user session (often stored in a DB) | Up to hundreds of turns (≈\\u202f10–30\\u202fk tokens) | Linear scan or indexed by turn‑id | Keeps conversation coherent across a session | Still limited by model context; scaling to many concurrent sessions can be costly |\\n| **User‑Profile / Persistent Memory** | Structured attributes (name, preferences, past purchases) + optional raw dialogue snippets | Unlimited (persisted in relational/NoSQL store) | Key‑value lookup, rule‑based or learned retrieval | Enables personalization across sessions | Requires schema design; privacy & data‑governance concerns |\\n| **Retrieval‑Augmented Generation (RAG)** | External knowledge base (documents, FAQs, web snippets) + optional dialogue excerpts | Millions of documents (vector‑indexed) | Approximate nearest‑neighbor (ANN) search on embeddings | Provides up‑to‑date factual grounding; bypasses context‑window limits | Retrieval latency; hallucination if retrieved text is mis‑interpreted |\\n| **Memory‑Network / Differentiable Neural Memory** | Learned latent memory slots (e.g., Neural Turing Machine, Memory‑augmented Transformer) | Typically 64–512 slots, each 64–1024‑dim vectors | Content‑based addressing via attention | End‑to‑end trainable; can store abstract “facts” | Hard to interpret; scaling to real‑world dialogue size is still experimental |\\n| **Hybrid “Episodic + Semantic” Stores** | *Episodic*: raw turn‑by‑turn logs; *Semantic*: extracted triples, intents, sentiment | Episodic: unlimited; Semantic: compact graph (hundreds of nodes) | Graph queries + vector search | Supports both recall of exact phrasing and abstract reasoning | Requires robust information extraction pipelines |\\n| **Prospective Memory Modules** | Future actions or reminders (e.g., “remind me in 2\\u202fh”) | Small queue of pending tasks | Scheduler + trigger detection (time or cue) | Enables follow‑up actions, task‑oriented dialogue | Needs reliable time‑keeping & user‑consent handling |\\n| **Meta‑Memory / Confidence Estimation** | Model’s self‑estimated certainty per token/utterance | One scalar per token (or per turn) | Directly from model logits or calibrated post‑hoc | Allows the system to ask clarifying questions when confidence is low | Calibration remains an open research problem |\\n\\n### Typical Pipeline (RAG‑style)  \\n\\n```\\nUser Input → Embedding (e.g., OpenAI ada) → Vector DB Retrieval (top‑k) → \\nRerank (cross‑encoder) → Concatenate retrieved passages + recent dialogue → \\nLLM Prompt → Model Generation → (Optional) Post‑process (filter, safety)\\n```\\n\\n### Evaluation Metrics for Conversational Memory\\n\\n| Metric | What It Measures | How to Compute |\\n|--------|------------------|----------------|\\n| **Recall@k (R@k)** | Ability to retrieve the correct past turn or fact | Count of times the ground‑truth turn appears in top‑k retrieved items |\\n| **Mean Reciprocal Rank (MRR)** | Rank quality of the first relevant item | 1 / rank of first relevant result, averaged |\\n| **Coherence Scores (e.g., Entity Grid, Coherence Model)** | Global discourse coherence across turns | Compare model‑generated dialogue to human‑annotated coherence |\\n| **Personalization Accuracy** | Correct use of user‑profile facts | % of turns where a known user attribute is correctly referenced |\\n| **Prospective Memory Success Rate** | Whether scheduled reminders are delivered | % of pending tasks that fire at the right time |\\n| **Meta‑Memory Calibration (ECE, Brier Score)** | Alignment between confidence and actual correctness | Plot confidence vs. accuracy; compute Expected Calibration Error |\\n\\n---\\n\\n## 3. Bridging Human and AI Memory: Research Angles\\n\\n| Human Concept | AI Analogue | Open Research Questions |\\n|---------------|-------------|--------------------------|\\n| **Chunking (grouping items into meaningful units)** | Token‑level vs. “semantic chunk” embeddings; hierarchical context windows | How can we automatically discover optimal chunk boundaries for dialogue? |\\n| **Forgetting Curve (Ebbinghaus)** | Decay functions for episodic logs (e.g., weighted attention) | What decay schedule best balances relevance and storage cost in long‑term chat logs? |\\n| **Schema‑Based Retrieval** | Pre‑defined conversation schemas (e.g., “booking flight”) that guide retrieval | Can we learn schemas from raw logs and use them to prune the retrieval space? |\\n| **Metacognition (knowing what you know)** | Confidence calibration, self‑ask‑and‑answer loops | How to integrate calibrated confidence into turn‑taking decisions (e.g., ask clarification vs. answer)? |\\n| **Prospective Memory Triggers** | Event‑based reminders (e.g., “when you mention X, ask Y”) | How to detect natural language cues that should schedule a future action? |\\n| **Social Memory (shared history, common ground)** | Shared user‑group embeddings, community‑level facts | How to maintain a “common ground” across many users without leaking private data? |\\n| **Emotion‑linked Memory** | Sentiment‑tagged episodic entries | Does storing affective tags improve empathetic response generation? |\\n\\n### Practical Tips for Your Research\\n\\n1. **Start Small, Scale Gradually**  \\n   - Prototype a **session‑level buffer** (e.g., SQLite table) and a **RAG retriever** (e.g., FAISS).  \\n   - Measure **Recall@5** for retrieving the correct prior turn when you ask a follow‑up question.\\n\\n2. **Instrument Meta‑Memory Early**  \\n   - Log the model’s token‑level log‑probabilities or use a calibrated classifier to get a confidence score.  \\n   - Use this signal to decide when to **fallback to retrieval** or **ask clarification**.\\n\\n3. **Design a Minimal “Episodic + Semantic” Schema**  \\n   - Store each turn as raw text **plus** a set of extracted triples (subject‑predicate‑object).  \\n   - Tools: spaCy, OpenAI function‑calling, or a lightweight OpenIE pipeline.  \\n   - This dual store lets you retrieve exact phrasing *or* abstract facts.\\n\\n4. **Privacy‑First Architecture**  \\n   - Encrypt user‑profile tables at rest.  \\n   - Use **differential privacy** when aggregating community‑level statistics.  \\n   - Provide a clear **data‑deletion API** (GDPR/CCPA compliance).\\n\\n5. **Benchmark Against Human Baselines**  \\n   - Collect a small corpus where humans answer follow‑up questions using only the last *N* turns vs. the full transcript.  \\n   - Compare AI performance to see how much “memory” is actually needed for a given task.\\n\\n6. **Explore “Memory‑Augmented Transformers”**  \\n   - Libraries like **Memorizing Transformers** (Google) or **Longformer‑Encoder‑Decoder** let you attend over thousands of tokens with a sliding‑window + global memory.  \\n   - Test whether adding a **learned memory bank** improves factual consistency in multi‑turn QA.\\n\\n---\\n\\n## Quick Reference Cheat‑Sheet\\n\\n| Memory Type | Human Analogue | AI Implementation | Typical Use‑Case |\\n|-------------|----------------|-------------------|------------------|\\n| **Working / Short‑Term** | Holding the last 2–3 utterances | Context window (4\\u202fk tokens) | Immediate turn‑taking |\\n| **Episodic** | Remembering “what we talked about earlier” | Session logs + vector store of turn embeddings | Follow‑up questions, continuity |\\n| **Semantic** | General world knowledge | Knowledge base / RAG corpus | Fact‑grounded answers |\\n| **Procedural** | Politeness formulas, turn‑taking rules | Prompt templates, system messages | Consistent style, safety |\\n| **Prospective** | “I’ll remind you later” | Scheduler + pending‑task queue | Reminders, task‑oriented dialogue |\\n| **Meta‑Memory** | Confidence in one’s own answer | Log‑probabilities, calibrated confidence scores | Clarification requests, self‑correction |\\n\\n---\\n\\n### Suggested Reading List\\n\\n| Category | Title | Authors / Source | Year |\\n|----------|-------|------------------|------|\\n| **Cognitive Foundations** | *Working Memory* | Baddeley & Hitch | 1974 |\\n| | *Episodic Memory* | Tulving | 1972 |\\n| | *The Forgetting Curve* | Ebbinghaus | 1885 |\\n| **Conversational AI** | *Retrieval‑Augmented Generation for Knowledge‑Intensive NLP* | Lewis et al. (FAIR) | 2020 |\\n| | *Long‑Context Transformers* (Longformer, BigBird) | Beltagy et\\u202fal., Zaheer et\\u202fal. | 2020‑2021 |\\n| | *Memory‑Augmented Neural Networks* | Graves et\\u202fal. (Neural Turing Machines) | 2014 |\\n| **Evaluation** | *Coherence Modeling for Dialogue* | Liu et\\u202fal. (ACL) | 2022 |\\n| | *Calibration of Language Models* | Zhao et\\u202fal. (NeurIPS) | 2021 |\\n| **Ethics & Privacy** | *Differential Privacy for Language Models* | McMahan et\\u202fal. | 2023 |\\n\\n---\\n\\n## TL;DR Summary\\n\\n- **Human conversation** draws on multiple memory systems (working, episodic, semantic, procedural, prospective, meta‑memory).  \\n- **Conversational AI** mirrors these with a stack: short‑term context window → session‑level logs → persistent user profiles → external knowledge retrieval → learned neural memory modules.  \\n- **Key research frontiers**: chunking & forgetting strategies, calibrated meta‑memory, prospective‑memory triggers, privacy‑preserving shared memory, and hybrid episodic‑semantic stores.  \\n\\nIf you’d like a deeper dive into any specific architecture (e.g., how to build a vector‑store‑backed episodic memory) or want help designing an experiment to compare human vs. model memory performance, just let me know!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 3082, 'prompt_tokens': 119, 'total_tokens': 3201, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 163, 'rejected_prediction_tokens': None, 'image_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0, 'video_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_name': 'openai/gpt-oss-120b:free', 'system_fingerprint': None, 'id': 'gen-1766228329-4RnCRvx3ctJOAoG0yuxN', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--7bd9deb1-5707-4da8-b27d-b199b3e33299-0', usage_metadata={'input_tokens': 119, 'output_tokens': 3082, 'total_tokens': 3201, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'reasoning': 163}})]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"I'm researching the different types of conversational memory.\"},\n",
    "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
    ")\n",
    "\n",
    "chat_map[\"id_123\"].messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e367f9b",
   "metadata": {},
   "source": [
    "Let's continue the conversation and see if the summary is updated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7858922c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: AttributeError(\"'list' object has no attribute 'content'\")\n",
      "Error in RootListenersTracer.on_chain_end callback: AttributeError(\"'list' object has no attribute 'content'\")\n",
      "Error in RootListenersTracer.on_chain_end callback: AttributeError(\"'list' object has no attribute 'content'\")\n"
     ]
    }
   ],
   "source": [
    "for msg in [\n",
    "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
    "    \"Buffer memory just stores the entire conversation\",\n",
    "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
    "]:\n",
    "    pipeline_with_history.invoke(\n",
    "        {\"query\": msg},\n",
    "        config={\"session_id\": \"id_123\", \"llm\": llm}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "97756864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi, my name is Dat', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Hello, Dat! Nice to meet you. How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 55, 'prompt_tokens': 86, 'total_tokens': 141, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 29, 'rejected_prediction_tokens': None, 'image_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0, 'video_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_name': 'openai/gpt-oss-120b:free', 'system_fingerprint': None, 'id': 'gen-1766228326-N1OwE2oIHjwTux0gpoCk', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--cded2bb4-af02-4efc-804a-a31ad80c8540-0', usage_metadata={'input_tokens': 86, 'output_tokens': 55, 'total_tokens': 141, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'reasoning': 29}}),\n",
       " HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Below is a compact yet comprehensive guide to the **different types of conversational memory**—both from a **human‑cognitive** perspective and from the **AI/LLM** side of things. I’ve organized the material into three sections:\\n\\n1. **Human memory systems that shape conversation**  \\n2. **Memory architectures used in conversational AI**  \\n3. **How the two worlds intersect & practical take‑aways for research**\\n\\nFeel free to let me know which part you’d like to dive deeper into (e.g., technical implementations, psychological studies, evaluation metrics, etc.).\\n\\n---\\n\\n## 1. Human Memory Systems that Shape Conversation\\n\\n| Memory Type | Core Characteristics | Role in Conversation | Typical Capacity / Duration | Key References |\\n|-------------|----------------------|----------------------|-----------------------------|----------------|\\n| **Sensory Memory** | Brief (≤\\u202f1\\u202fs) storage of raw sensory input (iconic, echoic) | Allows us to “catch” the last word or tone before we parse it | ~4–12\\u202fbits per modality | Sperling (1960) |\\n| **Short‑Term / Working Memory** | Actively maintained, limited (≈\\u202f4\\u202f±\\u202f1 “chunks”) | Holds the current turn, recent utterances, and any mental “scratchpad” for planning the next response | ~10–30\\u202fs without rehearsal | Baddeley & Hitch (1974) |\\n| **Episodic Memory (Explicit Long‑Term)** | Autobiographical events with context (time, place, emotions) | Enables us to refer back to past interactions (“Remember when you told me…”) | Potentially lifelong, but retrieval is cue‑dependent | Tulving (1972) |\\n| **Semantic Memory (Explicit Long‑Term)** | General world knowledge, facts, concepts | Supplies background knowledge (“Paris is the capital of France”) | Lifelong, highly organized | Anderson (1976) |\\n| **Procedural / Implicit Memory** | Skills, habits, conditioned responses | Drives conversational fluency (e.g., turn‑taking, politeness formulas) without conscious effort | Lifelong, often inaccessible to introspection | Reber (1993) |\\n| **Prospective Memory** | Remembering to do something in the future (e.g., “Ask me about the report tomorrow”) | Supports follow‑up questions, reminders, commitments | Can be triggered by internal or external cues | Einstein & McDaniel (1990) |\\n| **Meta‑Memory** | Awareness of what we know/forget | Guides self‑monitoring (“I’m not sure I understood that”) | Variable; often inaccurate | Koriat (2007) |\\n\\n### Why These Matter for Conversational Research\\n- **Turn‑taking** relies on working‑memory buffers that keep the last few utterances online.  \\n- **Coherence** (maintaining topic, referents, and discourse relations) draws on episodic recall of the dialogue history.  \\n- **Politeness & Pragmatics** are largely procedural—learned patterns that run automatically.  \\n- **Error recovery** often involves meta‑memory (“Did I just say that?”) and prospective memory (“I need to ask a clarification later”).\\n\\n---\\n\\n## 2. Memory Architectures in Conversational AI\\n\\n| Architecture | What It Stores | Typical Scope (tokens/records) | Retrieval Mechanism | Strengths | Weaknesses / Open Challenges |\\n|--------------|----------------|-------------------------------|---------------------|----------|------------------------------|\\n| **Stateless / Context‑Window Only** | The most recent *N* tokens (e.g., 4\\u202fk‑token window) | 2–8\\u202fk tokens (≈\\u202f3–5\\u202fk words) | Sliding window; no explicit retrieval | Simplicity; low latency | Forgetting beyond window; no long‑term personalization |\\n| **Session‑Level Memory** | All turns of a single user session (often stored in a DB) | Up to hundreds of turns (≈\\u202f10–30\\u202fk tokens) | Linear scan or indexed by turn‑id | Keeps conversation coherent across a session | Still limited by model context; scaling to many concurrent sessions can be costly |\\n| **User‑Profile / Persistent Memory** | Structured attributes (name, preferences, past purchases) + optional raw dialogue snippets | Unlimited (persisted in relational/NoSQL store) | Key‑value lookup, rule‑based or learned retrieval | Enables personalization across sessions | Requires schema design; privacy & data‑governance concerns |\\n| **Retrieval‑Augmented Generation (RAG)** | External knowledge base (documents, FAQs, web snippets) + optional dialogue excerpts | Millions of documents (vector‑indexed) | Approximate nearest‑neighbor (ANN) search on embeddings | Provides up‑to‑date factual grounding; bypasses context‑window limits | Retrieval latency; hallucination if retrieved text is mis‑interpreted |\\n| **Memory‑Network / Differentiable Neural Memory** | Learned latent memory slots (e.g., Neural Turing Machine, Memory‑augmented Transformer) | Typically 64–512 slots, each 64–1024‑dim vectors | Content‑based addressing via attention | End‑to‑end trainable; can store abstract “facts” | Hard to interpret; scaling to real‑world dialogue size is still experimental |\\n| **Hybrid “Episodic + Semantic” Stores** | *Episodic*: raw turn‑by‑turn logs; *Semantic*: extracted triples, intents, sentiment | Episodic: unlimited; Semantic: compact graph (hundreds of nodes) | Graph queries + vector search | Supports both recall of exact phrasing and abstract reasoning | Requires robust information extraction pipelines |\\n| **Prospective Memory Modules** | Future actions or reminders (e.g., “remind me in 2\\u202fh”) | Small queue of pending tasks | Scheduler + trigger detection (time or cue) | Enables follow‑up actions, task‑oriented dialogue | Needs reliable time‑keeping & user‑consent handling |\\n| **Meta‑Memory / Confidence Estimation** | Model’s self‑estimated certainty per token/utterance | One scalar per token (or per turn) | Directly from model logits or calibrated post‑hoc | Allows the system to ask clarifying questions when confidence is low | Calibration remains an open research problem |\\n\\n### Typical Pipeline (RAG‑style)  \\n\\n```\\nUser Input → Embedding (e.g., OpenAI ada) → Vector DB Retrieval (top‑k) → \\nRerank (cross‑encoder) → Concatenate retrieved passages + recent dialogue → \\nLLM Prompt → Model Generation → (Optional) Post‑process (filter, safety)\\n```\\n\\n### Evaluation Metrics for Conversational Memory\\n\\n| Metric | What It Measures | How to Compute |\\n|--------|------------------|----------------|\\n| **Recall@k (R@k)** | Ability to retrieve the correct past turn or fact | Count of times the ground‑truth turn appears in top‑k retrieved items |\\n| **Mean Reciprocal Rank (MRR)** | Rank quality of the first relevant item | 1 / rank of first relevant result, averaged |\\n| **Coherence Scores (e.g., Entity Grid, Coherence Model)** | Global discourse coherence across turns | Compare model‑generated dialogue to human‑annotated coherence |\\n| **Personalization Accuracy** | Correct use of user‑profile facts | % of turns where a known user attribute is correctly referenced |\\n| **Prospective Memory Success Rate** | Whether scheduled reminders are delivered | % of pending tasks that fire at the right time |\\n| **Meta‑Memory Calibration (ECE, Brier Score)** | Alignment between confidence and actual correctness | Plot confidence vs. accuracy; compute Expected Calibration Error |\\n\\n---\\n\\n## 3. Bridging Human and AI Memory: Research Angles\\n\\n| Human Concept | AI Analogue | Open Research Questions |\\n|---------------|-------------|--------------------------|\\n| **Chunking (grouping items into meaningful units)** | Token‑level vs. “semantic chunk” embeddings; hierarchical context windows | How can we automatically discover optimal chunk boundaries for dialogue? |\\n| **Forgetting Curve (Ebbinghaus)** | Decay functions for episodic logs (e.g., weighted attention) | What decay schedule best balances relevance and storage cost in long‑term chat logs? |\\n| **Schema‑Based Retrieval** | Pre‑defined conversation schemas (e.g., “booking flight”) that guide retrieval | Can we learn schemas from raw logs and use them to prune the retrieval space? |\\n| **Metacognition (knowing what you know)** | Confidence calibration, self‑ask‑and‑answer loops | How to integrate calibrated confidence into turn‑taking decisions (e.g., ask clarification vs. answer)? |\\n| **Prospective Memory Triggers** | Event‑based reminders (e.g., “when you mention X, ask Y”) | How to detect natural language cues that should schedule a future action? |\\n| **Social Memory (shared history, common ground)** | Shared user‑group embeddings, community‑level facts | How to maintain a “common ground” across many users without leaking private data? |\\n| **Emotion‑linked Memory** | Sentiment‑tagged episodic entries | Does storing affective tags improve empathetic response generation? |\\n\\n### Practical Tips for Your Research\\n\\n1. **Start Small, Scale Gradually**  \\n   - Prototype a **session‑level buffer** (e.g., SQLite table) and a **RAG retriever** (e.g., FAISS).  \\n   - Measure **Recall@5** for retrieving the correct prior turn when you ask a follow‑up question.\\n\\n2. **Instrument Meta‑Memory Early**  \\n   - Log the model’s token‑level log‑probabilities or use a calibrated classifier to get a confidence score.  \\n   - Use this signal to decide when to **fallback to retrieval** or **ask clarification**.\\n\\n3. **Design a Minimal “Episodic + Semantic” Schema**  \\n   - Store each turn as raw text **plus** a set of extracted triples (subject‑predicate‑object).  \\n   - Tools: spaCy, OpenAI function‑calling, or a lightweight OpenIE pipeline.  \\n   - This dual store lets you retrieve exact phrasing *or* abstract facts.\\n\\n4. **Privacy‑First Architecture**  \\n   - Encrypt user‑profile tables at rest.  \\n   - Use **differential privacy** when aggregating community‑level statistics.  \\n   - Provide a clear **data‑deletion API** (GDPR/CCPA compliance).\\n\\n5. **Benchmark Against Human Baselines**  \\n   - Collect a small corpus where humans answer follow‑up questions using only the last *N* turns vs. the full transcript.  \\n   - Compare AI performance to see how much “memory” is actually needed for a given task.\\n\\n6. **Explore “Memory‑Augmented Transformers”**  \\n   - Libraries like **Memorizing Transformers** (Google) or **Longformer‑Encoder‑Decoder** let you attend over thousands of tokens with a sliding‑window + global memory.  \\n   - Test whether adding a **learned memory bank** improves factual consistency in multi‑turn QA.\\n\\n---\\n\\n## Quick Reference Cheat‑Sheet\\n\\n| Memory Type | Human Analogue | AI Implementation | Typical Use‑Case |\\n|-------------|----------------|-------------------|------------------|\\n| **Working / Short‑Term** | Holding the last 2–3 utterances | Context window (4\\u202fk tokens) | Immediate turn‑taking |\\n| **Episodic** | Remembering “what we talked about earlier” | Session logs + vector store of turn embeddings | Follow‑up questions, continuity |\\n| **Semantic** | General world knowledge | Knowledge base / RAG corpus | Fact‑grounded answers |\\n| **Procedural** | Politeness formulas, turn‑taking rules | Prompt templates, system messages | Consistent style, safety |\\n| **Prospective** | “I’ll remind you later” | Scheduler + pending‑task queue | Reminders, task‑oriented dialogue |\\n| **Meta‑Memory** | Confidence in one’s own answer | Log‑probabilities, calibrated confidence scores | Clarification requests, self‑correction |\\n\\n---\\n\\n### Suggested Reading List\\n\\n| Category | Title | Authors / Source | Year |\\n|----------|-------|------------------|------|\\n| **Cognitive Foundations** | *Working Memory* | Baddeley & Hitch | 1974 |\\n| | *Episodic Memory* | Tulving | 1972 |\\n| | *The Forgetting Curve* | Ebbinghaus | 1885 |\\n| **Conversational AI** | *Retrieval‑Augmented Generation for Knowledge‑Intensive NLP* | Lewis et al. (FAIR) | 2020 |\\n| | *Long‑Context Transformers* (Longformer, BigBird) | Beltagy et\\u202fal., Zaheer et\\u202fal. | 2020‑2021 |\\n| | *Memory‑Augmented Neural Networks* | Graves et\\u202fal. (Neural Turing Machines) | 2014 |\\n| **Evaluation** | *Coherence Modeling for Dialogue* | Liu et\\u202fal. (ACL) | 2022 |\\n| | *Calibration of Language Models* | Zhao et\\u202fal. (NeurIPS) | 2021 |\\n| **Ethics & Privacy** | *Differential Privacy for Language Models* | McMahan et\\u202fal. | 2023 |\\n\\n---\\n\\n## TL;DR Summary\\n\\n- **Human conversation** draws on multiple memory systems (working, episodic, semantic, procedural, prospective, meta‑memory).  \\n- **Conversational AI** mirrors these with a stack: short‑term context window → session‑level logs → persistent user profiles → external knowledge retrieval → learned neural memory modules.  \\n- **Key research frontiers**: chunking & forgetting strategies, calibrated meta‑memory, prospective‑memory triggers, privacy‑preserving shared memory, and hybrid episodic‑semantic stores.  \\n\\nIf you’d like a deeper dive into any specific architecture (e.g., how to build a vector‑store‑backed episodic memory) or want help designing an experiment to compare human vs. model memory performance, just let me know!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 3082, 'prompt_tokens': 119, 'total_tokens': 3201, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 163, 'rejected_prediction_tokens': None, 'image_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0, 'video_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_name': 'openai/gpt-oss-120b:free', 'system_fingerprint': None, 'id': 'gen-1766228329-4RnCRvx3ctJOAoG0yuxN', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--7bd9deb1-5707-4da8-b27d-b199b3e33299-0', usage_metadata={'input_tokens': 119, 'output_tokens': 3082, 'total_tokens': 3201, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'reasoning': 163}}),\n",
       " HumanMessage(content='I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Below is a **complete, practical guide** to the two most‑used LangChain memory classes you mentioned:\\n\\n| Class | What it does | How it stores data | When it *fails* (i.e., you’ll hit a limitation) | Typical use‑cases |\\n|-------|--------------|-------------------|-----------------------------------------------|-------------------|\\n| **`ConversationBufferMemory`** | Keeps **the entire dialogue history** (all user‑assistant turns) in a single string that is appended to the prompt each time. | `self.chat_memory.messages` → a list of `HumanMessage` / `AIMessage`. When the memory is queried (`load_memory_variables`), LangChain concatenates the list into a single string (or a list of messages, depending on `return_messages`). | • The concatenated string grows without bound → eventually exceeds the model’s context window (e.g., 4\\u202fk tokens for GPT‑3.5‑turbo). <br>• No built‑in way to “forget” old turns, so you must prune manually or switch to a window‑based memory. | • Short‑lived bots (single‑session Q&A, tutoring, debugging) where you *want* the model to see everything it ever said. <br>• When you need a **complete audit trail** for debugging or compliance. |\\n| **`ConversationBufferWindowMemory`** | Keeps only the **most recent *k* turns** (default `k=2` meaning the last 2 user‑assistant exchanges). | Same internal list as above, but `load_memory_variables` slices the list to the last `k` pairs before concatenation. | • If `k` is too small, the model may lose essential context (e.g., a name introduced 5 turns ago). <br>• Still limited by the model’s context window, but the risk is far lower. | • Long‑running agents where you need to stay within a token budget (customer‑support bots, multi‑step tool‑use agents). <br>• Situations where you deliberately want the model to “forget” older details (privacy‑by‑design, GDPR‑style data minimisation). |\\n\\n---\\n\\n## 1. How the two classes are implemented (simplified)\\n\\n```python\\n# langchain/memory/buffer.py  (excerpt)\\n\\nclass ConversationBufferMemory(BaseMemory):\\n    def __init__(self, memory_key=\"history\", return_messages=False, **kwargs):\\n        self.chat_memory = ChatMessageHistory()\\n        self.memory_key = memory_key\\n        self.return_messages = return_messages\\n\\n    def load_memory_variables(self, inputs: dict) -> dict:\\n        # 1️⃣ Get the raw list of messages\\n        msgs = self.chat_memory.messages\\n\\n        # 2️⃣ Optionally convert to a single string\\n        if self.return_messages:\\n            # returns [{\"role\": \"human\", \"content\": \"...\"} , ...]\\n            history = msgs\\n        else:\\n            # concatenate with a separator (default \"\\\\n\")\\n            history = \"\\\\n\".join([m.content for m in msgs])\\n\\n        return {self.memory_key: history}\\n```\\n\\n```python\\n# langchain/memory/buffer_window.py  (excerpt)\\n\\nclass ConversationBufferWindowMemory(ConversationBufferMemory):\\n    def __init__(self, k=2, **kwargs):\\n        super().__init__(**kwargs)\\n        self.k = k                     # number of *pairs* to keep\\n\\n    def load_memory_variables(self, inputs: dict) -> dict:\\n        # Grab the last k *pairs* (human+AI) from the full list\\n        msgs = self.chat_memory.messages[-self.k * 2 :]   # 2 msgs per turn\\n        if self.return_messages:\\n            history = msgs\\n        else:\\n            history = \"\\\\n\".join([m.content for m in msgs])\\n        return {self.memory_key: history}\\n```\\n\\n*Key take‑away*: **Both classes share the same underlying `ChatMessageHistory`**; the only difference is the slicing logic in `load_memory_variables`.\\n\\n---\\n\\n## 2. When to pick one over the other\\n\\n| Situation | Recommended Memory | Why |\\n|-----------|--------------------|-----|\\n| **One‑off Q&A** (single user, no follow‑up) | `ConversationBufferMemory` (or even no memory) | The history is tiny; you can afford the full transcript. |\\n| **Multi‑turn tutoring** where the student may refer back to a concept introduced 8 turns ago | `ConversationBufferMemory` **or** a *custom* buffer that stores *semantic* summaries (see Section\\u202f4) | You need the model to see the original definition. |\\n| **Customer‑support bot that must stay under 2\\u202fk tokens** | `ConversationBufferWindowMemory(k=3‑5)` | Guarantees you never exceed the window while still giving the model enough recent context. |\\n| **Legal‑compliance / audit** (you must retain the full conversation) | `ConversationBufferMemory` + external persistence (e.g., write to a DB after each turn) | The memory object itself is in‑process, but you can dump it to durable storage. |\\n| **Privacy‑by‑design** (you must delete older turns after 24\\u202fh) | `ConversationBufferWindowMemory` **or** a *time‑based* custom memory (see Section\\u202f5) | The window automatically discards old data; you can also add a TTL. |\\n| **Tool‑using agents** (e.g., ReAct, self‑ask) that need to remember the *last* tool call and its result | `ConversationBufferWindowMemory(k=1‑2)` | Only the most recent tool interaction matters; older tool calls clutter the prompt. |\\n\\n---\\n\\n## 3. Quick “real‑code” demos\\n\\n### 3.1. Basic `ConversationBufferMemory`\\n\\n```python\\nfrom langchain.memory import ConversationBufferMemory\\nfrom langchain.llms import OpenAI\\nfrom langchain.chains import LLMChain\\nfrom langchain.prompts import PromptTemplate\\n\\n# 1️⃣ Prompt that expects a `history` variable\\nprompt = PromptTemplate(\\n    input_variables=[\"history\", \"question\"],\\n    template=\"\"\"\\nYou are a helpful assistant. Use the conversation history below to answer the new question.\\n\\nHistory:\\n{history}\\n\\nQuestion: {question}\\nAnswer:\"\"\",\\n)\\n\\n# 2️⃣ Memory + LLM\\nmemory = ConversationBufferMemory(memory_key=\"history\")\\nllm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\\nchain = LLMChain(llm=llm, prompt=prompt, memory=memory)\\n\\n# 3️⃣ Interact\\nprint(chain.run(\"What is the capital of France?\"))\\n# → Paris\\n\\nprint(chain.run(\"And what is its population?\"))\\n# The model now sees both turns, so it can answer with a more informed response.\\n```\\n\\n### 3.2. `ConversationBufferWindowMemory` (keep last 2 turns)\\n\\n```python\\nfrom langchain.memory import ConversationBufferWindowMemory\\n\\nmemory = ConversationBufferWindowMemory(k=2, memory_key=\"history\")\\nchain = LLMChain(llm=llm, prompt=prompt, memory=memory)\\n\\n# Simulate a longer dialogue\\nchain.run(\"Tell me a joke about cats.\")\\nchain.run(\"Now explain why cats are good for mental health.\")\\nchain.run(\"What about dogs?\")   # <-- only the last 2 pairs are visible to the model\\n```\\n\\n**Result:** The model will *not* see the very first cat joke when answering about dogs, which keeps the prompt short.\\n\\n---\\n\\n## 4. Extending the built‑in buffers (when the vanilla versions aren’t enough)\\n\\n| Goal | How to achieve it (code sketch) |\\n|------|---------------------------------|\\n| **Summarize older turns** (keep full history but compress the early part) | ```python\\\\nclass SummarizingConversationMemory(ConversationBufferMemory):\\\\n    def __init__(self, summarizer, max_tokens=3000, **kw):\\\\n        super().__init__(**kw)\\\\n        self.summarizer = summarizer\\\\n        self.max_tokens = max_tokens\\\\n    def load_memory_variables(self, inputs):\\\\n        # 1️⃣ If the raw string is > max_tokens, replace the *oldest* half with a summary.\\\\n        raw = super().load_memory_variables(inputs)[self.memory_key]\\\\n        if token_len(raw) > self.max_tokens:\\\\n            # split into two halves\\\\n            first, second = raw.split(\\'\\\\\\\\n\\', 1)\\\\n            summary = self.summarizer.run(first)   # call LLM to summarize\\\\n            raw = summary + \\'\\\\\\\\n\\' + second\\\\n        return {self.memory_key: raw}\\\\n``` |\\n| **Time‑based eviction** (delete anything older than X minutes) | ```python\\\\nfrom datetime import datetime, timedelta\\\\nclass TTLConversationMemory(ConversationBufferMemory):\\\\n    def __init__(self, ttl_seconds=86400, **kw):\\\\n        super().__init__(**kw)\\\\n        self.ttl = timedelta(seconds=ttl_seconds)\\\\n        self.timestamps = []   # parallel list of datetime objects\\\\n    def save_context(self, inputs, outputs):\\\\n        super().save_context(inputs, outputs)\\\\n        self.timestamps.append(datetime.utcnow())\\\\n    def load_memory_variables(self, inputs):\\\\n        # prune\\\\n        now = datetime.utcnow()\\\\n        keep = [i for i, t in enumerate(self.timestamps) if now - t < self.ttl]\\\\n        self.chat_memory.messages = [self.chat_memory.messages[i] for i in keep]\\\\n        self.timestamps = [self.timestamps[i] for i in keep]\\\\n        return super().load_memory_variables(inputs)\\\\n``` |\\n| **Hybrid episodic + semantic store** (retrieve by *entity* instead of raw text) | Use a vector DB (FAISS, Chroma, Pinecone) to index each turn’s embedding, then query it with the current user utterance. The retrieved snippets are concatenated with the window buffer. <br>**Pseudo‑code**: <br>```python\\\\nfrom langchain.embeddings import OpenAIEmbeddings\\\\nfrom langchain.vectorstores import FAISS\\\\n\\\\nemb = OpenAIEmbeddings()\\\\nvectorstore = FAISS(embedding_function=emb)\\\\n\\\\nclass RetrievalConversationMemory(ConversationBufferWindowMemory):\\\\n    def __init__(self, vectorstore, k=2, retrieve_k=3, **kw):\\\\n        super().__init__(k=k, **kw)\\\\n        self.vs = vectorstore\\\\n        self.retrieve_k = retrieve_k\\\\n    def load_memory_variables(self, inputs):\\\\n        # 1️⃣ Get recent window as usual\\\\n        base = super().load_memory_variables(inputs)[self.memory_key]\\\\n        # 2️⃣ Retrieve similar past turns\\\\n        query = inputs.get(\\'question\\') or \\'\\'\\\\n        docs = self.vs.similarity_search(query, k=self.retrieve_k)\\\\n        retrieved = \\\\\"\\\\\\\\n---\\\\\\\\n\\\\\".join([d.page_content for d in docs])\\\\n        return {self.memory_key: base + \\\\\"\\\\\\\\n---\\\\\\\\n\\\\\" + retrieved}\\\\n``` |\\n| **Memory‑augmented neural module** (learnable slots) | Use `langchain.memory.ChatMessageHistory` + a **MemoryNetwork** from `transformers` or `torch`. This is more research‑oriented; you would replace the string concatenation with a learned vector that you feed into the model as an extra “memory” token. See the “Long‑Context Transformers” section below. |\\n\\n\\n---\\n\\n## 5. Token‑budget math (so you can decide *k* analytically)\\n\\nAssume:\\n\\n* Model context = **C** tokens (e.g., 4096 for `gpt-3.5‑turbo`).\\n* System prompt + chain prompt = **P** tokens (usually 50‑200).\\n* Average user turn = **U** tokens.\\n* Average assistant turn = **A** tokens.\\n\\nFor a window memory with `k` pairs:\\n\\n```\\nTotal tokens ≈ P + (k * (U + A))\\n```\\n\\nSolve for `k`:\\n\\n```\\nk ≤ (C - P) / (U + A)\\n```\\n\\n**Example**  \\nC = 4096, P = 150, U ≈ 30, A ≈ 70 → (U+A)=100  \\n\\n```\\nk ≤ (4096-150)/100 ≈ 39\\n```\\n\\nSo you could safely keep **up to 39 recent pairs**. In practice you leave a safety margin (≈\\u202f10\\u202f% extra) → **k ≈ 35**.  \\n\\nIf you notice the model starts truncating the prompt (LangChain will raise a `TokenLimitError`), lower `k` or switch to a summarizing memory.\\n\\n---\\n\\n## 6. Common pitfalls & how to avoid them\\n\\n| Pitfall | Symptom | Fix |\\n|---------|---------|-----|\\n| **Memory grows beyond the model’s context** | `TokenLimitError` or the model silently drops the earliest part of the prompt. | Switch to `ConversationBufferWindowMemory`, add a summarizer, or enforce a hard token limit (`max_token_limit` in LangChain’s `LLMChain`). |\\n| **User‑specific data leaks across sessions** | A user sees information from a previous user because the same `ConversationBufferMemory` instance is reused. | Instantiate a **new memory object per session** (or per user ID) and store it in a session dict. |\\n| **Prompt becomes unreadable** (lots of `\\\\n` separators) | Model starts answering “I’m not sure what you’re asking.” | Use a **structured prompt** (e.g., `{{history}}` formatted as a list of `Human:` / `AI:` lines) or set `return_messages=True` and let LangChain render a message list. |\\n| **Privacy violation** (old personal data retained) | GDPR audit flags that you keep data longer than needed. | Use `ConversationBufferWindowMemory` with a small `k`, or implement a TTL memory (see Section\\u202f4). |\\n| **Memory not updated** (assistant repeats the same answer) | The same user question yields identical responses despite earlier clarification. | Ensure you call `memory.save_context(inputs, outputs)` after each turn (LangChain’s `LLMChain` does this automatically, but custom loops need it). |\\n| **Embedding‑based retrieval returns unrelated turns** | Retrieval‑augmented memory pulls irrelevant snippets, confusing the model. | Clean the text before indexing, use a **cross‑encoder reranker**, and limit `retrieve_k` to a small number (1‑3). |\\n\\n---\\n\\n## 7. Choosing a “future‑proof” architecture\\n\\nIf you anticipate **any of the following**:\\n\\n* Scaling to **hundreds of concurrent users**,\\n* Need to **persist** history across restarts,\\n* Want to **search** old conversations (e.g., “What did I ask about my order on 2024‑11‑02?”),\\n\\nthen consider **decoupling the memory from the in‑process object**:\\n\\n1. **Persist the `ChatMessageHistory`** in a database (PostgreSQL, MongoDB, DynamoDB).  \\n   ```python\\n   from langchain.memory import ConversationBufferMemory\\n   from langchain.schema import BaseChatMessageHistory\\n\\n   class SQLChatMessageHistory(BaseChatMessageHistory):\\n       def __init__(self, session_id, db):\\n           self.session_id = session_id\\n           self.db = db\\n       # implement add_message, clear, get_messages …\\n   ```\\n\\n2. **Wrap it** in a LangChain memory class:\\n   ```python\\n   memory = ConversationBufferMemory(chat_memory=SQLChatMessageHistory(session_id, db))\\n   ```\\n\\n3. **Add a vector store** for fast semantic lookup (FAISS/Chroma) and combine it with a window buffer (see the hybrid example in Section\\u202f4).\\n\\nThis pattern gives you:\\n\\n* **Unlimited archival** (the DB can store years of chats).  \\n* **Selective retrieval** (semantic search, time‑range queries).  \\n* **Compliance** (you can delete a user’s rows on request).  \\n\\n---\\n\\n## 8. Quick checklist for your next experiment\\n\\n| ✅ | Item |\\n|----|------|\\n| **[ ]** Define the *maximum* token budget you can afford per turn (model context – system prompt). |\\n| **[ ]** Choose a memory class: <br>‑ `ConversationBufferMemory` if you need *full* history. <br>‑ `ConversationBufferWindowMemory(k=…)` if you need a *hard* bound. |\\n| **[ ]** If you need *more* than a raw window, decide whether you’ll: <br>‑ Summarize old turns, <br>‑ Store them in a vector DB, <br>‑ Use a TTL‑based custom memory. |\\n| **[ ]** Implement a **per‑session** memory store (dictionary keyed by `session_id`). |\\n| **[ ]** Add **logging** of token counts (`len(tokenizer.encode(prompt))`) to catch overflow early. |\\n| **[ ]** Write a small unit test that simulates a 30‑turn conversation and asserts that the final prompt length ≤ model context. |\\n| **[ ]** (Optional) Add a **confidence‑calibration** step: after generation, compute `logprob`‑based confidence; if below a threshold, ask the user for clarification. |\\n| **[ ]** Verify **privacy compliance**: can you delete a user’s memory with a single API call? |\\n\\n---\\n\\n## 9. TL;DR – One‑sentence answer\\n\\n- **`ConversationBufferMemory`** = *keep everything* (great for short sessions, audits, but can blow up the token budget).  \\n- **`ConversationBufferWindowMemory`** = *keep only the last\\u202fk turns* (keeps prompts small, ideal for long‑running bots, but you may lose older context).\\n\\nPick the one that matches **how much past context you truly need** versus **how many tokens you can afford**. If you need something in‑between, layer a summarizer, a vector‑store, or a TTL‑based custom memory on top of the basic buffer.\\n\\n---\\n\\n### Want a ready‑to‑run notebook?\\n\\nI can give you a minimal **Colab‑style** notebook that:\\n\\n1. Instantiates both memory types.  \\n2. Simulates a 20‑turn conversation.  \\n3. Prints token counts and shows when the window starts truncating.  \\n\\nJust let me know, and I’ll paste the full notebook code. Happy experimenting!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 4025, 'prompt_tokens': 3084, 'total_tokens': 7109, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 119, 'rejected_prediction_tokens': None, 'image_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0, 'video_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_name': 'openai/gpt-oss-120b:free', 'system_fingerprint': None, 'id': 'gen-1766228366-u1Ur6JSW6HxinONSWqBc', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--bbfa5565-fef4-4b6d-8347-c5e04ff5545e-0', usage_metadata={'input_tokens': 3084, 'output_tokens': 4025, 'total_tokens': 7109, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'reasoning': 119}}),\n",
       " HumanMessage(content='Buffer memory just stores the entire conversation', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Exactly—you’ve got it.\\u202f`ConversationBufferMemory` (the plain “buffer” version) simply appends every user‑assistant turn to an internal list and, when the chain runs, concatenates that whole list into the prompt. In other words, the model sees the *entire* dialogue history each time it generates a response.\\n\\nThat’s why it’s great for short‑lived sessions or when you need a complete audit trail, but it can quickly exceed a model’s context window if the conversation gets long. If you ever need to limit the size, you’d switch to a window‑based memory (e.g., `ConversationBufferWindowMemory`) or add a summarization/retention strategy.  \\n\\nLet me know if you’d like to see how to set up a window memory, add summarization, or anything else!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 232, 'prompt_tokens': 7020, 'total_tokens': 7252, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 72, 'rejected_prediction_tokens': None, 'image_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0, 'video_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_name': 'openai/gpt-oss-120b:free', 'system_fingerprint': None, 'id': 'gen-1766228402-Z5vcqiskvbUtjDPoGX6K', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--09529796-2450-4438-8899-75e5c6ca86b1-0', usage_metadata={'input_tokens': 7020, 'output_tokens': 232, 'total_tokens': 7252, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'reasoning': 72}}),\n",
       " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Exactly!\\u202f`ConversationBufferWindowMemory` is the “sliding‑window” version of the plain buffer: it keeps only the most recent\\u202f*k*\\u202fturns (a *turn* = one user message\\u202f+\\u202fone assistant reply) and discards everything older when the memory is queried.  \\n\\nBelow is a compact cheat‑sheet that shows **what the two classes do, how to configure them, and when each one is the right tool**. I’ve also added a few practical code snippets and some tips for extending the window‑memory pattern if you need something a little more sophisticated.\\n\\n---\\n\\n## 1. Quick Reference Table\\n\\n| Feature | `ConversationBufferMemory` | `ConversationBufferWindowMemory` |\\n|---------|----------------------------|-----------------------------------|\\n| **What is stored?** | **All** turns from the start of the session. | Only the **last\\u202fk** turns (default\\u202fk\\u202f=\\u202f2). |\\n| **How is it built?** | `chat_memory.messages` → concatenated into a single string (or list of messages). | Same list, but `load_memory_variables` slices `[-k*2:]` (2 messages per turn). |\\n| **Token growth** | Linear with conversation length → can exceed model context. | Bounded by `k` → predictable token budget. |\\n| **Typical use‑case** | Short Q&A, tutoring, audit logs, debugging. | Long‑running bots, customer‑support, tool‑using agents, GDPR‑style “forgetting”. |\\n| **Configuration knobs** | None (except `return_messages`, `memory_key`). | `k` (number of turns to keep), plus the same `return_messages`/`memory_key`. |\\n| **Pros** | Full context → model can reference anything said earlier. | Keeps prompt size small → safe for large conversations. |\\n| **Cons** | May hit token limits; no built‑in forgetting. | May lose older facts (e.g., a name introduced 10 turns ago). |\\n\\n---\\n\\n## 2. Minimal Working Example\\n\\n```python\\n# -------------------------------------------------\\n# 1️⃣  Install / import\\n# -------------------------------------------------\\n!pip install -q langchain openai   # run once in a fresh env\\nfrom langchain.llms import OpenAI\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.chains import LLMChain\\nfrom langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory\\n\\n# -------------------------------------------------\\n# 2️⃣  Prompt that expects a `history` variable\\n# -------------------------------------------------\\nprompt = PromptTemplate(\\n    input_variables=[\"history\", \"question\"],\\n    template=\"\"\"\\nYou are a helpful assistant. Use the conversation history below to answer the new question.\\n\\nHistory:\\n{history}\\n\\nQuestion: {question}\\nAnswer:\"\"\",\\n)\\n\\n# -------------------------------------------------\\n# 3️⃣  Choose the memory type\\n# -------------------------------------------------\\n# ---- Full buffer (keeps everything) ----\\nfull_mem = ConversationBufferMemory(memory_key=\"history\")\\n\\n# ---- Window buffer (keep last 3 turns) ----\\nwindow_mem = ConversationBufferWindowMemory(k=3, memory_key=\"history\")\\n\\n# -------------------------------------------------\\n# 4️⃣  Build two separate chains (they share the same LLM)\\n# -------------------------------------------------\\nllm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\\n\\nfull_chain   = LLMChain(llm=llm, prompt=prompt, memory=full_mem)\\nwindow_chain = LLMChain(llm=llm, prompt=prompt, memory=window_mem)\\n\\n# -------------------------------------------------\\n# 5️⃣  Run a short dialogue with each chain\\n# -------------------------------------------------\\ndef run_demo(chain, label):\\n    print(f\"\\\\n--- {label} ---\")\\n    print(chain.run(\"What is the capital of France?\"))\\n    print(chain.run(\"What is its population?\"))\\n    print(chain.run(\"Give me a fun fact about the Eiffel Tower.\"))\\n    print(chain.run(\"Now, what did we talk about earlier?\"))   # <-- this tests memory depth\\n\\nrun_demo(full_chain,   \"Full Buffer (keeps everything)\")\\nrun_demo(window_chain, \"Window Buffer (k=3 turns)\")\\n```\\n\\n**What you’ll see**\\n\\n* In the **Full Buffer** run, the last question (“Now, what did we talk about earlier?”) can reference *any* prior turn, because the entire transcript is still in the prompt.\\n* In the **Window Buffer** run, only the last three turns are visible. If the earlier “capital of France” turn falls outside the window, the model will answer “I don’t have that information” (or will guess), demonstrating the forgetting effect.\\n\\n---\\n\\n## 3. How to Pick a Good `k`\\n\\nA quick back‑of‑the‑envelope calculation helps you stay inside the model’s context window.\\n\\n```\\nC   = model context size (tokens)          # e.g. 4096 for gpt‑3.5‑turbo\\nP   = static prompt tokens (system + template)   # usually 100‑200\\nU   = average user turn length (tokens)   # 20‑40 for short questions\\nA   = average assistant turn length (tokens) # 30‑80 for typical answers\\nk   = number of *pairs* you want to keep\\n```\\n\\n**Maximum safe k**  \\n\\n```\\nk ≤ (C - P) / (U + A)\\n```\\n\\n*Example*:  \\nC = 4096, P = 150, U ≈ 30, A ≈ 70 → (U+A) = 100  \\n\\n```\\nk ≤ (4096 - 150) / 100 ≈ 39\\n```\\n\\nSo you could keep up to **~39 recent turns** without exceeding the token budget. In practice you leave a safety margin (≈\\u202f10\\u202f%) → **k ≈ 35**.  \\n\\nIf you notice the model starts truncating the prompt (LangChain will raise a `TokenLimitError`), lower `k` or switch to a summarizing memory (see Section\\u202f4).\\n\\n---\\n\\n## 4. Extending the Window Memory\\n\\nSometimes you need *more* than a raw sliding window but *less* than the full transcript. Below are three common patterns you can layer on top of `ConversationBufferWindowMemory`.\\n\\n### 4.1. Summarize Older Turns (Hybrid Full\\u202f+\\u202fWindow)\\n\\n```python\\nfrom langchain.memory import ConversationBufferMemory\\nfrom langchain.prompts import PromptTemplate\\n\\nclass SummarizingWindowMemory(ConversationBufferMemory):\\n    \"\"\"\\n    Keeps the last `k` pairs verbatim.\\n    Anything older is replaced by a short LLM‑generated summary.\\n    \"\"\"\\n    def __init__(self, summarizer, k=3, max_tokens=3000, **kwargs):\\n        super().__init__(**kwargs)\\n        self.summarizer = summarizer          # any LLMChain that returns a summary\\n        self.k = k\\n        self.max_tokens = max_tokens          # hard cap for the *full* prompt\\n\\n    def load_memory_variables(self, inputs):\\n        # 1️⃣ Get the raw list of messages\\n        msgs = self.chat_memory.messages\\n\\n        # 2️⃣ Split into “old” and “recent”\\n        recent = msgs[-self.k * 2 :]          # keep last k pairs verbatim\\n        old    = msgs[:-self.k * 2]           # everything else\\n\\n        # 3️⃣ Summarize the old part (if any)\\n        if old:\\n            old_text = \"\\\\n\".join(m.content for m in old)\\n            summary  = self.summarizer.run(old_text)   # e.g. \"The user asked about X...\"\\n            # turn the summary into a single pseudo‑message\\n            summary_msg = HumanMessage(content=summary)\\n            final_msgs = [summary_msg] + recent\\n        else:\\n            final_msgs = recent\\n\\n        # 4️⃣ Return in the requested format\\n        if self.return_messages:\\n            history = final_msgs\\n        else:\\n            history = \"\\\\n\".join(m.content for m in final_msgs)\\n\\n        # 5️⃣ (Optional) enforce a hard token limit\\n        #    (you could truncate `history` here if len(tokens) > self.max_tokens)\\n\\n        return {self.memory_key: history}\\n```\\n\\n*Result*: The model sees a concise recap of everything that happened before the last\\u202fk\\u202fturns, so it can still refer to “earlier” facts without blowing up the token budget.\\n\\n### 4.2. Time‑Based Eviction (TTL)\\n\\nIf you need to comply with data‑retention policies (e.g., delete anything older than 24\\u202fh), you can add timestamps and prune automatically:\\n\\n```python\\nfrom datetime import datetime, timedelta\\n\\nclass TTLWindowMemory(ConversationBufferWindowMemory):\\n    def __init__(self, ttl_seconds=86400, **kwargs):\\n        super().__init__(**kwargs)\\n        self.ttl = timedelta(seconds=ttl_seconds)\\n        self.timestamps = []   # parallel list of datetime objects\\n\\n    def save_context(self, inputs, outputs):\\n        super().save_context(inputs, outputs)\\n        self.timestamps.append(datetime.utcnow())\\n\\n    def load_memory_variables(self, inputs):\\n        # prune any messages older than ttl\\n        now = datetime.utcnow()\\n        keep_idx = [i for i, t in enumerate(self.timestamps) if now - t < self.ttl]\\n        # each turn = 2 messages, so we keep the corresponding slices\\n        self.chat_memory.messages = [self.chat_memory.messages[i] for i in keep_idx]\\n        self.timestamps = [self.timestamps[i] for i in keep_idx]\\n        return super().load_memory_variables(inputs)\\n```\\n\\n### 4.3. Retrieval‑Augmented Window (semantic search)\\n\\nWhen the *content* of older turns matters more than the exact wording, you can index every turn in a vector store and retrieve the most relevant snippets in addition to the sliding window:\\n\\n```python\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom langchain.vectorstores import FAISS\\n\\nemb = OpenAIEmbeddings()\\nvectorstore = FAISS(embedding_function=emb)\\n\\nclass RetrievalWindowMemory(ConversationBufferWindowMemory):\\n    def __init__(self, vectorstore, retrieve_k=3, **kwargs):\\n        super().__init__(**kwargs)\\n        self.vs = vectorstore\\n        self.retrieve_k = retrieve_k\\n\\n    def save_context(self, inputs, outputs):\\n        # store each turn as a separate document for later retrieval\\n        turn_text = f\"Human: {inputs[\\'question\\']}\\\\nAI: {outputs[\\'text\\']}\"\\n        self.vs.add_texts([turn_text])\\n        super().save_context(inputs, outputs)\\n\\n    def load_memory_variables(self, inputs):\\n        # 1️⃣ recent window (as usual)\\n        base = super().load_memory_variables(inputs)[self.memory_key]\\n\\n        # 2️⃣ retrieve semantically similar older turns\\n        query = inputs.get(\"question\", \"\")\\n        docs = self.vs.similarity_search(query, k=self.retrieve_k)\\n        retrieved = \"\\\\n---\\\\n\".join([d.page_content for d in docs])\\n\\n        # 3️⃣ concatenate\\n        combined = base + \"\\\\n---\\\\n\" + retrieved\\n        return {self.memory_key: combined}\\n```\\n\\n*Why this helps*: The model still gets the freshest turns verbatim, but it also receives a few **relevant** older facts, even if they fall outside the raw window.\\n\\n---\\n\\n## 5. Practical Tips & Gotchas\\n\\n| Issue | Symptom | How to Fix / Mitigate |\\n|-------|---------|------------------------|\\n| **Token overflow** | `TokenLimitError` or truncated prompt. | Lower `k`, add a summarizer, or switch to retrieval‑augmented memory. |\\n| **User‑specific data leaking across sessions** | One user sees another’s name or order number. | Create a **new memory instance per session** (e.g., `session_id → memory` dict) and never share the same object. |\\n| **Memory not updating** | The assistant repeats the same answer despite new input. | Ensure you call `memory.save_context(inputs, outputs)` after each turn (LLMChain does this automatically). |\\n| **Privacy / GDPR** | Old personal data retained indefinitely. | Use `ConversationBufferWindowMemory` with a small `k`, or implement a TTL memory (see 4.2). |\\n| **Model “forgets” a name introduced early** | User says “My name is Alice” early, later asks “What’s my name?” and the model says “I don’t know.” | Increase `k` enough to keep that turn, or add a **semantic summary** that explicitly stores “User name = Alice”. |\\n| **Prompt becomes unreadable** (lots of `\\\\n` separators) | Model outputs “I’m not sure what you’re asking.” | Use a **structured prompt** (e.g., list each turn as `Human:` / `AI:`) or set `return_messages=True` so LangChain formats it as a list of message objects. |\\n\\n---\\n\\n## 6. Quick Decision Flowchart\\n\\n```\\n               Need full history? ──► Use ConversationBufferMemory\\n                     |\\n                     | No (you need to stay under a token budget)\\n                     ▼\\n          How many recent turns are essential?\\n                     |\\n          ┌──────────┴───────────┐\\n          |                      |\\n   Keep ≤ 5‑10 turns?      Keep > 10 turns?\\n          |                      |\\n   Use WindowMemory(k=5‑10)   Use WindowMemory(k=15‑30)\\n          |                      |\\n   Still too big?            Still too big?\\n          |                      |\\n   Add summarizer            Add summarizer OR\\n   (HybridFull+Window)      Retrieval‑augmented layer\\n```\\n\\n---\\n\\n## 7. One‑Line Boilerplate for Production\\n\\nIf you’re wiring this into a FastAPI/Flask endpoint that serves many concurrent users, the pattern looks like this:\\n\\n```python\\n# global objects (LLM, prompt template)\\nllm = OpenAI(model=\"gpt-4\", temperature=0.0)\\nprompt = PromptTemplate(...)\\n\\n# per‑session factory\\ndef get_memory(session_id: str):\\n    # you could store a dict in Redis, a DB, or an in‑memory cache\\n    if session_id not in MEMORY_REGISTRY:\\n        MEMORY_REGISTRY[session_id] = ConversationBufferWindowMemory(k=5,\\n                                                                    memory_key=\"history\")\\n    return MEMORY_REGISTRY[session_id]\\n\\ndef chat_endpoint(session_id: str, user_msg: str):\\n    mem = get_memory(session_id)\\n    chain = LLMChain(llm=llm, prompt=prompt, memory=mem)\\n    return chain.run(user_msg)\\n```\\n\\n*Key points*:\\n\\n* **One memory object per session** → isolation.\\n* **`k`** can be tuned per‑user (e.g., premium users get a larger window).\\n* You can **persist** the `chat_memory.messages` list to a DB after each turn for auditability.\\n\\n---\\n\\n## 8. TL;DR Summary\\n\\n* **`ConversationBufferMemory`** = *store everything* → great for short, fully‑traceable dialogs, but token‑budget unsafe for long chats.  \\n* **`ConversationBufferWindowMemory(k)`** = *store only the last\\u202fk* turns → keeps prompts predictable, ideal for production bots; just pick a `k` that fits your model’s context.  \\n* If you need **something in‑between**, layer a **summarizer**, a **TTL eviction**, or a **semantic retrieval** on top of the window.\\n\\nFeel free to ask for a deeper dive on any of the extensions (summarization pipelines, vector‑store setup, or a full‑stack example with a database), or let me know if you’d like a ready‑to‑run notebook that demonstrates all three patterns side‑by‑side. Happy building!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 3384, 'prompt_tokens': 7211, 'total_tokens': 10595, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 60, 'rejected_prediction_tokens': None, 'image_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0, 'video_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_name': 'openai/gpt-oss-120b:free', 'system_fingerprint': None, 'id': 'gen-1766228407-qOVBTtxf9keiEPFkygDs', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--82bc349f-e389-43d9-9414-acc16cf830f4-0', usage_metadata={'input_tokens': 7211, 'output_tokens': 3384, 'total_tokens': 10595, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'reasoning': 60}})]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"id_123\"].messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b777c87",
   "metadata": {},
   "source": [
    "The information still maintained, let's check again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d5eb19a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: AttributeError(\"'list' object has no attribute 'content'\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is **Dat**.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 95, 'prompt_tokens': 10551, 'total_tokens': 10646, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 90, 'rejected_prediction_tokens': None, 'image_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0, 'video_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_name': 'openai/gpt-oss-120b:free', 'system_fingerprint': None, 'id': 'gen-1766228436-I20tKgsn4kNiCDD7gLwD', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--0fa06d46-fab0-40cb-b941-bec09caf2bb6-0', usage_metadata={'input_tokens': 10551, 'output_tokens': 95, 'total_tokens': 10646, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'reasoning': 90}})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"What is my name again?\"},\n",
    "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7656500e",
   "metadata": {},
   "source": [
    "# Agent and Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d77bf93",
   "metadata": {},
   "source": [
    "Tools are a way augment our LLMs with code execution. A tool is simply a function formatted so that our agent can undertstand how to use it, and then execute it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9f57e5",
   "metadata": {},
   "source": [
    "We can use the @tool decorator to create an LLM-compatible tool from a standard python function — this function should include a few things for optimal performance:\n",
    "- A docstring describing what the tool does and when it should be used, this will be read by our LLM/agent and used to decide when to use the tool, and also how to use the tool.\n",
    "- Clear parameter names that ideally tell the LLM what each parameter is, if it isn't clear we make sure the docstring explains what the parameter is for and how to use it.\n",
    "- Both parameter and return type annotations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3f2e76b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def add(x: float, y: float) -> float:\n",
    "    \"\"\"Add 'x' and 'y'.\"\"\"\n",
    "    return x + y\n",
    "\n",
    "@tool\n",
    "def multiply(x: float, y: float) -> float:\n",
    "    \"\"\"Multiply 'x' and 'y'.\"\"\"\n",
    "    return x * y\n",
    "\n",
    "@tool\n",
    "def exponentiate(x: float, y: float) -> float:\n",
    "    \"\"\"Raise 'x' to the power of 'y'.\"\"\"\n",
    "    return x ** y\n",
    "\n",
    "@tool\n",
    "def subtract(x: float, y: float) -> float:\n",
    "    \"\"\"Subtract 'x' from 'y'.\"\"\"\n",
    "    return y - x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1866103",
   "metadata": {},
   "source": [
    "With the `@tool` decorator our function is turned into a `StructuredTool` object, which we can see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bf2ce9a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredTool(name='add', description=\"Add 'x' and 'y'.\", args_schema=<class 'langchain_core.utils.pydantic.add'>, func=<function add at 0x7e66a241a3a0>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d600b585",
   "metadata": {},
   "source": [
    "We can see the tool name, description, and arg schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7a8941f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add.name='add'\n",
      "add.description=\"Add 'x' and 'y'.\"\n"
     ]
    }
   ],
   "source": [
    "print(f\"{add.name=}\\n{add.description=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d737e2da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': \"Add 'x' and 'y'.\",\n",
       " 'properties': {'x': {'title': 'X', 'type': 'number'},\n",
       "  'y': {'title': 'Y', 'type': 'number'}},\n",
       " 'required': ['x', 'y'],\n",
       " 'title': 'add',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add.args_schema.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "85e28a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': \"Raise 'x' to the power of 'y'.\",\n",
       " 'properties': {'x': {'title': 'X', 'type': 'number'},\n",
       "  'y': {'title': 'Y', 'type': 'number'}},\n",
       " 'required': ['x', 'y'],\n",
       " 'title': 'exponentiate',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exponentiate.args_schema.model_json_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1a3aab",
   "metadata": {},
   "source": [
    "When invoking the tool, a JSON string output by the LLM will be parsed into JSON and then consumed as kwargs, similar to the below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "47f27fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': 5, 'y': 2}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "llm_output_string = \"{\\\"x\\\": 5, \\\"y\\\": 2}\"  # this is the output from the LLM\n",
    "llm_output_dict = json.loads(llm_output_string)  # load as dictionary\n",
    "llm_output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f68097e",
   "metadata": {},
   "source": [
    "This is then passed into the tool function as `kwargs` (keyword arguments) as indicated by the `**` operator - the `**` operator is used to unpack the dictionary into keyword arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5fb30eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exponentiate.func(**llm_output_dict)  # call the function with unpacked args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadb5c23",
   "metadata": {},
   "source": [
    "This covers the basics of tools and how they work, let's move on to creating the agent itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba04c90c",
   "metadata": {},
   "source": [
    "### Creating an Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740d5c7e",
   "metadata": {},
   "source": [
    "We need this agent to remember previous interactions within the conversation. To do that, we will use the `ChatPromptTemplate` with a system message, a placeholder for our chat history, a placeholder for the user query, and finally a placeholder for the agent scratchpad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ee201d",
   "metadata": {},
   "source": [
    "The agent scratchpad is where the agent will write it's \"notes\" as it is working through multiple internal thought and tool-use steps to produce a final output to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d6b8c2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"you're a helpful assistant\"),\n",
    "    MessagesPlaceholder(variable_name = \"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a821771",
   "metadata": {},
   "source": [
    "When creating an agent we need to add conversational memory to make the agent remember previous interactions. We'll be using the older `ConversationBufferMemory` class rather than the newer `RunnableWithMessageHistory` — the reason being that we will also be using the older `create_tool_calling_agent` and `AgentExecutor` method and class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "56a80b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(\n",
    "    memory_key = \"chat_history\",  # must align with MessagesPlaceholder variable_name\n",
    "    return_messages = True  # to return Message objects\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "32f98112",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_tool_calling_agent\n",
    "\n",
    "tools = [add, subtract, multiply, exponentiate]\n",
    "\n",
    "agent = create_tool_calling_agent(\n",
    "    llm = llm, tools = tools, prompt = prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c54ded9",
   "metadata": {},
   "source": [
    "Our agent by itself is like one-step of our agent execution loop. So, if we call the `agent.invoke` method it will get the LLM to generate a single response and go no further, so no tools will be executed, and no next iterations will be performed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f202c36",
   "metadata": {},
   "source": [
    "We can see this by asking a query that should trigger a tool call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "42d2b4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentFinish(return_values={'output': '\\\\(10.7 \\\\times 7.68 = 82.176\\\\)'}, log='\\\\(10.7 \\\\times 7.68 = 82.176\\\\)')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.invoke({\n",
    "    \"input\": \"what is 10.7 multiplied by 7.68?\",\n",
    "    \"chat_history\": memory.chat_memory.messages,\n",
    "    \"intermediate_steps\": []  # agent will append it's internal steps here\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aef312d",
   "metadata": {},
   "source": [
    "Here, we can see the LLM has generated that we should use the multiply tool and the tool input should be `{\"x\": 10.7, \"y\": 7.68}`. However, the tool is not executed. For that to happen we need an agent execution loop, which will handle the multiple iterations of generation to tool calling to generation, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0d0a29",
   "metadata": {},
   "source": [
    "We use the `AgentExecutor` class to handle the execution loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f28ae548",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent = agent,\n",
    "    tools = tools,\n",
    "    memory = memory,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db73c5b",
   "metadata": {},
   "source": [
    "Now let's try the same query with the executor, note that the `intermediate_steps` parameter that we added before is no longer needed as the executor handles it internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b409b06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\\(10.7 \\times 7.68 = 82.176\\)\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is 10.7 multiplied by 7.68?',\n",
       " 'chat_history': [HumanMessage(content='what is 10.7 multiplied by 7.68?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='\\\\(10.7 \\\\times 7.68 = 82.176\\\\)', additional_kwargs={}, response_metadata={})],\n",
       " 'output': '\\\\(10.7 \\\\times 7.68 = 82.176\\\\)'}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\n",
    "    \"input\": \"what is 10.7 multiplied by 7.68?\",\n",
    "    \"chat_history\": memory.chat_memory.messages,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfb0670",
   "metadata": {},
   "source": [
    "We can see that the multiply tool was invoked, producing the observation of 82.175999.... After the observation was provided, we can see that the LLM then generated a final response of:\n",
    "\n",
    "```\n",
    "10.7 multiplied by 7.68 is approximately 82.18.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df475213",
   "metadata": {},
   "source": [
    "This final response was generated based on the original query and the tool output (ie the observation). We can also confirm that this answer is accurate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "938656c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82.17599999999999"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10.7*7.68"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59e6f80",
   "metadata": {},
   "source": [
    "Let's test our agent with some memory and tool use. First, we tell it our name, then we will perform a few tool calls, then see if the agent can still recall our name.\n",
    "\n",
    "First, give the agent our name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c1d70e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mNice to meet you, Dat! How can I assist you today?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'My name is Dat',\n",
       " 'chat_history': [HumanMessage(content='what is 10.7 multiplied by 7.68?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='\\\\(10.7 \\\\times 7.68 = 82.176\\\\)', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='My name is Dat', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Nice to meet you, Dat! How can I assist you today?', additional_kwargs={}, response_metadata={})],\n",
       " 'output': 'Nice to meet you, Dat! How can I assist you today?'}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\n",
    "    \"input\": \"My name is Dat\",\n",
    "    \"chat_history\": memory\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0780257",
   "metadata": {},
   "source": [
    "Now let's try and get the agent to perform multiple tool calls within a single execution loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "04f473bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThe expression can be interpreted as  \n",
      "\n",
      "\\[\n",
      "(9 + 10 - 4 \\times 2)^{3}\n",
      "\\]\n",
      "\n",
      "1. \\(4 \\times 2 = 8\\)  \n",
      "2. \\(9 + 10 = 19\\)  \n",
      "3. \\(19 - 8 = 11\\)  \n",
      "4. \\(11^{3} = 11 \\times 11 \\times 11 = 1331\\)\n",
      "\n",
      "**Result:** **1331**.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is nine plus 10, minus 4 * 2, to the power of 3',\n",
       " 'chat_history': [HumanMessage(content='what is 10.7 multiplied by 7.68?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='\\\\(10.7 \\\\times 7.68 = 82.176\\\\)', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='My name is Dat', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Nice to meet you, Dat! How can I assist you today?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='What is nine plus 10, minus 4 * 2, to the power of 3', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The expression can be interpreted as  \\n\\n\\\\[\\n(9 + 10 - 4 \\\\times 2)^{3}\\n\\\\]\\n\\n1. \\\\(4 \\\\times 2 = 8\\\\)  \\n2. \\\\(9 + 10 = 19\\\\)  \\n3. \\\\(19 - 8 = 11\\\\)  \\n4. \\\\(11^{3} = 11 \\\\times 11 \\\\times 11 = 1331\\\\)\\n\\n**Result:** **1331**.', additional_kwargs={}, response_metadata={})],\n",
       " 'output': 'The expression can be interpreted as  \\n\\n\\\\[\\n(9 + 10 - 4 \\\\times 2)^{3}\\n\\\\]\\n\\n1. \\\\(4 \\\\times 2 = 8\\\\)  \\n2. \\\\(9 + 10 = 19\\\\)  \\n3. \\\\(19 - 8 = 11\\\\)  \\n4. \\\\(11^{3} = 11 \\\\times 11 \\\\times 11 = 1331\\\\)\\n\\n**Result:** **1331**.'}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\n",
    "    \"input\": \"What is nine plus 10, minus 4 * 2, to the power of 3\",\n",
    "    \"chat_history\": memory\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e05a24d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-493"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9+10-(4*2)**3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36da8a5",
   "metadata": {},
   "source": [
    "Perfect, now let's see if the agent can still recall our name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "83c18be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mYour name is Dat.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is my name',\n",
       " 'chat_history': [HumanMessage(content='what is 10.7 multiplied by 7.68?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='\\\\(10.7 \\\\times 7.68 = 82.176\\\\)', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='My name is Dat', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Nice to meet you, Dat! How can I assist you today?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='What is nine plus 10, minus 4 * 2, to the power of 3', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The expression can be interpreted as  \\n\\n\\\\[\\n(9 + 10 - 4 \\\\times 2)^{3}\\n\\\\]\\n\\n1. \\\\(4 \\\\times 2 = 8\\\\)  \\n2. \\\\(9 + 10 = 19\\\\)  \\n3. \\\\(19 - 8 = 11\\\\)  \\n4. \\\\(11^{3} = 11 \\\\times 11 \\\\times 11 = 1331\\\\)\\n\\n**Result:** **1331**.', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='What is my name', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Your name is Dat.', additional_kwargs={}, response_metadata={})],\n",
       " 'output': 'Your name is Dat.'}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\n",
    "    \"input\": \"What is my name\",\n",
    "    \"chat_history\": memory\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f920a8c4",
   "metadata": {},
   "source": [
    "For other tool provided by 3rd-party, we can use `load_tools` library which Langchain support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5d66bf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Define toolfunctions with correct @tool decorator as above\n",
    "# Invoke the call with tools list passing in\n",
    "# For better display in Jupyter notebooks, we use Markdown to format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e234d6e1",
   "metadata": {},
   "source": [
    "# Agent Executor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8885508",
   "metadata": {},
   "source": [
    "When we talk about agents, a significant part of an \"agent\" is simple code logic,\n",
    "iteratively rerunning LLM calls and processing their output. The exact logic varies\n",
    "significantly, but one well-known example is the **ReAct** agent.\n",
    "\n",
    "![ReAct process](https://www.aurelio.ai/_next/image?url=%2Fimages%2Fposts%2Fai-agents%2Fai-agents-00.png&w=640&q=75)\n",
    "\n",
    "**Re**ason + **Act**ion (ReAct) agents use iterative _reasoning_ and _action_ steps to\n",
    "incorporate chain-of-thought and tool-use into their execution. During the _reasoning_\n",
    "step, the LLM generates the steps to take to answer the query. Next, the LLM generates\n",
    "the _action_ input, which our code logic parses into a tool call.\n",
    "\n",
    "![Agentic graph of ReAct](https://www.aurelio.ai/_next/image?url=%2Fimages%2Fposts%2Fai-agents%2Fai-agents-01.png&w=640&q=75)\n",
    "\n",
    "Following our action step, we get an observation from the tool call. Then, we feed the\n",
    "observation back into the agent executor logic for a final answer or further reasoning\n",
    "and action steps.\n",
    "\n",
    "The agent and agent executor we will be building will follow this pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6fa34e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", (\n",
    "        \"You're a helpful assistant. When answering a user's question \"\n",
    "        \"you should first use one of the tools provided. After using a \"\n",
    "        \"tool the tool output will be provided in the \"\n",
    "        \"'scratchpad' below. If you have an answer in the \"\n",
    "        \"scratchpad you should not use any more tools and \"\n",
    "        \"instead answer directly to the user.\"\n",
    "    )),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f476fee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.base import RunnableSerializable\n",
    "\n",
    "# define the agent runnable\n",
    "agent: RunnableSerializable = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "        \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
    "    }\n",
    "    | prompt\n",
    "    | llm.bind_tools(tools, tool_choice = \"any\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f37537",
   "metadata": {},
   "source": [
    "We invoke the agent with the `invoke` method, passing in the input and chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2ca4b091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'chatcmpl-tool-89cc7030bfac4d8d', 'function': {'arguments': '{\\n  \"x\": 10,\\n  \"y\": 10\\n}', 'name': 'add'}, 'type': 'function', 'index': 0}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 280, 'total_tokens': 321, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 6, 'rejected_prediction_tokens': None, 'image_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0, 'video_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_name': 'openai/gpt-oss-120b:free', 'system_fingerprint': None, 'id': 'gen-1766228607-nD1BRZwtOmqWb82T8mqS', 'service_tier': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--7db5ae84-80f1-4c4d-b644-8f246d41aef7-0', tool_calls=[{'name': 'add', 'args': {'x': 10, 'y': 10}, 'id': 'chatcmpl-tool-89cc7030bfac4d8d', 'type': 'tool_call'}], usage_metadata={'input_tokens': 280, 'output_tokens': 41, 'total_tokens': 321, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'reasoning': 6}})"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_call = agent.invoke({\"input\": \"What is 10 + 10\", \"chat_history\": []})\n",
    "tool_call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a82961",
   "metadata": {},
   "source": [
    "Because we set `tool_choice = \"any\"` to force the tool output, the usual `content` field will be empty as that field is used for natural language output, ie the final answer of the LLM. To find our tool output, we need to look at the `tool_calls` field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5ebbeb40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'add',\n",
       "  'args': {'x': 10, 'y': 10},\n",
       "  'id': 'chatcmpl-tool-89cc7030bfac4d8d',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_call.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f279e5e0",
   "metadata": {},
   "source": [
    "From here, we have the tool name that our LLM wants to use and the `args` that it wants to pass to that tool. We can see that the tool `add` is being used with the arguments `x = 10` and `y = 10`. The agent.invoke method has not executed the tool function; we need to write that part of the agent code ourselves.\n",
    "\n",
    "Executing the tool code requires two steps:\n",
    "- Map the tool name to the tool function.\n",
    "- Execute the tool function with the generated args."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4876018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tool name to function mapping\n",
    "name2tool = {tool.name: tool.func for tool in tools}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1db37d",
   "metadata": {},
   "source": [
    "Now execute to get our answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8c2ac242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_exec_content = name2tool[tool_call.tool_calls[0][\"name\"]](\n",
    "    **tool_call.tool_calls[0][\"args\"]\n",
    ")\n",
    "tool_exec_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a03cd8f",
   "metadata": {},
   "source": [
    "That is our answer and tool execution logic. We feed this back into our LLM via the `agent_scratchpad` placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6fe7f4a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The sum of 10\\u202f+\\u202f10 is **20**.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 318, 'total_tokens': 351, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 8, 'rejected_prediction_tokens': None, 'image_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0, 'video_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_name': 'openai/gpt-oss-120b:free', 'system_fingerprint': None, 'id': 'gen-1766228610-bnhPdWmnh76B8ZeKau6I', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--10a2e034-1a90-48d6-9710-1cd4804fff03-0', usage_metadata={'input_tokens': 318, 'output_tokens': 33, 'total_tokens': 351, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'reasoning': 8}})"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "tool_exec = ToolMessage(\n",
    "    content=f\"The {tool_call.tool_calls[0]['name']} tool returned {tool_exec_content}\",\n",
    "    tool_call_id=tool_call.tool_calls[0][\"id\"]\n",
    ")\n",
    "\n",
    "out = agent.invoke({\n",
    "    \"input\": \"What is 10 + 10\",\n",
    "    \"chat_history\": [],\n",
    "    \"agent_scratchpad\": [tool_call, tool_exec]\n",
    "})\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5967ae7e",
   "metadata": {},
   "source": [
    "Despite having the answer in our `agent_scratchpad`, the LLM still tries to use the tool again. This behaviour happens because we bonded the tools to the LLM with `tool_choice = \"any\"`. When we set `tool_choice` to `\"any\"` or `\"required\"`, we tell the LLM that it MUST use a tool, i.e., it cannot provide a final answer.\n",
    "\n",
    "There's two options to fix this:\n",
    "- Set `tool_choice = \"auto\"`to tell the LLM that it can choose to use a tool or provide a final answer.\n",
    "- Create a `final_answer` tool - we'll explain this shortly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d9d88538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1\n",
    "agent: RunnableSerializable = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "        \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
    "    }\n",
    "    | prompt\n",
    "    | llm.bind_tools(tools, tool_choice=\"auto\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f45b82d",
   "metadata": {},
   "source": [
    "We'll start from the start again, so `agent_scratchpad` is empty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1f14135b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'chatcmpl-tool-935c299ff56bde01', 'function': {'arguments': '{\\n  \"x\": 10,\\n  \"y\": 10\\n}', 'name': 'add'}, 'type': 'function', 'index': 0}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 280, 'total_tokens': 321, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 6, 'rejected_prediction_tokens': None, 'image_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0, 'video_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_name': 'openai/gpt-oss-120b:free', 'system_fingerprint': None, 'id': 'gen-1766228616-wTgkfnShniLO3jPqGERF', 'service_tier': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--3ed6c08b-0766-4de8-9eb0-cc51c1c9d8e1-0', tool_calls=[{'name': 'add', 'args': {'x': 10, 'y': 10}, 'id': 'chatcmpl-tool-935c299ff56bde01', 'type': 'tool_call'}], usage_metadata={'input_tokens': 280, 'output_tokens': 41, 'total_tokens': 321, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'reasoning': 6}})"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_call = agent.invoke({\"input\": \"What is 10 + 10\", \"chat_history\": []})\n",
    "tool_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a7e417c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='10\\u202f+\\u202f10\\u202f=\\u202f20.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 318, 'total_tokens': 346, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 7, 'rejected_prediction_tokens': None, 'image_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0, 'video_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_name': 'openai/gpt-oss-120b:free', 'system_fingerprint': None, 'id': 'gen-1766228619-ztovFTteiTFWZCSVbWv2', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--02135d4f-be47-41ba-9b67-562747f2a20f-0', usage_metadata={'input_tokens': 318, 'output_tokens': 28, 'total_tokens': 346, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'reasoning': 7}})"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_output = name2tool[tool_call.tool_calls[0][\"name\"]](\n",
    "    **tool_call.tool_calls[0][\"args\"]\n",
    ")\n",
    "\n",
    "tool_exec = ToolMessage(\n",
    "    content=f\"The {tool_call.tool_calls[0]['name']} tool returned {tool_output}\",\n",
    "    tool_call_id=tool_call.tool_calls[0][\"id\"]\n",
    ")\n",
    "\n",
    "out = agent.invoke({\n",
    "    \"input\": \"What is 10 + 10\",\n",
    "    \"chat_history\": [],\n",
    "    \"agent_scratchpad\": [tool_call, tool_exec]\n",
    "})\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2af49f2",
   "metadata": {},
   "source": [
    "We now have the final answer in the content field! This method is perfectly functional; however, we recommend option 2 as it provides more control over the agent's output.\n",
    "\n",
    "There are several reasons that option 2 can provide more control, those are:\n",
    "- It removes the possibility of an agent using the direct content field when it is not appropriate; for example, some LLMs (particularly smaller ones) may try to use the content field when using a tool.\n",
    "- We can enforce a specific structured output in our answers. Structured outputs are handy when we require particular fields for downstream code or multi-part answers. For example, a RAG agent may return a natural language answer and a list of sources used to generate that answer.\n",
    "\n",
    "To implement option 2, we must create a `final_answer` tool. We will add a `tools_used` field to give our output some structure—in a real-world use case, we probably wouldn't want to generate this field, but it's useful for our example here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0e853b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def final_answer(answer: str, tools_used: list[str]) -> str:\n",
    "    \"\"\"Use this tool to provide a final answer to the user.\n",
    "    The answer should be in natural language as this will be provided\n",
    "    to the user directly. The tools_used must include a list of tool\n",
    "    names that were used within the `scratchpad`.\n",
    "    \"\"\"\n",
    "    return {\"answer\": answer, \"tools_used\": tools_used}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b0ae9b",
   "metadata": {},
   "source": [
    "Our `final_answer` tool doesn't necessarily need to do anything; in this example, we're using it purely to structure our final response. We can now add this tool to our agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5baa178f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [final_answer, add, subtract, multiply, exponentiate]\n",
    "\n",
    "# we need to update our name2tool mapping too\n",
    "name2tool = {tool.name: tool.func for tool in tools}\n",
    "\n",
    "agent: RunnableSerializable = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "        \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
    "    }\n",
    "    | prompt\n",
    "    | llm.bind_tools(tools, tool_choice = \"any\")  # we're forcing tool use again\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c81b3a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'add',\n",
       "  'args': {'x': 10, 'y': 10},\n",
       "  'id': 'chatcmpl-tool-976b52be56fde777',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_call = agent.invoke({\"input\": \"What is 10 + 10\", \"chat_history\": []})\n",
    "tool_call.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c903e647",
   "metadata": {},
   "source": [
    "We execute the tool and provide it's output to the agent again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f65da148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='{\\n  \"answer\": \"10 + 10 = 20.\",\\n  \"tools_used\": [\"add\"]\\n}', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 393, 'total_tokens': 441, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 12, 'rejected_prediction_tokens': None, 'image_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0, 'video_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_name': 'openai/gpt-oss-120b:free', 'system_fingerprint': None, 'id': 'gen-1766228633-kgbuBpQNvdbmri1Ex1Ga', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--eb4fc8ee-3aea-4d2e-ba9e-45e93140a410-0', usage_metadata={'input_tokens': 393, 'output_tokens': 48, 'total_tokens': 441, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'reasoning': 12}})"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_out = name2tool[tool_call.tool_calls[0][\"name\"]](\n",
    "    **tool_call.tool_calls[0][\"args\"]\n",
    ")\n",
    "\n",
    "tool_exec = ToolMessage(\n",
    "    content=f\"The {tool_call.tool_calls[0]['name']} tool returned {tool_out}\",\n",
    "    tool_call_id=tool_call.tool_calls[0][\"id\"]\n",
    ")\n",
    "\n",
    "out = agent.invoke({\n",
    "    \"input\": \"What is 10 + 10\",\n",
    "    \"chat_history\": [],\n",
    "    \"agent_scratchpad\": [tool_call, tool_exec]\n",
    "})\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65123e36",
   "metadata": {},
   "source": [
    "### Full Custom Agent Execution Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f4c320b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "class CustomAgentExecutor:\n",
    "    chat_history: list[BaseMessage]\n",
    "\n",
    "    def __init__(self, max_iterations: int = 3):\n",
    "        self.chat_history = []\n",
    "        self.max_iterations = max_iterations\n",
    "        self.agent: RunnableSerializable = (\n",
    "            {\n",
    "                \"input\": lambda x: x[\"input\"],\n",
    "                \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "                \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
    "            }\n",
    "            | prompt\n",
    "            | llm.bind_tools(tools, tool_choice=\"any\")  # we're forcing tool use again\n",
    "        )\n",
    "\n",
    "    def invoke(self, input: str) -> dict:\n",
    "        # invoke the agent but we do this iteratively in a loop until\n",
    "        # reaching a final answer\n",
    "        count = 0\n",
    "        agent_scratchpad = []\n",
    "        while count < self.max_iterations:\n",
    "            # invoke a step for the agent to generate a tool call\n",
    "            tool_call = self.agent.invoke({\n",
    "                \"input\": input,\n",
    "                \"chat_history\": self.chat_history,\n",
    "                \"agent_scratchpad\": agent_scratchpad\n",
    "            })\n",
    "            # add initial tool call to scratchpad\n",
    "            agent_scratchpad.append(tool_call)\n",
    "            # otherwise we execute the tool and add it's output to the agent scratchpad\n",
    "            tool_name = tool_call.tool_calls[0][\"name\"]\n",
    "            tool_args = tool_call.tool_calls[0][\"args\"]\n",
    "            tool_call_id = tool_call.tool_calls[0][\"id\"]\n",
    "            tool_out = name2tool[tool_name](**tool_args)\n",
    "            # add the tool output to the agent scratchpad\n",
    "            tool_exec = ToolMessage(\n",
    "                content=f\"{tool_out}\",\n",
    "                tool_call_id=tool_call_id\n",
    "            )\n",
    "            agent_scratchpad.append(tool_exec)\n",
    "            # add a print so we can see intermediate steps\n",
    "            print(f\"{count}: {tool_name}({tool_args})\")\n",
    "            count += 1\n",
    "            # if the tool call is the final answer tool, we stop\n",
    "            if tool_name == \"final_answer\":\n",
    "                break\n",
    "        # add the final output to the chat history\n",
    "        final_answer = tool_out[\"answer\"]\n",
    "        self.chat_history.extend([\n",
    "            HumanMessage(content=input),\n",
    "            AIMessage(content=final_answer)\n",
    "        ])\n",
    "        # return the final answer in dict form\n",
    "        return json.dumps(tool_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "421a7176",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = CustomAgentExecutor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ccfe05be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: add({'x': 10, 'y': 10})\n",
      "1: final_answer({'answer': '10 + 10 = 20.', 'tools_used': ['add']})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"answer\": \"10 + 10 = 20.\", \"tools_used\": [\"add\"]}'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke(input = \"What is 10 + 10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be726655",
   "metadata": {},
   "source": [
    "# LangChains Expression Language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d08c25",
   "metadata": {},
   "source": [
    "### Traditional Chains vs LCEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f78ed5",
   "metadata": {},
   "source": [
    "In this section we're going to dive into a basic example using the traditional method for building chains before jumping into LCEL. We will build a pipeline where the user must input a specific topic, and then the LLM will look and return a report on the specified topic. Generating a _research report_ for the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17470fe",
   "metadata": {},
   "source": [
    "##### Traditional LLMChain to LCEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6123def1",
   "metadata": {},
   "source": [
    "The `LLMChain` is the simplest chain originally introduced in LangChain. This chain takes a prompt, feeds it into an LLM, and _optionally_ adds an output parsing step before returning the result.\n",
    "\n",
    "Let's see how we construct this using the traditional method, for this we need:\n",
    "\n",
    "* `prompt` — a `PromptTemplate` that will be used to generate the prompt for the LLM.\n",
    "* `llm` — the LLM we will be using to generate the output.\n",
    "* `output_parser` — an optional output parser that will be used to parse the structured output of the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5dce5c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "prompt_template = \"Give me a small report on {topic}\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables = [\"topic\"],\n",
    "    template = prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a331c95e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I help you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 71, 'total_tokens': 111, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 26, 'rejected_prediction_tokens': None, 'image_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0, 'video_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_name': 'openai/gpt-oss-120b:free', 'system_fingerprint': None, 'id': 'gen-1766228752-dFIAaCgX0PG646hHWjnu', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--088ee9e4-e8f7-4f1d-8439-6b5830610ff4-0', usage_metadata={'input_tokens': 71, 'output_tokens': 40, 'total_tokens': 111, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'reasoning': 26}})"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_out = llm.invoke(\"Hello there\")\n",
    "llm_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177a5ed9",
   "metadata": {},
   "source": [
    "Then we define our output parser, this will be used to parse the output of the LLM. In this case, we will use the `StrOutputParser` which will parse the `AIMessage` output from our LLM into a single string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "dda0ad41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c49903c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I help you today?'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = output_parser.invoke(llm_out)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d71eea",
   "metadata": {},
   "source": [
    "Through the `LLMChain` class we can place each of our components into a linear `chain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9b9abedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1108/2960353250.py:3: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(prompt=prompt, llm=llm, output_parser=output_parser)\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(prompt = prompt,\n",
    "                llm = llm,\n",
    "                output_parser = output_parser\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81766217",
   "metadata": {},
   "source": [
    "Note that the `LLMChain` _was_ deprecated in LangChain `0.1.17`, the expected way of constructing these chains today is through LCEL, which we'll cover in a moment.\n",
    "\n",
    "We can `invoke` our `chain`, providing a `topic` that we'd like to be researched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "bf9d60c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'retrieval augmented generation',\n",
       " 'text': '**Retrieval‑Augmented Generation (RAG): A Brief Report**\\n\\n---\\n\\n### 1. Overview  \\nRetrieval‑Augmented Generation (RAG) is a hybrid AI architecture that couples a **retrieval module** (searching an external knowledge source) with a **generative language model**. The retriever supplies relevant documents or passages, and the generator conditions its output on this retrieved context, enabling the system to produce up‑to‑date, factual, and domain‑specific responses that go beyond the static knowledge baked into the model’s parameters.\\n\\n---\\n\\n### 2. Core Components  \\n\\n| Component | Function | Typical Implementations |\\n|-----------|----------|--------------------------|\\n| **Retriever** | Finds the most relevant pieces of information from a large corpus (e.g., Wikipedia, internal documents, web index). | • Dense vector search (e.g., FAISS, ScaNN) using bi‑encoders like DPR, Contriever, or Sentence‑Transformers.<br>• Sparse lexical search (BM25, Elasticsearch) for exact term matching.<br>• Hybrid (dense\\u202f+\\u202fsparse) retrieval for robustness. |\\n| **Reader/Generator** | Consumes the retrieved texts and produces a natural‑language answer or continuation. | • Large language models (LLMs) such as GPT‑3/4, LLaMA, PaLM, or instruction‑tuned variants.<br>• Encoder‑decoder models (T5, BART) fine‑tuned on QA or summarisation tasks. |\\n| **Fusion / Integration Layer** | Merges retrieved evidence with the model’s internal knowledge. | • Concatenation of passages to the prompt (prompt‑engineering).<br>• Cross‑attention over retrieved documents (e.g., Fusion‑in‑Decoder, RAG‑Sequence/Token).<br>• Knowledge‑aware adapters or LoRA modules that bias the generator toward the evidence. |\\n| **Index & Update Mechanism** | Stores the external corpus and allows incremental updates. | • Vector databases (FAISS, Milvus, Weaviate).<br>• Periodic re‑embedding pipelines to keep the index fresh. |\\n\\n---\\n\\n### 3. Typical Workflow  \\n\\n1. **Query Encoding** – The user’s input is encoded into a dense vector (or a set of lexical terms).  \\n2. **Document Retrieval** – The vector is used to retrieve *k* top‑ranked passages from the external index.  \\n3. **Context Construction** – Retrieved passages are formatted (e.g., “Document 1: …”) and concatenated with the original query.  \\n4. **Conditional Generation** – The LLM receives the combined prompt and generates a response, often with a **grounding loss** that penalizes hallucinations not supported by the retrieved text.  \\n5. **Post‑processing** – Optional steps include citation extraction, answer verification, or re‑ranking of multiple generated candidates.\\n\\n---\\n\\n### 4. Advantages  \\n\\n| Benefit | Why It Matters |\\n|---------|----------------|\\n| **Improved factuality** | The model can cite up‑to‑date sources, reducing hallucinations. |\\n| **Scalability of knowledge** | Adding new facts only requires updating the external index, not retraining the LLM. |\\n| **Domain adaptability** | Specialized corpora (legal, medical, corporate) can be plugged in without massive model fine‑tuning. |\\n| **Interpretability** | Retrieved passages act as evidence that can be shown to users. |\\n| **Parameter efficiency** | Smaller LLMs can achieve performance comparable to larger models when paired with a strong retriever. |\\n\\n---\\n\\n### 5. Challenges & Open Issues  \\n\\n| Challenge | Current Mitigations |\\n|-----------|----------------------|\\n| **Retriever quality** – Poor recall leads to bad generations. | • Joint training of retriever and generator (e.g., RAG‑Fine‑Tuning).<br>• Hybrid retrieval and query expansion. |\\n| **Prompt length limits** – LLM context windows may truncate evidence. | • Passage selection/ranking, summarisation of retrieved docs, or use of long‑context models (e.g., LLaMA‑2‑70B‑Chat with 4‑8\\u202fk tokens). |\\n| **Hallucination despite evidence** – Model may ignore retrieved text. | • Grounding losses, reinforcement learning from human feedback (RLHF) with evidence‑aware rewards. |\\n| **Latency** – Two‑stage pipeline can be slower than pure generation. | • Approximate nearest‑neighbor search, caching frequent queries, or distilling the retriever into a lightweight model. |\\n| **Evaluation** – Measuring factual correctness and citation quality is non‑trivial. | • Benchmarks such as **Natural Questions**, **TriviaQA**, **KILT**, **MMLU‑RAG**, and metrics like **Exact Match**, **F1**, **Citation Recall**. |\\n\\n---\\n\\n### 6. Representative Applications  \\n\\n| Domain | Example Use‑Case |\\n|--------|------------------|\\n| **Search Engines** | Bing Chat, Google Gemini’s “search‑augmented” mode. |\\n| **Customer Support** | Knowledge‑base‑driven chatbots that pull policy documents in real time. |\\n| **Healthcare** | Clinical decision support that retrieves up‑to‑date guidelines or research papers. |\\n| **Legal** | Contract analysis tools that cite statutes and case law. |\\n| **Education** | Tutoring assistants that reference textbooks or scholarly articles. |\\n| **Enterprise** | Internal Q&A over proprietary documents (e.g., Confluence, SharePoint). |\\n\\n---\\n\\n### 7. Emerging Trends (2023‑2025)\\n\\n1. **Multimodal RAG** – Retrieval of images, tables, or code snippets alongside text, with multimodal generators (e.g., Flamingo‑RAG, LLaVA‑RAG).  \\n2. **Self‑RAG** – Models that generate their own retrieval queries or synthesize intermediate “knowledge” representations (e.g., “Chain‑of‑Thought Retrieval”).  \\n3. **Dynamic Indexing** – Real‑time ingestion pipelines that allow the system to reflect news or streaming data within seconds.  \\n4. **Instruction‑tuned RAG** – Fine‑tuning LLMs on datasets where the answer must be explicitly grounded in retrieved citations (e.g., **RAG‑QA**, **OpenRAG**).  \\n5. **Privacy‑preserving Retrieval** – Use of encrypted indexes or federated search to keep proprietary data confidential while still benefiting from RAG.  \\n\\n---\\n\\n### 8. Sample Implementation Blueprint (Python‑style)\\n\\n```python\\n# 1️⃣ Load a dense retriever (e.g., Sentence‑Transformer)\\nfrom sentence_transformers import CrossEncoder, SentenceTransformer\\nretriever = SentenceTransformer(\\'facebook/dpr-ctx_encoder-single-nq-base\\')\\nindex = faiss.read_index(\\'my_corpus.index\\')   # pre‑built vector index\\n\\n# 2️⃣ Encode the query\\nquery = \"What are the main causes of urban heat islands?\"\\nq_vec = retriever.encode([query], normalize_embeddings=True)\\n\\n# 3️⃣ Retrieve top‑k passages\\nk = 5\\nD, I = index.search(q_vec, k)                # distances, ids\\npassages = [corpus[i] for i in I[0]]\\n\\n# 4️⃣ Build the prompt\\nprompt = f\"\"\"Answer the question using only the information below.\\nQuestion: {query}\\nContext:\\n{chr(10).join([f\"[{i+1}] {p}\" for i, p in enumerate(passages)])}\\nAnswer:\"\"\"\\n\\n# 5️⃣ Generate with an LLM (e.g., OpenAI API)\\nimport openai\\nresponse = openai.ChatCompletion.create(\\n    model=\"gpt-4o-mini\",\\n    messages=[{\"role\": \"user\", \"content\": prompt}],\\n    temperature=0.2,\\n    max_tokens=300,\\n)\\n\\nprint(response.choices[0].message.content)\\n```\\n\\n*The code illustrates the classic “retrieve‑then‑generate” pattern; production systems typically add reranking, citation extraction, and safety filters.*\\n\\n---\\n\\n### 9. Key References (selected)\\n\\n| Year | Citation |\\n|------|----------|\\n| 2020 | **Lewis et al.** “Retrieval‑Augmented Generation for Knowledge‑Intensive NLP Tasks” (RAG). |\\n| 2021 | **Karpukhin et al.** “Dense Passage Retrieval for Open‑Domain Question Answering”. |\\n| 2022 | **Izacard & Grave** “Leveraging Passage Retrieval with Generative Models”. |\\n| 2023 | **Gao et al.** “Self‑RAG: Generating Retrieval Queries from the Model Itself”. |\\n| 2024 | **Wang et al.** “Multimodal Retrieval‑Augmented Generation”. |\\n| 2025 | **OpenRAG** (open‑source framework) – https://github.com/openrag/openrag |\\n\\n---\\n\\n### 10. Take‑away Message  \\n\\nRetrieval‑Augmented Generation bridges the gap between **static, parametric knowledge** and **dynamic, external information**. By grounding LLM outputs in retrieved evidence, RAG delivers more factual, up‑to‑date, and explainable results while keeping model sizes manageable. Ongoing research focuses on tighter integration, multimodal evidence, and real‑time indexing—making RAG a cornerstone of next‑generation AI assistants and enterprise knowledge tools.'}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chain.invoke(\"retrieval augmented generation\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651c898c",
   "metadata": {},
   "source": [
    "We can view a formatted version of this output using the `Markdown` display:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "413fa028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Retrieval‑Augmented Generation (RAG): A Brief Report**\n",
       "\n",
       "---\n",
       "\n",
       "### 1. Overview  \n",
       "Retrieval‑Augmented Generation (RAG) is a hybrid AI architecture that couples a **retrieval module** (searching an external knowledge source) with a **generative language model**. The retriever supplies relevant documents or passages, and the generator conditions its output on this retrieved context, enabling the system to produce up‑to‑date, factual, and domain‑specific responses that go beyond the static knowledge baked into the model’s parameters.\n",
       "\n",
       "---\n",
       "\n",
       "### 2. Core Components  \n",
       "\n",
       "| Component | Function | Typical Implementations |\n",
       "|-----------|----------|--------------------------|\n",
       "| **Retriever** | Finds the most relevant pieces of information from a large corpus (e.g., Wikipedia, internal documents, web index). | • Dense vector search (e.g., FAISS, ScaNN) using bi‑encoders like DPR, Contriever, or Sentence‑Transformers.<br>• Sparse lexical search (BM25, Elasticsearch) for exact term matching.<br>• Hybrid (dense + sparse) retrieval for robustness. |\n",
       "| **Reader/Generator** | Consumes the retrieved texts and produces a natural‑language answer or continuation. | • Large language models (LLMs) such as GPT‑3/4, LLaMA, PaLM, or instruction‑tuned variants.<br>• Encoder‑decoder models (T5, BART) fine‑tuned on QA or summarisation tasks. |\n",
       "| **Fusion / Integration Layer** | Merges retrieved evidence with the model’s internal knowledge. | • Concatenation of passages to the prompt (prompt‑engineering).<br>• Cross‑attention over retrieved documents (e.g., Fusion‑in‑Decoder, RAG‑Sequence/Token).<br>• Knowledge‑aware adapters or LoRA modules that bias the generator toward the evidence. |\n",
       "| **Index & Update Mechanism** | Stores the external corpus and allows incremental updates. | • Vector databases (FAISS, Milvus, Weaviate).<br>• Periodic re‑embedding pipelines to keep the index fresh. |\n",
       "\n",
       "---\n",
       "\n",
       "### 3. Typical Workflow  \n",
       "\n",
       "1. **Query Encoding** – The user’s input is encoded into a dense vector (or a set of lexical terms).  \n",
       "2. **Document Retrieval** – The vector is used to retrieve *k* top‑ranked passages from the external index.  \n",
       "3. **Context Construction** – Retrieved passages are formatted (e.g., “Document 1: …”) and concatenated with the original query.  \n",
       "4. **Conditional Generation** – The LLM receives the combined prompt and generates a response, often with a **grounding loss** that penalizes hallucinations not supported by the retrieved text.  \n",
       "5. **Post‑processing** – Optional steps include citation extraction, answer verification, or re‑ranking of multiple generated candidates.\n",
       "\n",
       "---\n",
       "\n",
       "### 4. Advantages  \n",
       "\n",
       "| Benefit | Why It Matters |\n",
       "|---------|----------------|\n",
       "| **Improved factuality** | The model can cite up‑to‑date sources, reducing hallucinations. |\n",
       "| **Scalability of knowledge** | Adding new facts only requires updating the external index, not retraining the LLM. |\n",
       "| **Domain adaptability** | Specialized corpora (legal, medical, corporate) can be plugged in without massive model fine‑tuning. |\n",
       "| **Interpretability** | Retrieved passages act as evidence that can be shown to users. |\n",
       "| **Parameter efficiency** | Smaller LLMs can achieve performance comparable to larger models when paired with a strong retriever. |\n",
       "\n",
       "---\n",
       "\n",
       "### 5. Challenges & Open Issues  \n",
       "\n",
       "| Challenge | Current Mitigations |\n",
       "|-----------|----------------------|\n",
       "| **Retriever quality** – Poor recall leads to bad generations. | • Joint training of retriever and generator (e.g., RAG‑Fine‑Tuning).<br>• Hybrid retrieval and query expansion. |\n",
       "| **Prompt length limits** – LLM context windows may truncate evidence. | • Passage selection/ranking, summarisation of retrieved docs, or use of long‑context models (e.g., LLaMA‑2‑70B‑Chat with 4‑8 k tokens). |\n",
       "| **Hallucination despite evidence** – Model may ignore retrieved text. | • Grounding losses, reinforcement learning from human feedback (RLHF) with evidence‑aware rewards. |\n",
       "| **Latency** – Two‑stage pipeline can be slower than pure generation. | • Approximate nearest‑neighbor search, caching frequent queries, or distilling the retriever into a lightweight model. |\n",
       "| **Evaluation** – Measuring factual correctness and citation quality is non‑trivial. | • Benchmarks such as **Natural Questions**, **TriviaQA**, **KILT**, **MMLU‑RAG**, and metrics like **Exact Match**, **F1**, **Citation Recall**. |\n",
       "\n",
       "---\n",
       "\n",
       "### 6. Representative Applications  \n",
       "\n",
       "| Domain | Example Use‑Case |\n",
       "|--------|------------------|\n",
       "| **Search Engines** | Bing Chat, Google Gemini’s “search‑augmented” mode. |\n",
       "| **Customer Support** | Knowledge‑base‑driven chatbots that pull policy documents in real time. |\n",
       "| **Healthcare** | Clinical decision support that retrieves up‑to‑date guidelines or research papers. |\n",
       "| **Legal** | Contract analysis tools that cite statutes and case law. |\n",
       "| **Education** | Tutoring assistants that reference textbooks or scholarly articles. |\n",
       "| **Enterprise** | Internal Q&A over proprietary documents (e.g., Confluence, SharePoint). |\n",
       "\n",
       "---\n",
       "\n",
       "### 7. Emerging Trends (2023‑2025)\n",
       "\n",
       "1. **Multimodal RAG** – Retrieval of images, tables, or code snippets alongside text, with multimodal generators (e.g., Flamingo‑RAG, LLaVA‑RAG).  \n",
       "2. **Self‑RAG** – Models that generate their own retrieval queries or synthesize intermediate “knowledge” representations (e.g., “Chain‑of‑Thought Retrieval”).  \n",
       "3. **Dynamic Indexing** – Real‑time ingestion pipelines that allow the system to reflect news or streaming data within seconds.  \n",
       "4. **Instruction‑tuned RAG** – Fine‑tuning LLMs on datasets where the answer must be explicitly grounded in retrieved citations (e.g., **RAG‑QA**, **OpenRAG**).  \n",
       "5. **Privacy‑preserving Retrieval** – Use of encrypted indexes or federated search to keep proprietary data confidential while still benefiting from RAG.  \n",
       "\n",
       "---\n",
       "\n",
       "### 8. Sample Implementation Blueprint (Python‑style)\n",
       "\n",
       "```python\n",
       "# 1️⃣ Load a dense retriever (e.g., Sentence‑Transformer)\n",
       "from sentence_transformers import CrossEncoder, SentenceTransformer\n",
       "retriever = SentenceTransformer('facebook/dpr-ctx_encoder-single-nq-base')\n",
       "index = faiss.read_index('my_corpus.index')   # pre‑built vector index\n",
       "\n",
       "# 2️⃣ Encode the query\n",
       "query = \"What are the main causes of urban heat islands?\"\n",
       "q_vec = retriever.encode([query], normalize_embeddings=True)\n",
       "\n",
       "# 3️⃣ Retrieve top‑k passages\n",
       "k = 5\n",
       "D, I = index.search(q_vec, k)                # distances, ids\n",
       "passages = [corpus[i] for i in I[0]]\n",
       "\n",
       "# 4️⃣ Build the prompt\n",
       "prompt = f\"\"\"Answer the question using only the information below.\n",
       "Question: {query}\n",
       "Context:\n",
       "{chr(10).join([f\"[{i+1}] {p}\" for i, p in enumerate(passages)])}\n",
       "Answer:\"\"\"\n",
       "\n",
       "# 5️⃣ Generate with an LLM (e.g., OpenAI API)\n",
       "import openai\n",
       "response = openai.ChatCompletion.create(\n",
       "    model=\"gpt-4o-mini\",\n",
       "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
       "    temperature=0.2,\n",
       "    max_tokens=300,\n",
       ")\n",
       "\n",
       "print(response.choices[0].message.content)\n",
       "```\n",
       "\n",
       "*The code illustrates the classic “retrieve‑then‑generate” pattern; production systems typically add reranking, citation extraction, and safety filters.*\n",
       "\n",
       "---\n",
       "\n",
       "### 9. Key References (selected)\n",
       "\n",
       "| Year | Citation |\n",
       "|------|----------|\n",
       "| 2020 | **Lewis et al.** “Retrieval‑Augmented Generation for Knowledge‑Intensive NLP Tasks” (RAG). |\n",
       "| 2021 | **Karpukhin et al.** “Dense Passage Retrieval for Open‑Domain Question Answering”. |\n",
       "| 2022 | **Izacard & Grave** “Leveraging Passage Retrieval with Generative Models”. |\n",
       "| 2023 | **Gao et al.** “Self‑RAG: Generating Retrieval Queries from the Model Itself”. |\n",
       "| 2024 | **Wang et al.** “Multimodal Retrieval‑Augmented Generation”. |\n",
       "| 2025 | **OpenRAG** (open‑source framework) – https://github.com/openrag/openrag |\n",
       "\n",
       "---\n",
       "\n",
       "### 10. Take‑away Message  \n",
       "\n",
       "Retrieval‑Augmented Generation bridges the gap between **static, parametric knowledge** and **dynamic, external information**. By grounding LLM outputs in retrieved evidence, RAG delivers more factual, up‑to‑date, and explainable results while keeping model sizes manageable. Ongoing research focuses on tighter integration, multimodal evidence, and real‑time indexing—making RAG a cornerstone of next‑generation AI assistants and enterprise knowledge tools."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(result[\"text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85477094",
   "metadata": {},
   "source": [
    "That is a simple `LLMChain` using the traditional LangChain method. Now let's move onto LCEL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837cfb1b",
   "metadata": {},
   "source": [
    "**L**ang**C**hain **E**xpression **L**anguage (LCEL) is the recommended approach to building chains in LangChain. Having superceeded the traditional methods with `LLMChain`, etc. LCEL gives us a more flexible system for building chains. The pipe operator `|` is used by LCEL to _chain_ together components. Let's see how we'd construct an `LLMChain` using LCEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "69aa93e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lcel_chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba74ce0",
   "metadata": {},
   "source": [
    "We can `invoke` this chain in the same way as we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a8ace48a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 5 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1766275200000'}, 'provider_name': None}}, 'user_id': 'user_2iO7ni5EqyszYlcKGCHsy3SBaE3'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[133], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mlcel_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mretrieval augmented generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m display(Markdown(result))\n",
      "File \u001b[0;32m/mnt/c/Working/WORKING/github/Artificial-Intelligent-Skills/.venv/lib/python3.9/site-packages/langchain_core/runnables/base.py:3246\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3244\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3245\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3246\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3247\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3248\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/mnt/c/Working/WORKING/github/Artificial-Intelligent-Skills/.venv/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:395\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    391\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    392\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    394\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 395\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    405\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/mnt/c/Working/WORKING/github/Artificial-Intelligent-Skills/.venv/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:1025\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m   1018\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1023\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m   1024\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m-> 1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Working/WORKING/github/Artificial-Intelligent-Skills/.venv/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:842\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    841\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 842\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    848\u001b[0m         )\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    850\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/mnt/c/Working/WORKING/github/Artificial-Intelligent-Skills/.venv/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:1091\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1089\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1091\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1095\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/mnt/c/Working/WORKING/github/Artificial-Intelligent-Skills/.venv/lib/python3.9/site-packages/langchain_openai/chat_models/base.py:1213\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_response\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1212\u001b[0m         e\u001b[38;5;241m.\u001b[39mresponse \u001b[38;5;241m=\u001b[39m raw_response\u001b[38;5;241m.\u001b[39mhttp_response  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m-> 1213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_response_headers\n\u001b[1;32m   1216\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1218\u001b[0m ):\n\u001b[1;32m   1219\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response\u001b[38;5;241m.\u001b[39mheaders)}\n",
      "File \u001b[0;32m/mnt/c/Working/WORKING/github/Artificial-Intelligent-Skills/.venv/lib/python3.9/site-packages/langchain_openai/chat_models/base.py:1208\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _construct_lc_result_from_responses_api(\n\u001b[1;32m   1202\u001b[0m             response,\n\u001b[1;32m   1203\u001b[0m             schema\u001b[38;5;241m=\u001b[39moriginal_schema_obj,\n\u001b[1;32m   1204\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mgeneration_info,\n\u001b[1;32m   1205\u001b[0m             output_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_version,\n\u001b[1;32m   1206\u001b[0m         )\n\u001b[1;32m   1207\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1208\u001b[0m         raw_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_raw_response\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1209\u001b[0m         response \u001b[38;5;241m=\u001b[39m raw_response\u001b[38;5;241m.\u001b[39mparse()\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/mnt/c/Working/WORKING/github/Artificial-Intelligent-Skills/.venv/lib/python3.9/site-packages/openai/_legacy_response.py:364\u001b[0m, in \u001b[0;36mto_raw_response_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m extra_headers[RAW_RESPONSE_HEADER] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    362\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m extra_headers\n\u001b[0;32m--> 364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/mnt/c/Working/WORKING/github/Artificial-Intelligent-Skills/.venv/lib/python3.9/site-packages/openai/_utils/_utils.py:286\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Working/WORKING/github/Artificial-Intelligent-Skills/.venv/lib/python3.9/site-packages/openai/resources/chat/completions/completions.py:1192\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m   1147\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m not_given,\n\u001b[1;32m   1190\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m   1191\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m-> 1192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1211\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1212\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_cache_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1213\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_cache_retention\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1214\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1215\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1216\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msafety_identifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1218\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mverbosity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1230\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweb_search_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m   1238\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Working/WORKING/github/Artificial-Intelligent-Skills/.venv/lib/python3.9/site-packages/openai/_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1247\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1256\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1257\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1258\u001b[0m     )\n\u001b[0;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/mnt/c/Working/WORKING/github/Artificial-Intelligent-Skills/.venv/lib/python3.9/site-packages/openai/_base_client.py:1047\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1044\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1046\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1047\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 5 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1766275200000'}, 'provider_name': None}}, 'user_id': 'user_2iO7ni5EqyszYlcKGCHsy3SBaE3'}"
     ]
    }
   ],
   "source": [
    "result = lcel_chain.invoke(\"retrieval augmented generation\")\n",
    "\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d5d64c",
   "metadata": {},
   "source": [
    "##### How Does the Pipe Operator Work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb108ee7",
   "metadata": {},
   "source": [
    "Before moving onto other LCEL features, let's take a moment to understand what the pipe operator `|` is doing and _how_ it works.\n",
    "\n",
    "Functionality wise, the pipe tells you that whatever the _left_ side outputs will be fed as input into the _right_ side. In the example of `prompt | llm | output_parser`, we see that `prompt` feeds into `llm` feeds into `output_parser`.\n",
    "\n",
    "The pipe operator is a way of chaining together components, and is a way of saying that whatever the _left_ side outputs will be fed as input into the _right_ side.\n",
    "\n",
    "Let's make a basic class named `Runnable` that will transform our a provided function into a _runnable_ class that we will then use with the pipe `|` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "48101434",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runnable:\n",
    "    def __init__(self, func):\n",
    "        self.func = func\n",
    "    def __or__(self, other):\n",
    "        def chained_func(*args, **kwargs):\n",
    "            return other.invoke(self.func(*args, **kwargs))\n",
    "        return Runnable(chained_func)\n",
    "    def invoke(self, *args, **kwargs):\n",
    "        return self.func(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde31e61",
   "metadata": {},
   "source": [
    "With the `Runnable` class, we will be able wrap a function into the class, allowing us to then chain together multiple of these _runnable_ functions using the `__or__` method.\n",
    "\n",
    "First, let's create a few functions that we'll chain together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4819884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_five(x):\n",
    "    return x + 5\n",
    "\n",
    "def sub_five(x):\n",
    "    return x - 5\n",
    "\n",
    "def mul_five(x):\n",
    "    return x * 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9500b440",
   "metadata": {},
   "source": [
    "Now we wrap our functions with the `Runnable`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e8f77a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_five_runnable = Runnable(add_five)\n",
    "sub_five_runnable = Runnable(sub_five)\n",
    "mul_five_runnable = Runnable(mul_five)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359b195f",
   "metadata": {},
   "source": [
    "Finally, we can chain these together using the `__or__` method from the `Runnable` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "72af544f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = (add_five_runnable).__or__(sub_five_runnable).__or__(mul_five_runnable)\n",
    "\n",
    "chain.invoke(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae7d830",
   "metadata": {},
   "source": [
    "So we can see that we're able to chain together our functions using `__or__`. The pipe `|` operator is simply a shortcut for the `__or__` method, so we can create the exact same chain like so:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a1a03c",
   "metadata": {},
   "source": [
    "### LCEL `RunnableLambda`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0da2b3",
   "metadata": {},
   "source": [
    "The `RunnableLambda` class is LangChain's built-in method for constructing a _runnable_ object from a function. That is, it does the same thing as the custom `Runnable` class we created earlier. Let's try it out with the same functions as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a2fc05f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "add_five_runnable = RunnableLambda(add_five)\n",
    "sub_five_runnable = RunnableLambda(sub_five)\n",
    "mul_five_runnable = RunnableLambda(mul_five)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d43359",
   "metadata": {},
   "source": [
    "We chain these together again with the pipe `|` operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d91d0eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = add_five_runnable | sub_five_runnable | mul_five_runnable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9606d2",
   "metadata": {},
   "source": [
    "And call them using the `invoke` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c9325300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156fdefb",
   "metadata": {},
   "source": [
    "Now we want to try something a little more testing, so this time we will generate a report, and we will try and edit that report using this functionallity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "123ed549",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str = \"give me a small report about {topic}\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables = [\"topic\"],\n",
    "    template = prompt_str\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "49914a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca83520",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke(\"AI\")\n",
    "\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970cbba6",
   "metadata": {},
   "source": [
    "Here we are making two functions, `extract_fact` to pull out the main content of our text and `replace_word` that will replace AI with Skynet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1dec4713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fact(x):\n",
    "    if \"\\n\\n\" in x:\n",
    "        return \"\\n\".join(x.split(\"\\n\\n\")[1:])\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "old_word = \"AI\"\n",
    "new_word = \"skynet\"\n",
    "\n",
    "def replace_word(x):\n",
    "    return x.replace(old_word, new_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "aa5d8b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_fact_runnable = RunnableLambda(extract_fact)\n",
    "replace_word_runnable = RunnableLambda(replace_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b915e064",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser | extract_fact_runnable | replace_word_runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d90fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke(\"retrieval augmented generation\")\n",
    "\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2ebdee",
   "metadata": {},
   "source": [
    "Those are our `RunnableLambda` functions. It's worth noting that all inputs to these functions are expected to be a SINGLE arguments. If you have a function that accepts multiple arguments, you can input a dictionary with keys, then unpack them inside the function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b7e40f",
   "metadata": {},
   "source": [
    "### LCEL `RunnableParallel` and `RunnablePassthrough`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda7cf7d",
   "metadata": {},
   "source": [
    "LCEL provides us with various `Runnable` classes that allow us to control the flow of data and execution order through our chains. Two of these are `RunnableParallel` and `RunnablePassthrough`.\n",
    "\n",
    "* `RunnableParallel` — allows us to run multiple `Runnable` instances in parallel. Acting almost as a Y-fork in the chain.\n",
    "\n",
    "* `RunnablePassthrough` — allows us to pass through a variable to the next `Runnable` without modification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adeca31",
   "metadata": {},
   "source": [
    "To see these runnables in action, we will create two data sources, each source provides specific information but to answer the question we will need both to fed to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "daa1c566",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1108/246204494.py:4: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embedding = OpenAIEmbeddings()\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for OpenAIEmbeddings\n  Value error, Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. [type=value_error, input_value={'model_kwargs': {}, 'cli...20, 'http_client': None}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[146], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DocArrayInMemorySearch\n\u001b[0;32m----> 4\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m vecstore_a \u001b[38;5;241m=\u001b[39m DocArrayInMemorySearch\u001b[38;5;241m.\u001b[39mfrom_texts(\n\u001b[1;32m      7\u001b[0m     [\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhalf the info is here\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m embedding\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m vecstore_b \u001b[38;5;241m=\u001b[39m DocArrayInMemorySearch\u001b[38;5;241m.\u001b[39mfrom_texts(\n\u001b[1;32m     14\u001b[0m     [\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe other half of the info is here\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m embedding\n\u001b[1;32m     19\u001b[0m )\n",
      "File \u001b[0;32m/mnt/c/Working/WORKING/github/Artificial-Intelligent-Skills/.venv/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:226\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    225\u001b[0m     emit_warning()\n\u001b[0;32m--> 226\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Working/WORKING/github/Artificial-Intelligent-Skills/.venv/lib/python3.9/site-packages/pydantic/main.py:250\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    249\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[1;32m    252\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    256\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    257\u001b[0m     )\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for OpenAIEmbeddings\n  Value error, Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. [type=value_error, input_value={'model_kwargs': {}, 'cli...20, 'http_client': None}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/value_error"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "# If you're broke, use huggingface embeddings or find free alternatives\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "vecstore_a = DocArrayInMemorySearch.from_texts(\n",
    "    [\n",
    "        \"half the info is here\",\n",
    "        \"DeepSeek-V3 was released in December 2024\"\n",
    "    ],\n",
    "    embedding = embedding\n",
    ")\n",
    "vecstore_b = DocArrayInMemorySearch.from_texts(\n",
    "    [\n",
    "        \"the other half of the info is here\",\n",
    "        \"the DeepSeek-V3 LLM is a mixture of experts model with 671B parameters\"\n",
    "    ],\n",
    "    embedding = embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118b10ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str = \"\"\"Using the context provided, answer the user's question.\n",
    "Context:\n",
    "{context_a}\n",
    "{context_b}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688029ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(prompt_str),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66038d5",
   "metadata": {},
   "source": [
    "Here we are wrapping our vector stores as retrievers so they can be fitted into one big retrieval variable to be used by the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e31587",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "\n",
    "retriever_a = vecstore_a.as_retriever()\n",
    "retriever_b = vecstore_b.as_retriever()\n",
    "\n",
    "retrieval = RunnableParallel(\n",
    "    {\n",
    "        \"context_a\": retriever_a, \"context_b\": retriever_b, \"question\": RunnablePassthrough()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc57be13",
   "metadata": {},
   "source": [
    "The chain we'll be constructing will look something like this:\n",
    "\n",
    "![](https://github.com/aurelio-labs/langchain-course/blob/main/assets/lcel-flow.png?raw=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c1f09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = retrieval | prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf92250",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke(\n",
    "    \"what architecture does the model DeepSeek released in december use?\"\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2992d7d",
   "metadata": {},
   "source": [
    "With that we've seen how we can use `RunnableParallel` and `RunnablePassthrough` to control the flow of data and execution order through our chains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b02f57",
   "metadata": {},
   "source": [
    "# Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c983d96",
   "metadata": {},
   "source": [
    "### Streaming with `astream`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54558fdc",
   "metadata": {},
   "source": [
    "We will start by creating a aysnc stream from our LLM. We do this within an `async for` loop, allowing us to iterate through the chunks of data and use them as soon as the async `astream` method returns the tokens to us. By adding a pipe character `|` we can see the individual tokens that are generated. We set `flush` equal to `True` as this forces immediate output to the console, resulting in smoother streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efc1792",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import ConfigurableField\n",
    "\n",
    "llm_streaming = ChatOpenAI(temperature = 0.0, \n",
    "                           model = openai_model, \n",
    "                           api_key = getenv(\"OPENROUTER_API_KEY\"), \n",
    "                           base_url = \"https://openrouter.ai/api/v1\",\n",
    "                           streaming = True).configurable_fields(\n",
    "                               callbacks = ConfigurableField(\n",
    "                                    id = \"callbacks\",\n",
    "                                    name = \"Callbacks\",\n",
    "                                    description = \"The callbacks to use for streaming output\",\n",
    "                           ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce39f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "async for token in llm_streaming.astream(\"What is NLP?\"):\n",
    "    tokens.append(token)\n",
    "    print(token.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d4edc9",
   "metadata": {},
   "source": [
    "Since we appended each token to the `tokens` list, we can also see what is inside each and every token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e789ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c05d798",
   "metadata": {},
   "source": [
    "We can also merge multiple `AIMessageChunk` objects together with the `+` operator, creating a larger set of tokens / chunk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d645f17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[0] + tokens[1] + tokens[2] + tokens[3] + tokens[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376f8595",
   "metadata": {},
   "source": [
    "A word of caution, there is nothing preventing you from merging tokens in the incorrect order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690d6edf",
   "metadata": {},
   "source": [
    "### Streaming with Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739f3186",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent: RunnableSerializable = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "        \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_streaming.bind_tools(tools, tool_choice=\"any\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb224d4",
   "metadata": {},
   "source": [
    "Now, we will define our _custom_ callback handler. This will be a queue callback handler that will allow us to stream the output of the agent through an `asyncio.Queue` object and yield the tokens as they are generated elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f301e6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from langchain.callbacks.base import AsyncCallbackHandler\n",
    "\n",
    "\n",
    "class QueueCallbackHandler(AsyncCallbackHandler):\n",
    "    \"\"\"Callback handler that puts tokens into a queue.\"\"\"\n",
    "\n",
    "    def __init__(self, queue: asyncio.Queue):\n",
    "        self.queue = queue\n",
    "        self.final_answer_seen = False\n",
    "\n",
    "    async def __aiter__(self):\n",
    "        while True:\n",
    "            if self.queue.empty():\n",
    "                await asyncio.sleep(0.1)\n",
    "                continue\n",
    "            token_or_done = await self.queue.get()\n",
    "\n",
    "            if token_or_done == \"<<DONE>>\":\n",
    "                # this means we're done\n",
    "                return\n",
    "            if token_or_done:\n",
    "                yield token_or_done\n",
    "\n",
    "    async def on_llm_new_token(self, *args, **kwargs) -> None:\n",
    "        \"\"\"Put new token in the queue.\"\"\"\n",
    "        #print(f\"on_llm_new_token: {args}, {kwargs}\")\n",
    "        chunk = kwargs.get(\"chunk\")\n",
    "        if chunk:\n",
    "            # check for final_answer tool call\n",
    "            if tool_calls := chunk.message.additional_kwargs.get(\"tool_calls\"):\n",
    "                if tool_calls[0][\"function\"][\"name\"] == \"final_answer\":\n",
    "                    # this will allow the stream to end on the next `on_llm_end` call\n",
    "                    self.final_answer_seen = True\n",
    "        await self.queue.put(chunk)\n",
    "        return\n",
    "\n",
    "    async def on_llm_end(self, *args, **kwargs) -> None:\n",
    "        \"\"\"Put None in the queue to signal completion.\"\"\"\n",
    "        #print(f\"on_llm_end: {args}, {kwargs}\")\n",
    "        # this should only be used at the end of our agent execution, however LangChain\n",
    "        # will call this at the end of every tool call, not just the final tool call\n",
    "        # so we must only send the \"done\" signal if we have already seen the final_answer\n",
    "        # tool call\n",
    "        if self.final_answer_seen:\n",
    "            await self.queue.put(\"<<DONE>>\")\n",
    "        else:\n",
    "            await self.queue.put(\"<<STEP_END>>\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09fe258",
   "metadata": {},
   "source": [
    "We can see how this works together in our `agent` invocation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18ee742",
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = asyncio.Queue()\n",
    "streamer = QueueCallbackHandler(queue)\n",
    "\n",
    "tokens = []\n",
    "\n",
    "async def stream(query: str):\n",
    "    response = agent.with_config(\n",
    "        callbacks=[streamer]\n",
    "    )\n",
    "    async for token in response.astream({\n",
    "        \"input\": query,\n",
    "        \"chat_history\": [],\n",
    "        \"agent_scratchpad\": []\n",
    "    }):\n",
    "        tokens.append(token)\n",
    "        print(token, flush=True)\n",
    "\n",
    "await stream(\"What is 10 + 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b09b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = tokens[0]\n",
    "\n",
    "for token in tokens[1:]:\n",
    "    tk += token\n",
    "\n",
    "tk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bf9a37",
   "metadata": {},
   "source": [
    "Now we're seeing that the output is being streamed token-by-token. Because we're being streamed a tool call the `content` field is empty. Instead, we can see the tokens being added inside the `tool_calls` fields, within `id`, `function.name`, and `function.arguments`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5476931f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "class CustomAgentExecutor:\n",
    "    chat_history: list[BaseMessage]\n",
    "\n",
    "    def __init__(self, max_iterations: int = 3):\n",
    "        self.chat_history = []\n",
    "        self.max_iterations = max_iterations\n",
    "        self.agent: RunnableSerializable = (\n",
    "            {\n",
    "                \"input\": lambda x: x[\"input\"],\n",
    "                \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "                \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
    "            }\n",
    "            | prompt\n",
    "            | llm.bind_tools(tools, tool_choice=\"any\")  # we're forcing tool use again\n",
    "        )\n",
    "\n",
    "    async def invoke(self, input: str, streamer: QueueCallbackHandler, verbose: bool = False) -> dict:\n",
    "        # invoke the agent but we do this iteratively in a loop until\n",
    "        # reaching a final answer\n",
    "        count = 0\n",
    "        agent_scratchpad = []\n",
    "        while count < self.max_iterations:\n",
    "            # invoke a step for the agent to generate a tool call\n",
    "            async def stream(query: str):\n",
    "                response = self.agent.with_config(\n",
    "                    callbacks = [streamer]\n",
    "                )\n",
    "                # we initialize the output dictionary that we will be populating with\n",
    "                # our streamed output\n",
    "                output = None\n",
    "                # now we begin streaming\n",
    "                async for token in response.astream({\n",
    "                    \"input\": query,\n",
    "                    \"chat_history\": self.chat_history,\n",
    "                    \"agent_scratchpad\": agent_scratchpad\n",
    "                }):\n",
    "                    if output is None:\n",
    "                        output = token\n",
    "                    else:\n",
    "                        # we can just add the tokens together as they are streamed and\n",
    "                        # we'll have the full response object at the end\n",
    "                        output += token\n",
    "                    if token.content != \"\":\n",
    "                        # we can capture various parts of the response object\n",
    "                        if verbose: print(f\"content: {token.content}\", flush=True)\n",
    "                    tool_calls = token.additional_kwargs.get(\"tool_calls\")\n",
    "                    if tool_calls:\n",
    "                        if verbose: print(f\"tool_calls: {tool_calls}\", flush=True)\n",
    "                        tool_name = tool_calls[0][\"function\"][\"name\"]\n",
    "                        if tool_name:\n",
    "                            if verbose: print(f\"tool_name: {tool_name}\", flush=True)\n",
    "                        arg = tool_calls[0][\"function\"][\"arguments\"]\n",
    "                        if arg != \"\":\n",
    "                            if verbose: print(f\"arg: {arg}\", flush=True)\n",
    "                return AIMessage(\n",
    "                    content = output.content,\n",
    "                    tool_calls = output.tool_calls,\n",
    "                    tool_call_id = output.tool_calls[0][\"id\"]\n",
    "                )\n",
    "\n",
    "            tool_call = await stream(query = input)\n",
    "            # add initial tool call to scratchpad\n",
    "            agent_scratchpad.append(tool_call)\n",
    "            # otherwise we execute the tool and add it's output to the agent scratchpad\n",
    "            tool_name = tool_call.tool_calls[0][\"name\"]\n",
    "            tool_args = tool_call.tool_calls[0][\"args\"]\n",
    "            tool_call_id = tool_call.tool_call_id\n",
    "            tool_out = name2tool[tool_name](**tool_args)\n",
    "            # add the tool output to the agent scratchpad\n",
    "            tool_exec = ToolMessage(\n",
    "                content = f\"{tool_out}\",\n",
    "                tool_call_id = tool_call_id\n",
    "            )\n",
    "            agent_scratchpad.append(tool_exec)\n",
    "            count += 1\n",
    "            # if the tool call is the final answer tool, we stop\n",
    "            if tool_name == \"final_answer\":\n",
    "                break\n",
    "        # add the final output to the chat history, we only add the \"answer\" field\n",
    "        final_answer = tool_out[\"answer\"]\n",
    "        self.chat_history.extend([\n",
    "            HumanMessage(content = input),\n",
    "            AIMessage(content = final_answer)\n",
    "        ])\n",
    "        # return the final answer in dict form\n",
    "        return tool_args\n",
    "\n",
    "agent_executor = CustomAgentExecutor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc230e1e",
   "metadata": {},
   "source": [
    "We've added a few `print` statements to help us see what is being output, we activate those by setting `verbose=True`. Let's see what is returned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e458029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = asyncio.Queue()\n",
    "streamer = QueueCallbackHandler(queue)\n",
    "\n",
    "out = await agent_executor.invoke(\"What is 10 + 10\", streamer, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6bb846",
   "metadata": {},
   "source": [
    "We can see what is being output through the `verbose=True` flag. However, if we do _not_ `print` the output, we will see nothing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6c9f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = asyncio.Queue()\n",
    "streamer = QueueCallbackHandler(queue)\n",
    "\n",
    "out = await agent_executor.invoke(\"What is 10 + 10\", streamer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563b232f",
   "metadata": {},
   "source": [
    "Although we see nothing, it does not mean that nothing is being returned to us - we're just not using our callback handler and `asyncio.Queue`. To use these we create an `asyncio` task, iterate over the `__aiter__` method of our `streamer` object, and await the task, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3212a31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = asyncio.Queue()\n",
    "streamer = QueueCallbackHandler(queue)\n",
    "\n",
    "task = asyncio.create_task(agent_executor.invoke(\"What is 10 + 10\", streamer))\n",
    "\n",
    "async for token in streamer:\n",
    "    print(token, flush=True)\n",
    "\n",
    "await task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efc8899",
   "metadata": {},
   "source": [
    "Although this seems like a lot of work, we're now streaming tokens in a way that allows us to pass these tokens on to other parts of our code - such as through a websocket, streamed API response, or some downstream processing.\n",
    "\n",
    "Let's try this out, we'll put together some simple post-processing to allow us to more nicely format the streamed output from out agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43615232",
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = asyncio.Queue()\n",
    "streamer = QueueCallbackHandler(queue)\n",
    "\n",
    "task = asyncio.create_task(agent_executor.invoke(\"What is 10 + 10\", streamer))\n",
    "\n",
    "async for token in streamer:\n",
    "    # first identify if we have a <<STEP_END>> token\n",
    "    if token == \"<<STEP_END>>\":\n",
    "        print(\"\\n\", flush=True)\n",
    "    # we'll first identify if the token is a tool call\n",
    "    elif tool_calls := token.message.additional_kwargs.get(\"tool_calls\"):\n",
    "        # if we have a tool call with a tool name, we'll print it\n",
    "        if tool_name := tool_calls[0][\"function\"][\"name\"]:\n",
    "            print(f\"Calling {tool_name}...\", flush=True)\n",
    "        # if we have a tool call with arguments, we ad them to our args string\n",
    "        if tool_args := tool_calls[0][\"function\"][\"arguments\"]:\n",
    "            print(f\"{tool_args}\", end=\"\", flush=True)\n",
    "\n",
    "_ = await task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df45f9bb",
   "metadata": {},
   "source": [
    "With that we've produced a nice streaming output within our notebook - which ofcourse can be applied with very similar logic elsewhere, such as within a more polished web app."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Artificial-Intelligent-Skills",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
