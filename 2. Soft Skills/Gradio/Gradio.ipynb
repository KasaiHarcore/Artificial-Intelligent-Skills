{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d0e0d68",
   "metadata": {},
   "source": [
    "**NOTE**: I notice that Gradio docs is very good to follow not like the other so I will no further make any type of tutorial here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5d2cd3",
   "metadata": {},
   "source": [
    "# Code Section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0106fdf0",
   "metadata": {},
   "source": [
    "***All in one code (all thing in Gradio)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6ef2f876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from getpass import getpass\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from datetime import date, datetime\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from IPython.display import display, Markdown\n",
    "import json\n",
    "\n",
    "from typing import *\n",
    "from pydantic import *\n",
    "\n",
    "openai_model = \"openai/gpt-oss-120b:free\"\n",
    "os.environ[\"OPENROUTER_API_KEY\"] = os.getenv(\"OPENROUTER_API_KEY\") or getpass(\n",
    "    \"Enter OpenRouter API Key: \",\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model = openai_model,\n",
    "    base_url = \"https://openrouter.ai/api/v1\",\n",
    "    api_key = os.environ[\"OPENROUTER_API_KEY\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb208e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0 # In Gradio, this call Global State\n",
    "\n",
    "def clear(evt_data: gr.EventData):\n",
    "    return \"\"\n",
    "\n",
    "def generate_response(user_input: str, intensity: Optional[int] = 1) -> Tuple[str, str, int]:\n",
    "    global count\n",
    "    system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        You are a helpful assistant. Please considering the \\\"!\\\" in the prompt as an type of explaination indicator strength as user needs.\n",
    "        - \\\"!\\\": For simple explaination\n",
    "        - \\\"!!\\\": For detailed explaination\n",
    "        - \\\"!!!\\\": For very detailed explaination with examples\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    prefix = \"!\" * intensity\n",
    "\n",
    "    user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        {prefix}\n",
    "\n",
    "        {user_input}\n",
    "        \"\"\",\n",
    "        input_variables=[\"user_input\", \"prefix\"],\n",
    "    )\n",
    "    full = ChatPromptTemplate.from_messages([system_prompt, user_prompt])\n",
    "    messages = full.format_prompt(user_input=user_input, prefix=prefix).to_messages()\n",
    "    response = llm.invoke(messages)\n",
    "    count += 1 \n",
    "    return response.content, openai_model, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ee470d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "if time_now[11:13] >= \"12\":\n",
    "    greeting = \"Good afternoon! â˜€ï¸\"\n",
    "elif time_now[11:13] >= \"18\":\n",
    "    greeting = \"Good evening! ðŸŒ‡\"\n",
    "else:\n",
    "    greeting = \"Good morning! ðŸŒ…\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21c5d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will cache examples in '/mnt/c/Working/WORKING/github/Artificial-Intelligent-Skills/2. Soft Skills/Gradio/gradio_cached_examples/2326' directory at first use. \n",
      "\n",
      "\n",
      "Running on local URL:  http://127.0.0.1:7930\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7930/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the Standard Interface\n",
    "# Interface can be exist in 4 type as:\n",
    "# - Standard: gr.Interface(fn, inputs, outputs) - Normal behavior\n",
    "# - Output-only: gr.Interface(fn, None, outputs)\n",
    "# - Input-only: gr.Interface(fn, inputs, None)\n",
    "# - Input/Output: gr.Interface(fn inputs, inputs) - This means that the output produced overrides the input\n",
    "\n",
    "from matplotlib import interactive\n",
    "from platformdirs import user_music_dir\n",
    "\n",
    "\n",
    "chat_simple = gr.Interface(\n",
    "    fn = generate_response,\n",
    "    inputs = [\n",
    "        gr.Textbox(lines = 10, label = \"Enter your question here\", placeholder = f\"{greeting} What do you want to know?\"),\n",
    "        # The `lines` option can be tune with `visible = True` to fully hide or show the component\n",
    "    ],\n",
    "    outputs = [\n",
    "        gr.Textbox(lines = 10, label = \"Response\"),\n",
    "        # gr.skip() can be use like `interactive = False` to prevent output updating (Read docs for more)\n",
    "        gr.Text(label = \"Model Used\", interactive = False),\n",
    "        gr.Number(label = \"Response Gen Time\", interactive = True)\n",
    "    ],\n",
    "    additional_inputs = [\n",
    "        gr.Slider(minimum = 1, maximum = 3, step = 1, label = \"Explanation Intensity\", value = 1)\n",
    "    ],\n",
    "    title = \"LLM testing with Gradio\",\n",
    "    description = \"A simple interface to test LLM responses using Gradio.\",\n",
    "    article = \"Adjust the 'Explanation Intensity' slider to control the detail level of the response.\",\n",
    "    examples = [\n",
    "        [\"Explain the theory of relativity.\", 2],\n",
    "        [\"What is quantum computing?\", 3],\n",
    "        [\"How does a blockchain work?\", 1],\n",
    "    ],\n",
    "    cache_examples = \"lazy\", # To run when pressed or set to True but it take time to init at first\n",
    "    flagging_dir = \"./flagged\", # The default will be the same directory as the code run\n",
    "\n",
    "    # live = True, # This will automatically trigger the running function when the input changing default is False\n",
    "    )\n",
    "\n",
    "# Or in more advanced with Block for further customizations\n",
    "with gr.Blocks() as chat_complicated:\n",
    "    gr.Markdown(\"# LLM testing with Gradio Blocks\")\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        A simple interface to test LLM responses using Gradio Blocks.\n",
    "        \n",
    "        Adjust the 'Explanation Intensity' slider to control the detail level of the response.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # Some how error at 4.44.1 (python 3.9.29) which what I use for making this\n",
    "    # with gr.Sidebar(position = \"left\"):\n",
    "    #    gr.Dropdown(\n",
    "    #        choice = [\"Chat\", \"History\", \"API\"],\n",
    "    #        label = \"Mode\",\n",
    "    #        value = \"Chat\"\n",
    "    #    )\n",
    "\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Tab(\"Conversation\"):\n",
    "            with gr.Column():\n",
    "\n",
    "                user_input = gr.Textbox(lines = 10, label = \"Enter your question here\", placeholder = f\"{greeting} What do you want to know?\")\n",
    "                # msg.change(fn, inputs, outputs) # Can be use change event to trigger function when input change\n",
    "                # The method in comment above in gr.Blocks play the same role as `live = True` in Interface\n",
    "\n",
    "                # Use method below in higher version to make it work\n",
    "                # Work like example like inference\n",
    "                # gr.Example([\"I want to know about LLM\", \"I want to search for good Quantization method\"], user_input)\n",
    "                # user_input.render()\n",
    "                with gr.Accordion(\"Additional Feature\", open = False): # Work like `additional_inputs` option in Interface\n",
    "                    intensity = gr.Slider(minimum = 1, maximum = 3, step = 1, label = \"Explanation Intensity\", value = 1)\n",
    "                submit_btn = gr.Button(\"Submit\")\n",
    "            with gr.Column():\n",
    "                response_output = gr.Textbox(lines = 10, label = \"Response\")\n",
    "                model_output = gr.Textbox(label = \"Model Used\")\n",
    "                count_output = gr.Number(label = \"Response Gen Time\")\n",
    "        with gr.Tab(\"Event Listener\"):\n",
    "            # Test fun_box with event listener\n",
    "            def test_fun_box(text: str):\n",
    "                return text + \"!\"\n",
    "            def test_fun_box_again(text: str):\n",
    "                return text + \"abcd\"\n",
    "            fun_box = gr.Textbox(label = \"event listener\")\n",
    "            fun_box.submit(fn = test_fun_box, inputs = fun_box, outputs = gr.Text(label = \"First Run\")).then(\n",
    "                fn = test_fun_box_again, inputs = fun_box, outputs = gr.Text(label = \"then operator\")\n",
    "            )\n",
    "            # .success() have a same way but run only if the previous is runable\n",
    "            # .failed() run if fail to trigger previous\n",
    "\n",
    "    submit_btn.click(\n",
    "        fn = generate_response,\n",
    "        inputs = [user_input, intensity],\n",
    "        outputs = [response_output, model_output, count_output],\n",
    "    )\n",
    "\n",
    "    # Working flow with history implementation\n",
    "    # Save history button\n",
    "    history = gr.State([]) # Season State / Mostly used for storing object while running Gradio\n",
    "    save_button = gr.Button(\"Save History to File\")\n",
    "    save_status = gr.Textbox(label = \"Save Status\", interactive = False)\n",
    "    \n",
    "    @save_button.click(inputs = [history, user_input, response_output], outputs = [history, save_status])\n",
    "    def save_history(history: List[Dict], user_msg: str, assistant_msg: str):\n",
    "        # Add current conversation to history\n",
    "        if user_msg and assistant_msg:  # Only add if both exist\n",
    "            entry = {\n",
    "                \"number\": len(history) + 1,\n",
    "                \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"conversation\": [\n",
    "                    {\"role\": \"user\", \"input\": user_msg},\n",
    "                    {\"role\": \"assistant\", \"output\": assistant_msg},\n",
    "                ],\n",
    "            }\n",
    "            history.append(entry)\n",
    "        \n",
    "        # Save to file\n",
    "        try:\n",
    "            filename = f\"chat_history_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(history, f, indent=2, ensure_ascii=False)\n",
    "            status_message = f\"âœ“ History saved successfully to {filename}\"\n",
    "        except Exception as e:\n",
    "            status_message = f\"âœ— Error saving history: {str(e)}\"\n",
    "        \n",
    "        return history, status_message\n",
    "    \n",
    "    \n",
    "chat_complicated.launch(share = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Artificial-Intelligent-Skills",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
