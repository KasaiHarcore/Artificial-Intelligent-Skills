{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b56969e",
   "metadata": {},
   "source": [
    "# Top"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff159d9",
   "metadata": {},
   "source": [
    "We've already learned how to look at logs for k8s pods, but sometimes that's not enough when it comes to debugging. Sometimes we want to know about the resources that a pod is using."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dffd20d",
   "metadata": {},
   "source": [
    "To get metrics working, we need to enable the `metrics-server` addon. Run:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9901caf0",
   "metadata": {},
   "source": [
    "```python\n",
    "minikube addons enable metrics-server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8ca925",
   "metadata": {},
   "source": [
    "Take a look inside the `kube-system` namespace:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbebe7e7",
   "metadata": {},
   "source": [
    "```python\n",
    "kubectl -n kube-system get pod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70e76cb",
   "metadata": {},
   "source": [
    "You should see a new \"metrics-server\" pod. It *might take a couple of minutes to get started*, but once that pod is ready, you should be able to run:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daa5123",
   "metadata": {},
   "source": [
    "```python\n",
    "kubectl top pod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716dc322",
   "metadata": {},
   "source": [
    "You should see something like this:\n",
    "\n",
    "```\n",
    "NAME                               CPU(cores)   MEMORY(bytes)\n",
    "synergychat-api-76b796b58d-x5wpk   1m           14Mi\n",
    "synergychat-web-846d86c444-d9c8q   1m           15Mi\n",
    "synergychat-web-846d86c444-sk6n4   1m           15Mi\n",
    "synergychat-web-846d86c444-w2pqg   1m           15Mi\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251a94eb",
   "metadata": {},
   "source": [
    "The `kubectl top` command (just like the [unix top command](https://en.wikipedia.org/wiki/Top_(software))) will show you the resources that each pod is using. In the example above, each pod is using about 1 milliCPU and 15 megabytes of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37a1fd5",
   "metadata": {},
   "source": [
    "# Vertical and Horizontal Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fea3a8",
   "metadata": {},
   "source": [
    "Generally speaking, there are two ways to scale an application: vertically and horizontally. When I say \"Scaling\", I'm talking about increasing the capacity of an application. For example, maybe we have a web server, and to handle roughly 1000 requests per second, it uses about:\n",
    "- 1/2 of a CPU core\n",
    "- 1 GB of RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276f6fb0",
   "metadata": {},
   "source": [
    "If we want to \"scale up\" to handle 2000 requests per second, we could double the CPU and RAM:\n",
    "- 1 CPU core\n",
    "- 2 GB of RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df08cd14",
   "metadata": {},
   "source": [
    "This is called \"vertical scaling\" because we're increasing the capacity of the application by increasing the resources available to it. We're scaling up. Scaling up works until it doesn't. You can only scale up as much as your hardware will allow (the maximum number of CPUs and amount of RAM your node has)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92ad43a",
   "metadata": {},
   "source": [
    "The other way to scale is horizontally. Instead of increasing the resources available to the application, we increase the number of instances of the application (pods). Pods can be distributed across nodes, so we can scale horizontally until we run out of nodes. When working in a system like Kubernetes, it's generally better to scale horizontally than vertically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad7103d",
   "metadata": {},
   "source": [
    "# Resource Limits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87fea16",
   "metadata": {},
   "source": [
    "None of our current deployments have any resource limits set. We have very little traffic, so it's not currently an issue, but in a production environment, we would want to set resource limits to ensure that our pods don't consume too many resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b8d2a0",
   "metadata": {},
   "source": [
    "We wouldn't want a pod to hog all the CPU and RAM on its node, suffocating all of the other pods on the node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578018a8",
   "metadata": {},
   "source": [
    "### Setting Limits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766855d2",
   "metadata": {},
   "source": [
    "We can set [resource limits](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/) in our deployment files. Here's an example:\n",
    "\n",
    "```yaml\n",
    "spec:\n",
    "  containers:\n",
    "    - name: <container-name>\n",
    "      image: <image-name>\n",
    "      resources:\n",
    "        limits:\n",
    "          memory: <max-memory>\n",
    "          cpu: <max-cpu>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bca5a4e",
   "metadata": {},
   "source": [
    "Memory is measured in bytes, so we can use the suffixes Ki, Mi, and Gi to specify [kibibytes, mebibytes, and gibibytes](https://en.wikipedia.org/wiki/Byte#Multiple-byte_units), respectively. For example, 512Mi is 512 mebibytes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50262e1",
   "metadata": {},
   "source": [
    "CPU is [measured in cores](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#meaning-of-cpu), so we can use the suffix `m` to specify milli-cores. For example, `500m` is 500 milli-cores, or 0.5 cores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a78540c",
   "metadata": {},
   "source": [
    "It would be really hard to test resource limits with our SynergyChat web application because we have no production traffic. Instead, I've created a couple of custom applications we can use to test and debug resource limits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a20cc4",
   "metadata": {},
   "source": [
    "The `bootdotdev/synergychat-testcpu:latest` image on Docker Hub is an application that simply consumes as much CPU power as it can."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ec83b7",
   "metadata": {},
   "source": [
    "Create a new file called `testcpu-deployment.yaml` with the following:\n",
    "\n",
    "```yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  labels:\n",
    "    app: synergychat-testcpu\n",
    "  name: synergychat-testcpu\n",
    "spec:\n",
    "  replicas: 1\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: synergychat-testcpu\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: synergychat-testcpu\n",
    "    spec:\n",
    "      containers:\n",
    "        - image: bootdotdev/synergychat-testcpu:latest\n",
    "          name: synergychat-testcpu\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94db36e2",
   "metadata": {},
   "source": [
    "Add a CPU limit of `50m` to the deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa7166d",
   "metadata": {},
   "source": [
    "Apply the deployment, then make sure the pod is running:\n",
    "\n",
    "```python\n",
    "kubectl get pod\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94b916b",
   "metadata": {},
   "source": [
    "It might take a minute or so, but soon you should be able to see its metrics with top:\n",
    "\n",
    "```python\n",
    "kubectl top pod\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5bd99c",
   "metadata": {},
   "source": [
    "Assuming everything is working properly, you should see that the pod is using about 50 milli-cores of CPU. That's because k8s is throttling the pod to ensure that it doesn't use more than 50 milli-cores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25387cd",
   "metadata": {},
   "source": [
    "# Limits RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227aa80d",
   "metadata": {},
   "source": [
    "Create a new file called `testram-deployment.yaml` with the following"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9239bab6",
   "metadata": {},
   "source": [
    "```yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  labels:\n",
    "    app: synergychat-testram\n",
    "  name: synergychat-testram\n",
    "spec:\n",
    "  replicas: 1\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: synergychat-testram\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: synergychat-testram\n",
    "    spec:\n",
    "      containers:\n",
    "        - image: bootdotdev/synergychat-testram:latest\n",
    "          name: synergychat-testram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08072095",
   "metadata": {},
   "source": [
    "Add a memory limit of `256Mi` (256 Megabytes) to the deployment. Remember, this is the [syntax](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19a4cef",
   "metadata": {},
   "source": [
    "Then create a ConfigMap called `testram-configmap.yaml`:\n",
    "\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: ConfigMap\n",
    "metadata:\n",
    "  name: synergychat-testram-configmap\n",
    "data:\n",
    "  MEGABYTES: \"200\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a62a95",
   "metadata": {},
   "source": [
    "That will tell the application to allocate 200 megabytes of memory. Update the deployment to use the config map, then apply both."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd317982",
   "metadata": {},
   "source": [
    "# Breaking the Limits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf93dc5",
   "metadata": {},
   "source": [
    "You may have noticed that with the `testcpu` application, we never \"told\" the application how much CPU to use. That's because generally speaking, applications don't know how much CPU they should use. They just go as \"fast\" as they can when they're doing computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4374f110",
   "metadata": {},
   "source": [
    "Memory is different, applications allocate memory based on a variety of factors, and while an application can have its CPU throttled and just \"go slower\", if an application runs out of available memory, it will crash."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec25f6f",
   "metadata": {},
   "source": [
    "Update the `MEGABYTES` environment variable for the `testram` application to `500` and apply the change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3ecd11",
   "metadata": {},
   "source": [
    "Delete the `testram` pod so that the new environment variable takes effect. Assuming you did everything correctly, the pod should crash. You'll be able to check with:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5363df",
   "metadata": {},
   "source": [
    "```python\n",
    "kubectl describe pod <pod-name>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53f32f1",
   "metadata": {},
   "source": [
    "Look for a section in the output look like:\n",
    "\n",
    "```\n",
    "Containers:\n",
    "  synergychat-testram:\n",
    "    Container ID:   docker://453facc1515e05ec553ad755c6a0edffd3e67b62c14b4ddb328cc0f8d5c67250\n",
    "    Image:          bootdotdev/synergychat-testram:latest\n",
    "    Image ID:       docker-pullable://bootdotdev/synergychat-testram@sha256:a127779899f29d7b2e1fc80ed75e001eaed8e7cec0985707a802319fcdd9bec1\n",
    "    Port:           <none>\n",
    "    Host Port:      <none>\n",
    "    State:          Waiting\n",
    "      Reason:       CrashLoopBackOff\n",
    "    Last State:     Terminated\n",
    "      Reason:       XXX\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceac6408",
   "metadata": {},
   "source": [
    "### Fix the Limits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d6b726",
   "metadata": {},
   "source": [
    "I don't want the pod to be consuming too much of your machine's resources, nor do I want you to have a constantly crashing pod, so before moving on, let's just reduce the memory usage of the `testram` pod.\n",
    "\n",
    "Set the `MEGABYTES` environment variable to `10` and apply the change, then delete the pod so that the new environment variable takes effect.\n",
    "\n",
    "Use `get pods` and `top pods` to make sure the pod is healthy and is using less than 10 megabytes of memory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a495b925",
   "metadata": {},
   "source": [
    "Stop the `testcpu` deployment from eating a big chunk of your machine's resources.\n",
    "\n",
    "Set its resource limit to `cpu: 10m`, and update the max replicas in the hpa to `1`. Apply the changes and watch as the number of pods scales down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2d2838",
   "metadata": {},
   "source": [
    "# Horizontal Pod Autoscaling (HPA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d080d50",
   "metadata": {},
   "source": [
    "A [HPA](https://kubernetes.io/docs/concepts/workloads/autoscaling/horizontal-pod-autoscale/) can automatically scale the number of Pods in a Deployment based on observed CPU utilization or other custom metrics. It's very common in a Kubernetes environment to have a low number of pods in a deployment, and then scale up the number of pods automatically as CPU usage increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8780a8d3",
   "metadata": {},
   "source": [
    "First, delete the `replicas: 1` line from the `testcpu` deployment. This will allow our new autoscaler to have full control over the number of pods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2538e7b5",
   "metadata": {},
   "source": [
    "Create a new file called `testcpu-hpa.yaml`. Add the following YAML to it:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c9e650",
   "metadata": {},
   "source": [
    "```yaml\n",
    "apiVersion: autoscaling/v1\n",
    "kind: HorizontalPodAutoscaler\n",
    "metadata:\n",
    "  name: testcpu-hpa\n",
    "spec:\n",
    "  scaleTargetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: x\n",
    "  minReplicas: x\n",
    "  maxReplicas: x\n",
    "  targetCPUUtilizationPercentage: x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4406e141",
   "metadata": {},
   "source": [
    "Set the following values:\n",
    "- `name`: The name of the `testcpu` deployment\n",
    "- `minReplicas: 1`\n",
    "- `maxReplicas: 4`\n",
    "- `targetCPUUtilizationPercentage: 50`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4350e89b",
   "metadata": {},
   "source": [
    "This hpa will monitor the CPU usage of the pods in the `testcpu` deployment. Its goal is to scale up or down the number of pods in the deployment so that the average CPU usage of all pods is around 50%. As CPU usage increases, it will add more pods. As CPU usage decreases, it will remove pods. You can find the algorithm it uses [here](https://kubernetes.io/docs/concepts/workloads/autoscaling/horizontal-pod-autoscale/#algorithm-details) if you're interested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8547c49b",
   "metadata": {},
   "source": [
    "Apply the hpa, then run the following commands every few seconds to watch as the number of pods scales up:\n",
    "\n",
    "```python\n",
    "kubectl get pods\n",
    "kubectl top pods\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19017ebb",
   "metadata": {},
   "source": [
    "An hpa is just another resource, so you can also use kubectl get hpa to see the current state of the autoscaler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a137c0",
   "metadata": {},
   "source": [
    "# HPA - Web"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf7615d",
   "metadata": {},
   "source": [
    "Now that you've seen how an application that chews through CPU will quickly scale up from a single pod to multiple pods, let's see what happens with an application that doesn't have much going on in terms of compute resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14ad532",
   "metadata": {},
   "source": [
    "Delete the line `\"replicas: 3\"` from the `web` deployment. This will allow our new autoscaler to have full control over the number of pods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86f9266",
   "metadata": {},
   "source": [
    "Copy your `testcpu-hpa.yaml` file and call it `web-hpa.yaml`. Update the following values:\n",
    "- `name: web-hpa`\n",
    "- Target the \"web\" deployment\n",
    "- Keep the scaling values the same"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
