{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a73a27e9",
   "metadata": {},
   "source": [
    "# Storage in Kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5efaa9",
   "metadata": {},
   "source": [
    "By default, containers running in pods on Kubernetes have access to the filesystem, but as we're about to find out, there are some big limitations to this.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e4faae",
   "metadata": {},
   "source": [
    "The `api` application in SynergyChat can be configured to save its data (the messages) to a file on the filesystem. That way, even if the program is restarted, the messages will still be there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d90feeb",
   "metadata": {},
   "source": [
    "Take a look at the logs of the api pod with:\n",
    "\n",
    "```python\n",
    "kubectl logs <podname>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8785fdb0",
   "metadata": {},
   "source": [
    "Let's do a quick test:\n",
    "1. Open the webpage\n",
    "2. Create few messages\n",
    "3. Refresh it\n",
    "4. All message should still be there\n",
    "5. Delete the `api` pod\n",
    "6. Once K8s replaces the deleted pod with a new one, refresh it again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfe29dc",
   "metadata": {},
   "source": [
    "Ok now all the messages are gone. Why? Because K8s treat everything running / saving/ storing in a pods is server memory (or we say ephemeral). That mean when a pod is deleted, all the memory gone with it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e0aaaf",
   "metadata": {},
   "source": [
    "This cover a good theory behind K8s or even a container: When thing goes wrong, we make another back up in a blank state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbc21c8",
   "metadata": {},
   "source": [
    "So what if we want to keep our memory instead of delete? (aka Persistant Storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c34d03b",
   "metadata": {},
   "source": [
    "# Ephemeral Volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dff77d2",
   "metadata": {},
   "source": [
    "On-disk files in a container are ephemeral as we saw before. This presents some problems for applications that want to save long-lived data across restarts. For example, user data in a database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7655b30",
   "metadata": {},
   "source": [
    "The Kubernetes [volume](https://kubernetes.io/docs/concepts/storage/volumes/) abstraction solves two primary problems:\n",
    "- Data persistence\n",
    "- Data sharing across containers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09739f87",
   "metadata": {},
   "source": [
    "After a short glance at the document, as it turns out, there are a lot of different types of \"volumes\" in Kubernetes. Some are even ephemeral as well, just like a container's standard filesystem. The primary reason for using an ephemeral volume is to share data between containers in a pod."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9364d48d",
   "metadata": {},
   "source": [
    "It's time to shift our focus back to the crawler service. The crawler service continuously crawls Project Gutenberg and exposes the information that it finds via a JSON API. That data is then made available via slash commands in the chat application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6731f420",
   "metadata": {},
   "source": [
    "Let's check how it do until now:\n",
    "\n",
    "```python\n",
    "kubectl logs <crawler-podname>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254d1a83",
   "metadata": {},
   "source": [
    "You should see some logs with timestamps that show you the crawler's progress."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988851af",
   "metadata": {},
   "source": [
    "Let's update the crawler deployment to use a volume that will be shared across all containers in the crawler pod, and scale up the number of containers in the pod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9548bd0a",
   "metadata": {},
   "source": [
    "```yaml\n",
    "volumes:\n",
    "  - name: cache-volume\n",
    "    emptyDir: {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd004dab",
   "metadata": {},
   "source": [
    "Add a new `vvolumeMounts` section to the container entry. This will mount the volume we just created at the `/cache` path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab097ce",
   "metadata": {},
   "source": [
    "```yaml\n",
    "volumeMounts:\n",
    "  - name: cache-volume\n",
    "    mountPath: /cache\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab17569f",
   "metadata": {},
   "source": [
    "Duplicate the entire first entry in the `containers` list twice (To make it 3 different containers). Update the name of each:\n",
    "1. `synergychat-crawler-1`\n",
    "2. `synergychat-crawler-2`\n",
    "3. `synergychat-crawler-3`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9affd67",
   "metadata": {},
   "source": [
    "Now all the containers in the pod will share the same volume at `/cache`. It's just an empty directory, but the crawler will use it to store its data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63eb9ddc",
   "metadata": {},
   "source": [
    "Add a `CRAWLER_DB_PATH` environment variable to the crawler's ConfigMap. Set it to `/cache/db`. The crawler will use a directory called `db` inside the volume to store its data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d112281",
   "metadata": {},
   "source": [
    "Apply the new ConfigMap and Deployment, and check the status of your new pod."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3a94d0",
   "metadata": {},
   "source": [
    "You should notice that there's a problem with the pod! Only 1/3 of containers should be \"ready\". Use the `logs` command to get the logs for all 3 containers:\n",
    "\n",
    "```python\n",
    "kubectl logs <podname> --all-containers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c696ee61",
   "metadata": {},
   "source": [
    "You should see something like this:\n",
    "\n",
    "```\n",
    "listen tcp :8080: bind: address already in use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2d0d26",
   "metadata": {},
   "source": [
    "Because pods share the same network namespace, they can't all bind to the same port! Hmm... let's put a band-aid on this by binding each container to a different port. `8080` is the only one that will be exposed via the service, but that's okay for now. We can add redundancy later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c30c06d",
   "metadata": {},
   "source": [
    "Add two new values to the crawler's ConfigMap:\n",
    "1. `CRAWLER_PORT_2: 8081`\n",
    "2. `CRAWLER_PORT_3: 8082`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7a4f29",
   "metadata": {},
   "source": [
    "Update the crawler deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19f54d2",
   "metadata": {},
   "source": [
    "Change the second and third containers to map `CRAWLER_PORT_2 -> CRAWLER_PORT` and `CRAWLER_PORT_3 -> CRAWLER_PORT` respectively (the Docker image expects a variable named \"CRAWLER_PORT\"). I'm not going to give you the code, but know that it's gonna be a bit tedious because you need to use `env:` instead of `envFrom:` for the second and third containers. Don't forget to continue exposing the `CRAWLER_KEYWORDS` and `CRAWLER_DB_PATH` environment variables for all containers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3d1531",
   "metadata": {},
   "source": [
    "# Containers in Pods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d456b253",
   "metadata": {},
   "source": [
    "It's important to remember that while it's common for a pod to run just a single container, multiple containers can run in a single pod. This is useful when you have containers that need to share resources. In other words, we can scale up the instances of an application either at the container level or at the pod level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b51f4f",
   "metadata": {},
   "source": [
    "In our situation, there will be:\n",
    "| Application | Pods | Containers |\n",
    "| ----------- | ---- | ---------- |\n",
    "| Web         | 3    | 3          |\n",
    "| Crawler     | 1    | 3          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da947433",
   "metadata": {},
   "source": [
    "# Persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fffe712",
   "metadata": {},
   "source": [
    "All the volumes we've worked with so far have been ephemeral, meaning when the associated pod is deleted the volume is deleted as well. This is fine for some use cases, but for most CRUD apps we want to persist data even if the pod is deleted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a9ba08",
   "metadata": {},
   "source": [
    "If you think about it, it's not even just when pods are explicitly deleted with `kubectl` that we need to worry about data loss. Pods can be deleted for several reasons:\n",
    "- The node they're running on could fail\n",
    "- A new version of the image was published (code was updated, etc)\n",
    "- A new node was added to the cluster and the pod was rescheduled\n",
    "\n",
    "In all of these cases, we want to make sure that our data is still available. [Persistent volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) allow us to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176d7ad4",
   "metadata": {},
   "source": [
    "### Persistent Volumes (PV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae10cf92",
   "metadata": {},
   "source": [
    "Instead of simply adding a volume to a deployment, a persistent volume is a cluster-level resource that is created separately from the pod and then attached to the pod. It's similar to a ConfigMap in that way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cef91b",
   "metadata": {},
   "source": [
    "PVs can be created statically or dynamically.\n",
    "- Static PVs are created manually by a cluster admin\n",
    "- Dynamic PVs are created automatically when a pod requests a volume that doesn't exist yet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c025ddf2",
   "metadata": {},
   "source": [
    "Generally speaking, and especially in the cloud-native world, we want to use dynamic PVs. It's less work and more flexible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350e0893",
   "metadata": {},
   "source": [
    "### Persistent Volume Claims (PVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b77c3a",
   "metadata": {},
   "source": [
    "A [persistent volume claim](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) is a request for a persistent volume. When using dynamic provisioning, a PVC will automatically create a PV if one doesn't exist that matches the claim.\n",
    "\n",
    "The PVC is then attached to a pod, just like a volume would be.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efbc1d3",
   "metadata": {},
   "source": [
    "Create a new file called `api-pvc.yaml` and add the following:\n",
    "\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: synergychat-api-pvc\n",
    "spec:\n",
    "  accessModes:\n",
    "    - ReadWriteOnce\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 1Gi\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928321e3",
   "metadata": {},
   "source": [
    "This creates a new PVC called `synergychat-api-pvc` with a few properties that can be read from and written to by multiple pods at the same time. It also requests 1GB of storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0746159d",
   "metadata": {},
   "source": [
    "Apply it and verify by:\n",
    "\n",
    "```python\n",
    "kubectl get pvc\n",
    "kubectl get pv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744ccaa0",
   "metadata": {},
   "source": [
    "# Attach Persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d0a34d",
   "metadata": {},
   "source": [
    "So far all we've done is create an empty persistent volume. Let's get the `api` application to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a23296",
   "metadata": {},
   "source": [
    "Create a new volume in the `api-deployment` referencing your pvc based on `crawler` deployment:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9194544",
   "metadata": {},
   "source": [
    "```yaml\n",
    "volumes:\n",
    "  - name: synergychat-api-volume\n",
    "    persistentVolumeClaim:\n",
    "      claimName: synergychat-api-pvc\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4482070",
   "metadata": {},
   "source": [
    "Then mount it in the container under the `/persist` directory:\n",
    "```yaml\n",
    "volumeMounts:\n",
    "  - name: synergychat-api-volume\n",
    "    mountPath: /persist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a3a2a1",
   "metadata": {},
   "source": [
    "Update the `API_DB_FILEPATH` environment variable you added earlier to instead use the new mount path: `/persist/db.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a66436",
   "metadata": {},
   "source": [
    "Apply the changes, then check to make sure all your pods are healthy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a3e488",
   "metadata": {},
   "source": [
    "With your tunnel running, open the web in browser and do:\n",
    "1. Send some messages.\n",
    "2. Delete the api pod.\n",
    "3. Once the new pod is running, refresh the page and make sure your messages are still there. If they are, your persistent volume is working! (Need time because this one is cold-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba1c5ec",
   "metadata": {},
   "source": [
    "# Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f61be0",
   "metadata": {},
   "source": [
    "Running application databases inside Kubernetes is not something you should do by default in every situation. Kubernetes is fundamentally optimized for managing stateless workloads, while databases are stateful systems that require careful handling of storage, backups, performance tuning, and failure recovery."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e530ec",
   "metadata": {},
   "source": [
    "For local development, testing, learning environments, or small non-critical systems, hosting a database inside Kubernetes can be practical and even beneficial because it simplifies environment setup and keeps everything self-contained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b00e8bd",
   "metadata": {},
   "source": [
    "However, for production workloads—especially those that are business-critical, data-intensive, or require high availability—it is generally better to run databases outside the Kubernetes cluster using managed database services or dedicated database infrastructure."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
