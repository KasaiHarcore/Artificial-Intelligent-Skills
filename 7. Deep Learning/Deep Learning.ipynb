{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "\n",
    "**Theory:** Neural Networks are a class of machine learning models inspired by biological neural networks. They are composed of multiple layers of interconnected neurons, which process and transmit information using electrical and chemical signals.\n",
    "\n",
    "**How It Works:**\n",
    "- Each neuron receives input signals from the previous layer and computes an output signal using an activation function.\n",
    "- The output signal is then passed on to the next layer, which performs the same computation until the output layer is reached.\n",
    "- The output layer produces the final output signal, which is used to make a prediction.\n",
    "\n",
    "**Support Functions:** Neural Networks rely on the calculation of outputs from each neuron and the backpropagation of errors to update the model parameters (weights).\n",
    "\n",
    "**Pros:**\n",
    "- Can learn non-linear relationships in data.\n",
    "- Can be used for both classification and regression tasks.\n",
    "- Can be used for both supervised and unsupervised learning.\n",
    "- Can be used for both structured and unstructured data.\n",
    "- Can be used for both numerical and categorical data.\n",
    "\n",
    "**Cons:**\n",
    "- Computationally expensive, especially with large datasets.\n",
    "- Requires a large amount of training data.\n",
    "- Requires careful preprocessing of data.\n",
    "- Prone to overfitting.\n",
    "- Difficult to interpret and explain.\n",
    "\n",
    "**Formula:**\n",
    "- Activation Function: The activation function of a neuron defines the output of that neuron given a set of inputs. It introduces non-linearity to the model, which allows it to learn non-linear relationships in data.\n",
    "\n",
    "$$ a = f(z) $$\n",
    "\n",
    "Where:\n",
    "- $a$ is the output of the neuron.\n",
    "- $z$ is the weighted sum of inputs to the neuron.\n",
    "- $f$ is the activation function.\n",
    "\n",
    "- Loss Function: The loss function of a neural network defines the difference between the predicted and actual values. It is used to measure the model's performance and guide the optimization process.\n",
    "\n",
    "**When to Use:**\n",
    "- Use Neural Networks when you want to learn non-linear relationships in your data and have a large amount of training data.\n",
    "- They are commonly used in image classification, natural language processing, and speech recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward propagation:\n",
    "Forward propagation is the process of transforming an input tensor to an output tensor. It's the core of neural networks and deep learning.\n",
    "\n",
    "**Theory:** Forward propagation is the process of transforming an input tensor to an output tensor. It's the core of neural networks and deep learning.\n",
    "\n",
    "**How It Works:**\n",
    "- Forward propagation is the process of transforming an input tensor to an output tensor.\n",
    "- It involves a series of mathematical operations that transform the input tensor into an output tensor.\n",
    "- The output tensor is then used to calculate the loss and update the model parameters during training.\n",
    "\n",
    "**Code:**\n",
    "- In TensorFlow, you can perform forward propagation using the `tf.keras` API:\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Define a Sequential model\n",
    "model = Sequential([\n",
    "    Dense(64, input_dim=num_features, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Perform forward propagation\n",
    "outputs = model(X)\n",
    "\n",
    "```\n",
    "\n",
    "- In PyTorch, you can perform forward propagation as follows:\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a custom model class\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, num_classes):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.fc3 = nn.Linear(hidden_dim2, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "# Create an instance of the model\n",
    "model = CustomModel(input_dim=num_features, hidden_dim1=64, hidden_dim2=32, num_classes=num_classes)\n",
    "\n",
    "# Perform forward propagation\n",
    "outputs = model(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coding Forward prop in Numpy (Advanced Learning Algorithms - Andrew Ng)\n",
    "import numpy as np\n",
    "\n",
    "def dense(a_in,W,b):\n",
    "    units = W.shape[1]\n",
    "    a_out = np.zeros(units)\n",
    "    for j in range(units):\n",
    "        w = W[:,j]\n",
    "        z = np.dot(w, a_in) + b[j]\n",
    "        a_out[j] = g(z) # g is the activation function\n",
    "    return a_out\n",
    "\n",
    "# We can short it by using matrix multiplication\n",
    "def dense(a_in,W,b):\n",
    "    z = np.dot(W, a_in) + b\n",
    "    a_out = g(z) # g is the activation function\n",
    "    return a_out\n",
    "\n",
    "# Vectorization\n",
    "def dense(a_in,W,b):\n",
    "    z = np.matmul(W, a_in) + b\n",
    "    a_out = g(z) # g is the activation function\n",
    "    return a_out\n",
    "\n",
    "# Forward working by passing the input into the first layer, then the output of the first layer into the second layer, and so on.\n",
    "def sequential(x): \n",
    "    a1 = dense(x, W1, b1)\n",
    "    a2 = dense(a1, W2, b2)\n",
    "    a3 = dense(a2, W3, b3)\n",
    "    a4 = dense(a3, W4, b4)\n",
    "    return a4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation:\n",
    "\n",
    "Backpropagation is a fundamental concept in neural network training. It's a key part of the training process that allows neural networks to update their weights and biases in order to minimize the error (or loss) between the predicted output and the actual target values. I'll explain the theory behind backpropagation and provide code examples for implementing it in TensorFlow and PyTorch.\n",
    "\n",
    "**Theory:** Backpropagation, short for \"backward propagation of errors,\" is a supervised learning algorithm used to train neural networks. It's based on the chain rule of calculus and allows the network to adjust its weights and biases during training in order to reduce the error between predictions and actual target values.\n",
    "\n",
    "**How It Works:**\n",
    "1. Forward Pass: During the forward pass, the input data is fed into the neural network, and the network computes the predicted output. This involves a series of weighted sum calculations followed by activation functions in each layer.\n",
    "\n",
    "2. Compute Loss: The loss function (e.g., Mean Squared Error for regression or Cross-Entropy for classification) quantifies how far off the predicted output is from the actual target values.\n",
    "\n",
    "3. Backward Pass (Backpropagation): In the backward pass, the algorithm calculates the gradients of the loss function with respect to the network's weights and biases. This is done by applying the chain rule to compute the gradients layer by layer, starting from the output layer and moving backward through the network.\n",
    "\n",
    "4. Update Weights and Biases: The gradients obtained in the backward pass are used to update the weights and biases of the neural network's layers. Typically, gradient descent or its variants are used to adjust these parameters.\n",
    "\n",
    "5. Repeat: Steps 1 to 4 are repeated for multiple iterations or epochs until the loss converges to a minimum or reaches a stopping criterion.\n",
    "\n",
    "**Code Example (Backpropagation in TensorFlow):**\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define a simple neural network model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_dim=num_features),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with a loss function and optimizer\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model using backpropagation\n",
    "model.fit(X_train, y_train, epochs=num_epochs, batch_size=batch_size)\n",
    "```\n",
    "\n",
    "**Code Example (Backpropagation in PyTorch):**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a custom neural network model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "# Create an instance of the model\n",
    "model = NeuralNetwork(input_dim=num_features, hidden_dim=64, num_classes=num_classes)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop with backpropagation\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "```\n",
    "\n",
    "These code examples demonstrate how to implement backpropagation to train neural networks using TensorFlow and PyTorch. Replace placeholders with your actual data and parameters. If you have more questions or need further details, feel free to ask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron (MLP):\n",
    "\n",
    "**Theory:** Multi-Layer Perceptron, also known as a feedforward neural network or artificial neural network, is a type of neural network architecture that consists of multiple layers of interconnected neurons. It's used for both regression and classification tasks. MLPs are capable of capturing complex patterns in data.\n",
    "\n",
    "**How It Works:**\n",
    "- An MLP typically consists of an input layer, one or more hidden layers, and an output layer.\n",
    "- Each neuron in a layer is connected to every neuron in the subsequent layer, forming a dense, fully connected network.\n",
    "- Neurons in hidden layers apply an activation function to a weighted sum of their inputs, allowing the network to model non-linear relationships.\n",
    "- During training, MLPs use techniques like backpropagation and gradient descent to learn the optimal weights that minimize the loss function.\n",
    "\n",
    "**Support Functions:** MLPs rely on activation functions (e.g., ReLU, Sigmoid) and backpropagation for training. Libraries like TensorFlow and PyTorch provide built-in support for creating and training MLPs.\n",
    "\n",
    "Now, let's see code examples for implementing a Multi-Layer Perceptron using TensorFlow, PyTorch, and scikit-learn:\n",
    "\n",
    "**Using scikit-learn (sklearn):**\n",
    "\n",
    "Scikit-learn provides an easy way to create and train MLPs for classification and regression tasks:\n",
    "\n",
    "```python\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "\n",
    "# Create an MLP Classifier for classification or MLP Regressor for regression\n",
    "model = MLPClassifier(hidden_layer_sizes=(64, 32), activation='relu', max_iter=num_epochs)\n",
    "\n",
    "# Fit the model to your data (X_train and y_train are your feature and target data)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions for classification or regression\n",
    "predictions = model.predict(X_test)\n",
    "```\n",
    "\n",
    "**Using TensorFlow:**\n",
    "\n",
    "In TensorFlow, you can use the Keras API to create and train an MLP:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Define a Sequential model for an MLP\n",
    "model = Sequential([\n",
    "    Dense(64, input_dim=num_features, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model to your training data (X_train and y_train are feature and label data)\n",
    "model.fit(X_train, y_train, epochs=num_epochs, batch_size=batch_size)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "```\n",
    "\n",
    "**Using PyTorch:**\n",
    "\n",
    "In PyTorch, you can define a custom MLP model and train it as follows:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a custom MLP model class\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, num_classes):\n",
    "        super(MLPModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.fc3 = nn.Linear(hidden_dim2, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "# Create an instance of the model\n",
    "model = MLPModel(input_dim=num_features, hidden_dim1=64, hidden_dim2=32, num_classes=num_classes)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop (X_train is your feature data, y_train is class labels)\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test)\n",
    "    \n",
    "```\n",
    "\n",
    "In these code examples, I've shown how to create, train, and use a Multi-Layer Perceptron (MLP) for classification tasks in each of the mentioned libraries. Replace placeholders with your actual data and parameters. If you have more questions or need further details, feel free to ask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout\n",
    "\n",
    "**Theory:** Dropout is a regularization technique used to prevent neural networks from overfitting. It works by randomly dropping (setting to zero) a proportion of the neurons in each layer during training.\n",
    "\n",
    "**How It Works:**\n",
    "- Dropout is a regularization technique used to prevent neural networks from overfitting.\n",
    "- It works by randomly dropping (setting to zero) a proportion of the neurons in each layer during training.\n",
    "- This forces the network to learn redundant representations, which improves its generalization ability.\n",
    "\n",
    "**Support Functions:** Dropout relies on randomly dropping neurons during training. Most machine learning libraries provide built-in support for dropout.\n",
    "\n",
    "**Code Example (Dropout in TensorFlow):** (We will use it in MLP to see the difference)\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Define a Sequential model with Dropout\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_dim=num_features),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model to your training data (X_train and y_train are feature and label data)\n",
    "model.fit(X_train, y_train, epochs=num_epochs, batch_size=batch_size)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "```\n",
    "\n",
    "**Code Example (Dropout in PyTorch):**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a custom MLP model class with Dropout\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, num_classes):\n",
    "        super(MLPModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.fc3 = nn.Linear(hidden_dim2, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "# Create an instance of the model\n",
    "model = MLPModel(input_dim=num_features, hidden_dim1=64, hidden_dim2=32, num_classes=num_classes)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop (X_train is your feature data, y_train is class labels)\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "\n",
    "## Introduction to CNN\n",
    "**From MLP to CNN** \n",
    "- Models are suitable for data containing patterns and characteristics, but they don't assume their relationships. In cases where knowledge is lacking, a multi-layer perceptron is often the best solution. However, these unstructured networks may become too crowded when processing multidimensional cognitive data. For example, a high-quality labeled image set with a 1 million-pixel resolution would require a dense layer of 109 parameters, making learning the parameters impossible.\n",
    "- Despite the argument that the resolution of 1 million pixels may be unnecessary, the number of hidden buttons needed to find good hidden representations of images is overestimated. Learning a binary classifier with a lot of parameters would likely require a huge set of data, equivalent to the number of dogs and cats on Earth. However, both humans and computers can distinguish cats and dogs well due to the richly structured images, often exploited by humans and machine learning models.\n",
    "\n",
    "**Invariant features**\n",
    "- Imagine that we want to identify an object in the picture. It would seem logical to assume that whatever method we use should not be too careful about the exact location of the object in the picture. Ideally, we could learn a system that is capable of leveraging this knowledge in some way.\n",
    "- Back to the picture, the intuitions that we've discussed can be further specified to get some key principles in building neural networks for computer vision:\n",
    "    - In some respects, visual systems should react similarly to the same object regardless of where it appears in the image (progressive immutability).\n",
    "    - On the other hand, the visual systems should focus on the local areas and do not care about anything else that is further away in the image (locally).\n",
    "\n",
    "## Convolutional Neural Network (CNN)\n",
    "**Mathematical cross-correlation**\n",
    "- The convolutional layer is the core building block of a convolutional neural network. The convolutional layer is a linear operation that involves the multiplication of a set of weights with the input, much like a fully connected layer. However, the weights are arranged in a grid-like structure, rather than a single row. The convolutional layer is a linear operation that involves the multiplication of a set of weights with the input, much like a fully connected layer. However, the weights are arranged in a grid-like structure, rather than a single row.\n",
    "    - Example, we have input:\n",
    "    $$ \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} $$\n",
    "    - And we have kernel:\n",
    "    $$ \\begin{bmatrix} 0 & 1 \\\\ 2 & 3 \\end{bmatrix} $$\n",
    "    - We will take 4 values(2x2) from input and dot product with kernel:\n",
    "    $$ \\begin{bmatrix} 1 & 2 \\\\ 4 & 5 \\end{bmatrix} \\times \\begin{bmatrix} 0 & 1 \\\\ 2 & 3 \\end{bmatrix} = 1 * 0 + 2 * 1 + 4 * 2 + 5 * 3 = 25 $$\n",
    "    - The other values will be:\n",
    "    $$ 2 * 0 + 3 * 1 + 5 * 2 + 6 * 3 = 31 $$\n",
    "    $$ 4 * 0 + 5 * 1 + 7 * 2 + 8 * 3 = 37 $$\n",
    "    $$ 5 * 0 + 6 * 1 + 8 * 2 + 9 * 3 = 43 $$\n",
    "    - The result will be:\n",
    "    $$ \\begin{bmatrix} 25 & 31 \\\\ 37 & 43 \\end{bmatrix} $$\n",
    "\n",
    "- I will provide sample code to calculate:\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "kernel = np.array([[0, 1], [2, 3]])\n",
    "\n",
    "# Calculate the cross-correlation\n",
    "def 2Dcross_corr(x, k):\n",
    "    h , w = k.shape\n",
    "    y = np.zeros((x.shape[0] - h + 1, x.shape[1] - w + 1))\n",
    "    for i in range(y.shape[0]):\n",
    "        for j in range(y.shape[1]):\n",
    "            y[i, j] = np.sum(x[i:i+h, j:j+w] * k)\n",
    "    return y\n",
    "\n",
    "# Print the result of cross-correlation\n",
    "print(2Dcross_corr(X, kernel))\n",
    "```\n",
    "\n",
    "**Convolutional layer**\n",
    "    - The accumulator performs cross-correlation mathematics between the input and the nucleus, then adds an adjustment coefficient to get the output. The two parameters of the accumulation layer are the nucleus and the adjustment factor. When we train models that contain accumulated layers, we usually initiate random nuclei, just like we do with a fully connected layer.\n",
    "\n",
    "- Some method to create layer with library:\n",
    "```python\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "\n",
    "# Create a Convolutional layer\n",
    "layer = Conv2D(\n",
    "    filters=32,            # Number of output channels (filters)\n",
    "    kernel_size=3,         # Size of the kernel\n",
    "    activation='relu',     # Activation function\n",
    "    input_shape=(28, 28, 1),  # Input shape (height, width, channels)\n",
    "    strides=(2, 2),        # Stride for the convolution (2, 2) for example\n",
    "    padding='same'         # 'valid' for no padding, 'same' for zero-padding\n",
    ") # Two last feature I will explain below\n",
    "\n",
    "# Apply the Convolutional layer to the input data\n",
    "output = layer(X)\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a Convolutional layer\n",
    "layer = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=0)\n",
    "\n",
    "# Apply the Convolutional layer to the input data\n",
    "output = layer(X)\n",
    "```\n",
    "\n",
    "**Detect edges**\n",
    "- Let's observe a simple application of the accumulation layer: detecting the boundary of an object in an image by determining where the pixels change. First, we create a 'picture' of the size of 6×8 pixels. The four columns in the middle are black (value 0) and the rest are white (values 1).\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Create a 6x8 image with 4 black (0) and 4 white (1) pixels in the middle\n",
    "X = np.ones((6, 8))\n",
    "X[:, 2:6] = 0\n",
    "print(X)\n",
    "```\n",
    "\n",
    "- Then we create a K-core with a height of 1 and a width of 2. When performing cross-correlation with the input, if two elements horizontally adjacent to each other have the same value, the output will be equal to 0 and the other outputs will be different.\n",
    "\n",
    "```python\n",
    "# Create a 1x2 kernel\n",
    "kernel = np.array([[1, -1]])\n",
    "```\n",
    "\n",
    "- We're ready to do cross correlation with the X (input) and K (nuclear) arguments. You can see that the white-to-black boundary positions have a value of 1, whereas the black- to-blue positions are worth -1. The remaining output positions have a value of 0.\n",
    "\n",
    "```python\n",
    "# Apply cross-correlation and print the result\n",
    "print(2Dcross_corr(X, kernel)) # 2Dcross_corr is a function that I have created above\n",
    "```\n",
    "\n",
    "- Now, let's apply this to the transposition of the pixel matrix. As expected, the cross correlation value is zero. The K nucleus can only detect the vertical boundary.\n",
    "\n",
    "```python\n",
    "print(2Dcross_corr(X.T, kernel))\n",
    "```\n",
    "\n",
    "**Padding and strides**\n",
    "- In previous code, I've used padding=0 and strides=1. But what is padding and strides? Let's go into it.\n",
    "    - In the previous example, the input is equal in length and width to 3 (3x3 matrix), the cumulative core window is equally long and wide to 2, so we get an output representation of 2×2. Generally speaking, assuming the size of the input is $nh * nw$, and the dimension of the accumulated kernel window is $kh * kw$, the output size will be:\n",
    "    $$ (n_{h} - k_{h} + 1) \\times (n_{w} - k_{w} + 1) $$\n",
    "    \n",
    "    - Where:\n",
    "        - $n_{h}$ and $n_{w}$ are the height and width of the input.\n",
    "        - $k_{h}$ and $k_{w}$ are the height and width of the kernel.\n",
    "    - In some cases, we're going to combine other techniques that also affect the size of the output, such as adding buffers and accumulating spacing. Note that since the nuclei are generally larger than 1 in width and height, after applying multiple consecutive accumulations, the output is generally significantly smaller than the input. If we start with a picture of 240×240 pixels and apply 10 layers of 5×5 accumulation, then the image size will be reduced to 200×200 pixels, 30% of the image will be cut off, and all useful information on the border of the original picture will be deleted. Padding is the most common tool for dealing with this problem.\n",
    "\n",
    "    **Padding**\n",
    "    - We will demonstrate padding with an example:\n",
    "        - We have input:\n",
    "        $$ \\begin{bmatrix} 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 2 & 0 \\\\ 0 & 3 & 4 & 5 & 0 \\\\ 0 & 6 & 7 & 8 & 0 \\\\ 0 & 0 & 0 & 0 & 0 \\end{bmatrix} $$\n",
    "        - We have kernel:\n",
    "        $$ \\begin{bmatrix} 0 & 1 \\\\ 2 & 3 \\end{bmatrix} $$\n",
    "        - We will take 4 values(2x2) from input and dot product with kernel:\n",
    "        $$ \\begin{bmatrix} 0 & 0 \\\\ 0 & 0 \\end{bmatrix} \\times \\begin{bmatrix} 0 & 1 \\\\ 2 & 3 \\end{bmatrix} = 0 * 0 + 0 * 1 + 0 * 2 + 0 * 3 = 0 $$\n",
    "        - The other values will be:\n",
    "        $$ 0 * 0 + 0 * 1 + 0 * 2 + 1 * 3 = 3 $$\n",
    "        $$ 0 * 0 + 0 * 1 + 1 * 2 + 2 * 3 = 8 $$\n",
    "        $$ 0 * 0 + 0 * 1 + 2 * 2 + 0 * 3 = 4 $$\n",
    "        etc... with the other values\n",
    "        - The result will be:\n",
    "        $$ \\begin{bmatrix} 0 & 3 & 8 & 4 \\\\ 9 & 19 & 25 & 10 \\\\ 21 & 37 & 43 & 16 \\\\ 6 & 7 & 8 & 0 \\end{bmatrix} $$\n",
    "    - Generally speaking, if we insert the total of the $p_h$ (half in the top and half in the bottom) and $p_w$ (left and right), the output size will be:\n",
    "    $$ (n_{h} - k_{h} + p_{h} + 1) \\times (n_{w} - k_{w} + p_{w} + 1) $$\n",
    "\n",
    "    - Where:\n",
    "        - $p_{h}$ and $p_{w}$ are the height and width of the padding.\n",
    "    - We usually choosing padding to be a odd number like 1,3,5 or 7. Choosing an odd will helps us preserve the spatial dimensions by adding the same number of buffer rows for the upper and lower edges, and the same amount of buffering columns for the left and right edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Management\n",
    "\n",
    "The training cycle begins with selecting the network architecture and hyperparameter values, aiming to minimize the target function. Parameters are essential for future predictions, reuse, storage, and scientific analysis. These parameters are saved in a file called a checkpoint, which includes the following information:\n",
    "- Access parameters for debugging, model diagnostics, and visual representation.\n",
    "- Parameter initialization (typically handled automatically).\n",
    "- Model architecture and configuration.\n",
    "\n",
    "**Access the Parameters**\n",
    "After defining a model, you can access its parameters. In PyTorch, model parameters are stored in a dictionary called `state_dict`, which maps each layer to its parameter tensor.\n",
    "\n",
    "```python\n",
    "# PyTorch\n",
    "model.state_dict()\n",
    "\n",
    "# TensorFlow\n",
    "model.get_weights() # This method returns a list and contain all the weights and biases of the model\n",
    "```\n",
    "\n",
    "**Target Parameters**\n",
    "The target parameters are the parameters that are used to train the model. The target parameters are the parameters that are updated during the training process. The target parameters are stored in a dictionary called target_dict. The target_dict is a Python dictionary object that maps each layer to its target parameter tensor. The keys are the names of the layers, and the values are the target parameter tensors. The target_dict object is mutable. The target_dict object is accessed by calling the target_dict() function.\n",
    "\n",
    "```python\n",
    "# PyTorch\n",
    "model.target_dict()\n",
    "```\n",
    "**Initialize Parameters**\n",
    "    - In Deep learning, it's possible to customize the initialization of the parameters. This can be done using various techniques such as zero initialization, random initialization, and Xavier initialization. The choice of initialization can significantly impact the model's performance.\n",
    " \n",
    "```python\n",
    "# PyTorch\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "# TensorFlow\n",
    "initializer = tf.keras.initializers.GlorotUniform()\n",
    "model.add(Dense(64, kernel_initializer=initializer))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
