{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import thư viện sẽ dùng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import nltk\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "import pretty_errors\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import (\n",
    "    GemmaForCausalLM, # Model loader\n",
    "    GemmaTokenizer, # tokenizer loader\n",
    "    GemmaConfig, # Model config loader\n",
    "    BitsAndBytesConfig, # BitsAndBytes model loader\n",
    "    TrainingArguments, # Tham số huấn luyện\n",
    "    logging, # Kiểm soát log\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model # Parameter Efficient Fine-Tuning\n",
    "from trl import SFTTrainer # Hàm hỗ trợ quá trình huấn luyện (Supervised Fine-Tuning)\n",
    "\n",
    "logging.get_logger().setLevel(logging.ERROR)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Apr  6 10:13:27 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 551.76                 Driver Version: 551.76         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090      WDDM  |   00000000:05:00.0  On |                  N/A |\n",
      "| 99%   27C    P5             57W /  390W |     153MiB /  24576MiB |     30%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A     14548    C+G   ..._8wekyb3d8bbwe\\PAD.Console.Host.exe      N/A      |"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Load dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer'],\n",
      "    num_rows: 200035\n",
      "})\n",
      "{'question': 'Jungkook is the 5th place. Find the number of people who crossed the finish line faster than Jungkook.', 'answer': 'If Jungkook is in 5th place, then 4 people crossed the finish line faster than him.'}\n",
      "{'question': 'A number divided by 10 is 6. Yoongi got the result by subtracting 15 from a certain number. What is the result he got?', 'answer': 'Let\\'s call the certain number \"x\". According to the information given:\\n\\nA number divided by 10 is 6:\\nx / 10 = 6\\n\\nYoongi got the result by subtracting 15 from x:\\nResult = x - 15\\n\\nFirst, we need to find the value of x. We can do this by solving the first equation:\\n\\nx / 10 = 6\\nx = 6 * 10\\nx = 60\\n\\nNow that we know x is 60, we can find the result Yoongi got by subtracting 15 from x:\\n\\nResult = x - 15\\nResult = 60 - 15\\nResult = 45\\n\\nSo, the result Yoongi got is 45.'}\n",
      "{'question': 'Dongju selects a piece of paper with a number written on it, and wants to make a three-digit number by placing the first selected number in the hundreds place, the second selected in the tens place, and the third selected in the units place. If the numbers written on each paper was 1, 6, and 8, respectively, find the sum of the second smallest and third smallest three-digit numbers that Dongju can make. However, you cannot select the same numbered paper multiple times.', 'answer': \"To find the second smallest and third smallest three-digit numbers that Dongju can make with the digits 1, 6, and 8, we need to consider all the possible combinations of these three digits without repetition.\\n\\nThe possible three-digit numbers are:\\n- 168\\n- 186\\n- 618\\n- 681\\n- 816\\n- 861\\n\\nNow, let's arrange these numbers in ascending order to find the second smallest and third smallest numbers:\\n- 168 (smallest)\\n- 186 (second smallest)\\n- 618 (third smallest)\\n- 681\\n- 816\\n- 861 (largest)\\n\\nThe second smallest number is 186, and the third smallest number is 618.\\n\\nNow, we find the sum of the second smallest and third smallest numbers:\\n186 + 618 = 804\\n\\nTherefore, the sum of the second smallest and third smallest three-digit numbers that Dongju can make with the digits 1, 6, and 8 is 804.\"}\n",
      "{'question': 'You wanted to subtract 46 from a number, but you accidentally subtract 59 and get 43. How much do you get from the correct calculation?', 'answer': 'If you accidentally subtracted 59 instead of 46 and got 43, you can find the original number by adding 59 back to 43:\\n\\n43 + 59 = 102\\n\\nNow, to find the result of the correct calculation, subtract 46 from the original number:\\n\\n102 - 46 = 56\\n\\nSo, if you subtract 46 from the original number, you would get 56.'}\n",
      "{'question': 'The length of one span of Jinseo is about 12 centimeters (cm). When Jinseo measured the length of the shorter side of the bookshelf, it was about two spans. How many centimeters (cm) is the short side of the bookshelf?', 'answer': 'If one span of Jinseo is about 12 centimeters and the shorter side of the bookshelf is about two spans, then the length of the shorter side of the bookshelf would be:\\n\\n2 spans * 12 cm/span = 24 cm\\n\\nSo, the short side of the bookshelf is about 24 centimeters.'}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"microsoft/orca-math-word-problems-200k\", split = \"train\")\n",
    "print(dataset)\n",
    "\n",
    "for i in range(5):\n",
    "    print(dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': Value(dtype='string', id=None),\n",
      " 'question': Value(dtype='string', id=None)}\n"
     ]
    }
   ],
   "source": [
    "pprint(dataset.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['question', 'answer']\n"
     ]
    }
   ],
   "source": [
    "pprint(dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'If Jungkook is in 5th place, then 4 people crossed the finish line '\n",
      "           'faster than him.',\n",
      " 'question': 'Jungkook is the 5th place. Find the number of people who crossed '\n",
      "             'the finish line faster than Jungkook.'}\n"
     ]
    }
   ],
   "source": [
    "pprint(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_function(examples):\n",
    "    instruction_template =  \"\"\"\n",
    "                                **Instruction:**\n",
    "                                Please answer the following math problem.\n",
    "\n",
    "                                **Input:**\n",
    "                                {input}\n",
    "\n",
    "                                **Response:**\n",
    "                                {response}\n",
    "                            \"\"\"\n",
    "\n",
    "    # instruction = examples['instruction']\n",
    "    input_text = examples['question']\n",
    "    response_text = examples['answer']\n",
    "\n",
    "    formatted_data = instruction_template.format(\n",
    "        # instruction = instruction,\n",
    "        input = input_text,\n",
    "        response = response_text\n",
    "    )\n",
    "\n",
    "    return {\"prompt\": formatted_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_dataset = dataset.map(formatting_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_dataset = formatted_dataset.train_test_split(test_size = 0.1, seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Jinho spent half of his money and 300 won at the first store, then half of his money and 400 won at the second store, and he had no money left. Find how much money Jinho had at the beginning.',\n",
       " 'answer': \"Let's denote the amount of money Jinho had at the beginning as \\\\( M \\\\).\\n\\nAt the first store, he spent half of his money and an additional 300 won. So the amount he spent at the first store is \\\\( \\\\frac{M}{2} + 300 \\\\).\\n\\nAfter spending at the first store, he is left with \\\\( M - (\\\\frac{M}{2} + 300) = \\\\frac{M}{2} - 300 \\\\).\\n\\nAt the second store, he spent half of the remaining money and an additional 400 won. So the amount he spent at the second store is \\\\( \\\\frac{(\\\\frac{M}{2} - 300)}{2} + 400 \\\\).\\n\\nAfter spending at the second store, he has no money left, so the amount he spent at the second store is equal to the amount he had after the first store, which is \\\\( \\\\frac{M}{2} - 300 \\\\).\\n\\nTherefore, we have the equation:\\n\\n\\\\[ \\\\frac{(\\\\frac{M}{2} - 300)}{2} + 400 = \\\\frac{M}{2} - 300 \\\\]\\n\\nLet's solve this equation step by step:\\n\\n\\\\[ \\\\frac{M}{4} - 150 + 400 = \\\\frac{M}{2} - 300 \\\\]\\n\\nCombine the constants on the left side:\\n\\n\\\\[ \\\\frac{M}{4} + 250 = \\\\frac{M}{2} - 300 \\\\]\\n\\nMultiply every term by 4 to get rid of the fraction:\\n\\n\\\\[ M + 1000 = 2M - 1200 \\\\]\\n\\nNow, let's move all terms involving \\\\( M \\\\) to one side and constants to the other side:\\n\\n\\\\[ 1000 + 1200 = 2M - M \\\\]\\n\\n\\\\[ 2200 = M \\\\]\\n\\nSo, Jinho had 2200 won at the beginning.\",\n",
       " 'prompt': \"\\n                                **Instruction:**\\n                                Please answer the following math problem.\\n\\n                                **Input:**\\n                                Jinho spent half of his money and 300 won at the first store, then half of his money and 400 won at the second store, and he had no money left. Find how much money Jinho had at the beginning.\\n\\n                                **Response:**\\n                                Let's denote the amount of money Jinho had at the beginning as \\\\( M \\\\).\\n\\nAt the first store, he spent half of his money and an additional 300 won. So the amount he spent at the first store is \\\\( \\\\frac{M}{2} + 300 \\\\).\\n\\nAfter spending at the first store, he is left with \\\\( M - (\\\\frac{M}{2} + 300) = \\\\frac{M}{2} - 300 \\\\).\\n\\nAt the second store, he spent half of the remaining money and an additional 400 won. So the amount he spent at the second store is \\\\( \\\\frac{(\\\\frac{M}{2} - 300)}{2} + 400 \\\\).\\n\\nAfter spending at the second store, he has no money left, so the amount he spent at the second store is equal to the amount he had after the first store, which is \\\\( \\\\frac{M}{2} - 300 \\\\).\\n\\nTherefore, we have the equation:\\n\\n\\\\[ \\\\frac{(\\\\frac{M}{2} - 300)}{2} + 400 = \\\\frac{M}{2} - 300 \\\\]\\n\\nLet's solve this equation step by step:\\n\\n\\\\[ \\\\frac{M}{4} - 150 + 400 = \\\\frac{M}{2} - 300 \\\\]\\n\\nCombine the constants on the left side:\\n\\n\\\\[ \\\\frac{M}{4} + 250 = \\\\frac{M}{2} - 300 \\\\]\\n\\nMultiply every term by 4 to get rid of the fraction:\\n\\n\\\\[ M + 1000 = 2M - 1200 \\\\]\\n\\nNow, let's move all terms involving \\\\( M \\\\) to one side and constants to the other side:\\n\\n\\\\[ 1000 + 1200 = 2M - M \\\\]\\n\\n\\\\[ 2200 = M \\\\]\\n\\nSo, Jinho had 2200 won at the beginning.\\n                            \"}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Khởi tạo thông số fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0300543c9914b3e875990b4e2ea8a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"google/gemma-2b\" # Mô hình\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r = 32, # Số lượng bit cho Quantization\n",
    "    lora_alpha = 12, # Số lượng bit cho Activation\n",
    "    lora_dropout = 0.05, # Dropout\n",
    "    fan_in_fan_out = True,\n",
    "    bias = \"none\",\n",
    "    task_type = \"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = GemmaForCausalLM.from_pretrained(model_id, quantization_config = bnb_config, device_map = \"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GemmaTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens = {\n",
    "    \"pad_token\": \"<pad>\",\n",
    "    \"eos_token\": \"<eos>\",\n",
    "    \"bos_token\": \"<bos>\",\n",
    "    \"unk_token\": \"<unk>\",\n",
    "    \"sep_token\": \"<sep>\",\n",
    "    \"cls_token\": \"<cls>\"\n",
    "}\n",
    "\n",
    "tokenizer.add_tokens(list(special_tokens.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3686400 || all params: 1518954496 || trainable%: 0.24269324786935553\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(get_peft_model(base_model, lora_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GemmaForCausalLM(\n",
      "  (model): GemmaModel(\n",
      "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-17): 18 x GemmaDecoderLayer(\n",
      "        (self_attn): GemmaSdpaAttention(\n",
      "          (q_proj): lora.Linear4bit(\n",
      "            (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Dropout(p=0.05, inplace=False)\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=2048, out_features=32, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=32, out_features=2048, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
      "          (v_proj): lora.Linear4bit(\n",
      "            (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Dropout(p=0.05, inplace=False)\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=2048, out_features=32, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=32, out_features=256, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): GemmaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): GemmaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
      "          (act_fn): GELUActivation()\n",
      "        )\n",
      "        (input_layernorm): GemmaRMSNorm()\n",
      "        (post_attention_layernorm): GemmaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): GemmaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "pprint(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61e9801311e548dbba3309fb04172725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20004 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    base_model,\n",
    "    train_dataset = formatted_dataset['train'],\n",
    "    eval_dataset = formatted_dataset['test'],\n",
    "    args = TrainingArguments(\n",
    "        num_train_epochs = 3,\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 20,\n",
    "        max_grad_norm = 0.3,\n",
    "        learning_rate = 2e-4,\n",
    "        warmup_steps = 25,\n",
    "        save_steps = 25,\n",
    "        eval_steps = 25,\n",
    "        max_steps = -1,\n",
    "        warmup_ratio = 0.03,\n",
    "        weight_decay = 0.001,\n",
    "        fp16 = True,\n",
    "        output_dir = \"./Gemma-2B-finetuned-GVI\",\n",
    "        optim = \"paged_adamw_8bit\",\n",
    "        logging_steps = 25,\n",
    "    ),\n",
    "    peft_config = lora_config,\n",
    "    dataset_text_field = 'prompt',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1403, 'grad_norm': 0.1390424370765686, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 0.8804, 'grad_norm': 0.12615631520748138, 'learning_rate': 0.00019981466380013344, 'epoch': 0.01}\n",
      "{'loss': 0.6878, 'grad_norm': 0.09305422753095627, 'learning_rate': 0.0001996293276002669, 'epoch': 0.01}\n",
      "{'loss': 0.6532, 'grad_norm': 0.08976873010396957, 'learning_rate': 0.00019944399140040033, 'epoch': 0.01}\n",
      "{'loss': 0.636, 'grad_norm': 0.12817758321762085, 'learning_rate': 0.00019925865520053378, 'epoch': 0.01}\n",
      "{'loss': 0.6295, 'grad_norm': 0.09994305670261383, 'learning_rate': 0.00019907331900066721, 'epoch': 0.02}\n",
      "{'loss': 0.6243, 'grad_norm': 0.0967579260468483, 'learning_rate': 0.00019888798280080064, 'epoch': 0.02}\n",
      "{'loss': 0.6141, 'grad_norm': 0.1250607967376709, 'learning_rate': 0.0001987026466009341, 'epoch': 0.02}\n",
      "{'loss': 0.6095, 'grad_norm': 0.11771272867918015, 'learning_rate': 0.00019851731040106753, 'epoch': 0.02}\n",
      "{'loss': 0.6263, 'grad_norm': 0.11927448213100433, 'learning_rate': 0.000198331974201201, 'epoch': 0.03}\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[WinError 5] Access is denied: './Gemma-2B-finetuned-GVI\\\\tmp-checkpoint-250' -> './Gemma-2B-finetuned-GVI\\\\checkpoint-250'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m<timed eval>:1\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\ai\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:331\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m--> 331\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    333\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\ai\\lib\\site-packages\\transformers\\trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\ai\\lib\\site-packages\\transformers\\trainer.py:2029\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2026\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[0;32m   2027\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 2029\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2030\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2031\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\ai\\lib\\site-packages\\transformers\\trainer.py:2423\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2420\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mstep(metrics[metric_to_check])\n\u001b[0;32m   2422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[1;32m-> 2423\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2424\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\ai\\lib\\site-packages\\transformers\\trainer.py:2538\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[1;34m(self, model, trial, metrics)\u001b[0m\n\u001b[0;32m   2536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m staging_output_dir \u001b[38;5;241m!=\u001b[39m output_dir:\n\u001b[0;32m   2537\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(staging_output_dir):\n\u001b[1;32m-> 2538\u001b[0m         \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstaging_output_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2540\u001b[0m         \u001b[38;5;66;03m# Ensure rename completed in cases where os.rename is not atomic\u001b[39;00m\n\u001b[0;32m   2541\u001b[0m         \u001b[38;5;66;03m# And can only happen on non-windows based systems\u001b[39;00m\n\u001b[0;32m   2542\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 5] Access is denied: './Gemma-2B-finetuned-GVI\\\\tmp-checkpoint-250' -> './Gemma-2B-finetuned-GVI\\\\checkpoint-250'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tqdm(trainer.train())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Commit lên HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "\n",
    "# notebook_login() # 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model.push_to_hub(\"KasaiDanto/GVI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.push_to_hub(\"KasaiDanto/GVI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.push_to_hub(\"KasaiDanto/GVI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lora_config = LoraConfig.from_pretrained(\"KasaiDanto/GVI\")\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit = True,\n",
    "#     bnb_4bit_use_double_quant = True,\n",
    "#     bnb_4bit_quant_type = \"nf4\",\n",
    "#     bnb_4bit_compute_dtype = torch.bfloat16\n",
    "# )\n",
    "\n",
    "# tokenizer = GemmaTokenizer.from_pretrained(\"KasaiDanto/GVI\")\n",
    "# model = GemmaForCausalLM.from_pretrained(\n",
    "#     lora_config.base_model_name_or_path,\n",
    "#     quantization_config = bnb_config,\n",
    "#     device_map= {\"\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import display, Markdown\n",
    "\n",
    "# def make_inference(instruction, context = None):\n",
    "#     if context:\n",
    "#         prompt = f\"Below is an instruction that describes a task, paired with an input that provides further context.\\n\\n### Instruction: \\n{instruction}\\n\\n### Input: \\n{context}\\n\\n### Response: \\n\"\n",
    "#     else:\n",
    "#         prompt = f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction: \\n{instruction}\\n\\n### Response: \\n\"\n",
    "#     inputs = tokenizer(prompt, return_tensors = \"pt\", return_token_type_ids = False).to(\"cuda:0\")\n",
    "#     outputs = model.generate(**inputs, max_new_tokens = 50)\n",
    "#     display(Markdown((tokenizer.decode(outputs[0], skip_special_tokens = True))))\n",
    "#     outputs = base_model.generate(**inputs, max_new_tokens = 50)\n",
    "#     print(\"---- NON-INSTRUCT-TUNED-MODEL ----\")\n",
    "#     display(Markdown((tokenizer.decode(outputs[0], skip_special_tokens = True))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_inference('Tiền đề: Một người đàn ông đang trượt patin trước một chiếc ghế gỗ. Hãy dịch câu này sang tiếng Anh.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
