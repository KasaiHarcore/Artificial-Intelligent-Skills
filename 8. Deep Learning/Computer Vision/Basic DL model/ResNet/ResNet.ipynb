{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Resnet Model"
      ],
      "metadata": {
        "id": "2bwd9HzqqOsF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zNCf1_F1hxEZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as  nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "    def __init__(self, in_channels, out_channels, i_downsample=None, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
        "        self.batch_norm1 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.batch_norm2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(out_channels, out_channels*self.expansion, kernel_size=1, stride=1, padding=0)\n",
        "        self.batch_norm3 = nn.BatchNorm2d(out_channels*self.expansion)\n",
        "\n",
        "        self.i_downsample = i_downsample\n",
        "        self.stride = stride\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x.clone()\n",
        "        x = self.relu(self.batch_norm1(self.conv1(x)))\n",
        "\n",
        "        x = self.relu(self.batch_norm2(self.conv2(x)))\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.batch_norm3(x)\n",
        "\n",
        "        #downsample if needed\n",
        "        if self.i_downsample is not None:\n",
        "            identity = self.i_downsample(identity)\n",
        "        #add identity\n",
        "        x+=identity\n",
        "        x=self.relu(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "kT7RvSjBsEo6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    expansion = 1\n",
        "    def __init__(self, in_channels, out_channels, i_downsample=None, stride=1):\n",
        "        super(Block, self).__init__()\n",
        "\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride, bias=False)\n",
        "        self.batch_norm1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=stride, bias=False)\n",
        "        self.batch_norm2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.i_downsample = i_downsample\n",
        "        self.stride = stride\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "      identity = x.clone()\n",
        "\n",
        "      x = self.relu(self.batch_norm2(self.conv1(x)))\n",
        "      x = self.batch_norm2(self.conv2(x))\n",
        "\n",
        "      if self.i_downsample is not None:\n",
        "          identity = self.i_downsample(identity)\n",
        "      print(x.shape)\n",
        "      print(identity.shape)\n",
        "      x += identity\n",
        "      x = self.relu(x)\n",
        "      return x"
      ],
      "metadata": {
        "id": "8jWEz-QbsMtm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, ResBlock, layer_list, num_classes, num_channels=3):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(num_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.batch_norm1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size = 3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self._make_layer(ResBlock, layer_list[0], planes=64)\n",
        "        self.layer2 = self._make_layer(ResBlock, layer_list[1], planes=128, stride=2)\n",
        "        self.layer3 = self._make_layer(ResBlock, layer_list[2], planes=256, stride=2)\n",
        "        self.layer4 = self._make_layer(ResBlock, layer_list[3], planes=512, stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.fc = nn.Linear(512*ResBlock.expansion, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.batch_norm1(self.conv1(x)))\n",
        "        x = self.max_pool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _make_layer(self, ResBlock, blocks, planes, stride=1):\n",
        "        ii_downsample = None\n",
        "        layers = []\n",
        "\n",
        "        if stride != 1 or self.in_channels != planes*ResBlock.expansion:\n",
        "            ii_downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.in_channels, planes*ResBlock.expansion, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm2d(planes*ResBlock.expansion)\n",
        "            )\n",
        "\n",
        "        layers.append(ResBlock(self.in_channels, planes, i_downsample=ii_downsample, stride=stride))\n",
        "        self.in_channels = planes*ResBlock.expansion\n",
        "\n",
        "        for i in range(blocks-1):\n",
        "            layers.append(ResBlock(self.in_channels, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "\n",
        "def ResNet50(num_classes, channels=3):\n",
        "    return ResNet(Bottleneck, [3,4,6,3], num_classes, channels)\n",
        "\n",
        "def ResNet101(num_classes, channels=3):\n",
        "    return ResNet(Bottleneck, [3,4,23,3], num_classes, channels)\n",
        "\n",
        "def ResNet152(num_classes, channels=3):\n",
        "    return ResNet(Bottleneck, [3,8,36,3], num_classes, channels)"
      ],
      "metadata": {
        "id": "SSNrG87osNdF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "INSVFw-ut8gH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "nMMWKn68t_X-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "id": "4FLtos0FuCoO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "train = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "test = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(test, batch_size=128,shuffle=False, num_workers=2)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZv35GxBuEcv",
        "outputId": "aa9fabd2-8c8b-482f-8250-7804a28f49e8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:14<00:00, 12065534.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n"
      ],
      "metadata": {
        "id": "JrLeg1CDuGcw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = ResNet50(10).to('cuda')\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0001)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.1, patience=5)\n"
      ],
      "metadata": {
        "id": "roU6FEsLuKii"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20\n",
        "for epoch in range(EPOCHS):\n",
        "    losses = []\n",
        "    running_loss = 0\n",
        "    for i, inp in enumerate(trainloader):\n",
        "        inputs, labels = inp\n",
        "        inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i%100 == 0 and i > 0:\n",
        "            print(f'Loss [{epoch+1}, {i}](epoch, minibatch): ', running_loss / 100)\n",
        "            running_loss = 0.0\n",
        "\n",
        "    avg_loss = sum(losses)/len(losses)\n",
        "    scheduler.step(avg_loss)\n",
        "\n",
        "print('Training Done')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nekVN4DouMUS",
        "outputId": "6a8cb8c8-149d-405c-c459-eb931b792860"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss [1, 100](epoch, minibatch):  0.6288834398984909\n",
            "Loss [1, 200](epoch, minibatch):  0.604371713399887\n",
            "Loss [1, 300](epoch, minibatch):  0.5747403889894486\n",
            "Loss [2, 100](epoch, minibatch):  0.5272147789597511\n",
            "Loss [2, 200](epoch, minibatch):  0.5307522135972976\n",
            "Loss [2, 300](epoch, minibatch):  0.5190009015798569\n",
            "Loss [3, 100](epoch, minibatch):  0.5284273612499237\n",
            "Loss [3, 200](epoch, minibatch):  0.5139440298080444\n",
            "Loss [3, 300](epoch, minibatch):  0.5360714790225029\n",
            "Loss [4, 100](epoch, minibatch):  0.5055986496806145\n",
            "Loss [4, 200](epoch, minibatch):  0.511971250474453\n",
            "Loss [4, 300](epoch, minibatch):  0.5196000516414643\n",
            "Loss [5, 100](epoch, minibatch):  0.5153242221474648\n",
            "Loss [5, 200](epoch, minibatch):  0.5110887721180916\n",
            "Loss [5, 300](epoch, minibatch):  0.518493392765522\n",
            "Loss [6, 100](epoch, minibatch):  0.48878391534090043\n",
            "Loss [6, 200](epoch, minibatch):  0.5119347250461579\n",
            "Loss [6, 300](epoch, minibatch):  0.4965599378943443\n",
            "Loss [7, 100](epoch, minibatch):  0.5017348453402519\n",
            "Loss [7, 200](epoch, minibatch):  0.4909125444293022\n",
            "Loss [7, 300](epoch, minibatch):  0.5017595002055169\n",
            "Loss [8, 100](epoch, minibatch):  0.501226709485054\n",
            "Loss [8, 200](epoch, minibatch):  0.4962716430425644\n",
            "Loss [8, 300](epoch, minibatch):  0.5129116049408913\n",
            "Loss [9, 100](epoch, minibatch):  0.4898590150475502\n",
            "Loss [9, 200](epoch, minibatch):  0.5020825892686844\n",
            "Loss [9, 300](epoch, minibatch):  0.4976712214946747\n",
            "Loss [10, 100](epoch, minibatch):  0.48070173025131224\n",
            "Loss [10, 200](epoch, minibatch):  0.4943797016143799\n",
            "Loss [10, 300](epoch, minibatch):  0.49383217304944993\n",
            "Loss [11, 100](epoch, minibatch):  0.46848084330558776\n",
            "Loss [11, 200](epoch, minibatch):  0.4824127271771431\n",
            "Loss [11, 300](epoch, minibatch):  0.49356164008378983\n",
            "Loss [12, 100](epoch, minibatch):  0.4704739251732826\n",
            "Loss [12, 200](epoch, minibatch):  0.4777094760537148\n",
            "Loss [12, 300](epoch, minibatch):  0.49187258541584017\n",
            "Loss [13, 100](epoch, minibatch):  0.47481598794460295\n",
            "Loss [13, 200](epoch, minibatch):  0.46710976362228396\n",
            "Loss [13, 300](epoch, minibatch):  0.4703965884447098\n",
            "Loss [14, 100](epoch, minibatch):  0.4511883309483528\n",
            "Loss [14, 200](epoch, minibatch):  0.47741581246256826\n",
            "Loss [14, 300](epoch, minibatch):  0.4802881395816803\n",
            "Loss [15, 100](epoch, minibatch):  0.465725964307785\n",
            "Loss [15, 200](epoch, minibatch):  0.46723206490278246\n",
            "Loss [15, 300](epoch, minibatch):  0.46985376596450806\n",
            "Loss [16, 100](epoch, minibatch):  0.47343057811260225\n",
            "Loss [16, 200](epoch, minibatch):  0.46763730436563494\n",
            "Loss [16, 300](epoch, minibatch):  0.47527809590101244\n",
            "Loss [17, 100](epoch, minibatch):  0.4490664631128311\n",
            "Loss [17, 200](epoch, minibatch):  0.46070654183626175\n",
            "Loss [17, 300](epoch, minibatch):  0.4775165343284607\n",
            "Loss [18, 100](epoch, minibatch):  0.44744674891233444\n",
            "Loss [18, 200](epoch, minibatch):  0.46780346244573595\n",
            "Loss [18, 300](epoch, minibatch):  0.46314564883708953\n",
            "Loss [19, 100](epoch, minibatch):  0.4439138498902321\n",
            "Loss [19, 200](epoch, minibatch):  0.4420231032371521\n",
            "Loss [19, 300](epoch, minibatch):  0.44061892688274384\n",
            "Loss [20, 100](epoch, minibatch):  0.4528844141960144\n",
            "Loss [20, 200](epoch, minibatch):  0.44956313639879225\n",
            "Loss [20, 300](epoch, minibatch):  0.46511827543377876\n",
            "Training Done\n"
          ]
        }
      ]
    }
  ]
}