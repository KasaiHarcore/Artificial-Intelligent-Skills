{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\nguye\\anaconda3\\envs\\ai\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\nguye\\anaconda3\\envs\\ai\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\nguye\\anaconda3\\envs\\ai\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\nguye\\anaconda3\\envs\\ai\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\nguye\\anaconda3\\envs\\ai\\Lib\\asyncio\\base_events.py\", line 640, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\nguye\\anaconda3\\envs\\ai\\Lib\\asyncio\\base_events.py\", line 1992, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\nguye\\anaconda3\\envs\\ai\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\nguye\\anaconda3\\envs\\ai\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\nguye\\anaconda3\\envs\\ai\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\nguye\\anaconda3\\envs\\ai\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\nguye\\anaconda3\\envs\\ai\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\nguye\\anaconda3\\envs\\ai\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\nguye\\anaconda3\\envs\\ai\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\nguye\\anaconda3\\envs\\ai\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\nguye\\anaconda3\\envs\\ai\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3077, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\nguye\\anaconda3\\envs\\ai\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3132, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\nguye\\anaconda3\\envs\\ai\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\nguye\\anaconda3\\envs\\ai\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3336, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\nguye\\anaconda3\\envs\\ai\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\nguye\\anaconda3\\envs\\ai\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\nguye\\AppData\\Local\\Temp\\ipykernel_20392\\3489367781.py\", line 10, in <module>\n",
      "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "C:\\Users\\nguye\\AppData\\Local\\Temp\\ipykernel_20392\\3489367781.py:10: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update 0, Average Reward: 500.00\n",
      "Update 10, Average Reward: 500.00\n",
      "Update 20, Average Reward: 500.00\n",
      "Update 30, Average Reward: 500.00\n",
      "Update 40, Average Reward: 500.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nguye\\anaconda3\\envs\\ai\\Lib\\site-packages\\gymnasium\\envs\\classic_control\\cartpole.py:250: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward: 500.0\n",
      "Episode 2: Total Reward: 500.0\n",
      "Episode 3: Total Reward: 500.0\n",
      "Episode 4: Total Reward: 500.0\n",
      "Episode 5: Total Reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import time\n",
    "\n",
    "# Thiết lập thiết bị sử dụng GPU nếu có, ngược lại dùng CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- 1. Định nghĩa mạng neural cho Actor-Critic ---\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        # Mạng dùng chung\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        # Actor: Xuất xác suất cho từng hành động (dành cho không gian hành động rời rạc)\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        # Critic: Ước lượng giá trị trạng thái\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = self.shared(state)\n",
    "        action_probs = self.policy(x)\n",
    "        state_value = self.value(x)\n",
    "        return action_probs, state_value\n",
    "\n",
    "# --- 2. Hyperparameters ---\n",
    "env_name = \"CartPole-v1\"\n",
    "env = gym.make(env_name)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "hidden_dim = 64\n",
    "lr = 3e-4\n",
    "gamma = 0.99            # Hệ số chiết khấu\n",
    "lmbda = 0.95            # Hệ số lambda trong GAE\n",
    "clip_epsilon = 0.2      # Hệ số clipping của PPO\n",
    "ppo_epochs = 10         # Số epoch cập nhật mỗi vòng\n",
    "minibatch_size = 64\n",
    "max_timesteps_per_update = 2048\n",
    "total_updates = 50\n",
    "\n",
    "# Hệ số loss\n",
    "vf_coef = 0.5           # Hệ số cho Value Function loss\n",
    "entropy_coef = 0.01     # Hệ số cho Entropy bonus\n",
    "\n",
    "# --- 3. Khởi tạo mạng và optimizer ---\n",
    "model = ActorCritic(state_dim, action_dim, hidden_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# --- 4. Hàm tính Generalized Advantage Estimation (GAE) ---\n",
    "def compute_gae(rewards, masks, values, gamma, lmbda):\n",
    "    # Chuyển list values thành tensor\n",
    "    values = torch.tensor(values, dtype=torch.float32, device=device)\n",
    "    gae = 0\n",
    "    advantages = []\n",
    "    # Duyệt ngược từ cuối về đầu (len(rewards) = T)\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "        gae = delta + gamma * lmbda * masks[step] * gae\n",
    "        advantages.insert(0, gae)\n",
    "    advantages = torch.tensor(advantages, dtype=torch.float32, device=device)\n",
    "    # Value target = advantage + value (lấy từ values từ 0 đến T-1)\n",
    "    returns = advantages + values[:-1]\n",
    "    return advantages, returns\n",
    "\n",
    "# --- 5. Hàm thu thập dữ liệu từ môi trường ---\n",
    "def collect_trajectories(env, model, max_timesteps):\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    masks = []       # 1 nếu episode chưa kết thúc, 0 nếu kết thúc\n",
    "    log_probs = []\n",
    "    values = []\n",
    "    \n",
    "    # Với gymnasium (gym >=0.26), reset trả về (observation, info)\n",
    "    state, _ = env.reset()\n",
    "    for _ in range(max_timesteps):\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        probs, value = model(state_tensor)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        \n",
    "        # Lưu dữ liệu\n",
    "        states.append(state)\n",
    "        actions.append(action.item())\n",
    "        values.append(value.item())\n",
    "        log_probs.append(log_prob.item())\n",
    "        \n",
    "        # Thực hiện hành động: gymnasium step trả về (observation, reward, done, truncated, info)\n",
    "        next_state, reward, done, truncated, _ = env.step(action.item())\n",
    "        rewards.append(reward)\n",
    "        masks.append(0 if (done or truncated) else 1)\n",
    "        \n",
    "        state = next_state\n",
    "        if done or truncated:\n",
    "            state, _ = env.reset()\n",
    "    # Thêm giá trị của trạng thái cuối để tính advantage (sử dụng giá trị của state cuối cùng)\n",
    "    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "    _, next_value = model(state_tensor)\n",
    "    values.append(next_value.item())\n",
    "    \n",
    "    return states, actions, rewards, masks, log_probs, values\n",
    "\n",
    "# --- 6. Hàm cập nhật PPO ---\n",
    "def ppo_update(model, optimizer, states, actions, log_probs_old, returns, advantages, clip_epsilon, ppo_epochs, minibatch_size):\n",
    "    # Chuyển dữ liệu thành tensor nếu cần\n",
    "    if not isinstance(states, torch.Tensor):\n",
    "        states = torch.FloatTensor(states).to(device)\n",
    "    if not isinstance(actions, torch.Tensor):\n",
    "        actions = torch.LongTensor(actions).to(device)\n",
    "    if not isinstance(log_probs_old, torch.Tensor):\n",
    "        log_probs_old = torch.FloatTensor(log_probs_old).to(device)\n",
    "    if not isinstance(returns, torch.Tensor):\n",
    "        returns = torch.tensor(returns, dtype=torch.float32).to(device)\n",
    "    else:\n",
    "        returns = returns.to(device)\n",
    "    if not isinstance(advantages, torch.Tensor):\n",
    "        advantages = torch.tensor(advantages, dtype=torch.float32).to(device)\n",
    "    else:\n",
    "        advantages = advantages.to(device)\n",
    "    \n",
    "    dataset_size = states.size(0)\n",
    "    for _ in range(ppo_epochs):\n",
    "        indices = np.arange(dataset_size)\n",
    "        np.random.shuffle(indices)\n",
    "        for i in range(0, dataset_size, minibatch_size):\n",
    "            # Chuyển chỉ số sang tensor để index tensor\n",
    "            batch_idx = torch.tensor(indices[i: i + minibatch_size], dtype=torch.long, device=device)\n",
    "            batch_states = states[batch_idx]\n",
    "            batch_actions = actions[batch_idx]\n",
    "            batch_log_probs_old = log_probs_old[batch_idx]\n",
    "            batch_returns = returns[batch_idx]\n",
    "            batch_advantages = advantages[batch_idx]\n",
    "            \n",
    "            # Tính toán xác suất và giá trị mới từ model\n",
    "            probs, values = model(batch_states)\n",
    "            dist = Categorical(probs)\n",
    "            log_probs = dist.log_prob(batch_actions)\n",
    "            entropy = dist.entropy().mean()\n",
    "            \n",
    "            # Tỷ lệ cập nhật\n",
    "            ratio = torch.exp(log_probs - batch_log_probs_old)\n",
    "            \n",
    "            # Tính surrogate objective với clipping\n",
    "            surr1 = ratio * batch_advantages\n",
    "            surr2 = torch.clamp(ratio, 1.0 - clip_epsilon, 1.0 + clip_epsilon) * batch_advantages\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            \n",
    "            # Value function loss (Mean Squared Error)\n",
    "            value_loss = (batch_returns - values.squeeze()).pow(2).mean()\n",
    "            \n",
    "            # Tổng loss, bao gồm entropy bonus để khuyến khích khám phá\n",
    "            loss = policy_loss + vf_coef * value_loss - entropy_coef * entropy\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "# --- 7. Vòng lặp huấn luyện chính của PPO ---\n",
    "all_episode_rewards = []\n",
    "for update in range(total_updates):\n",
    "    states, actions, rewards, masks, log_probs_old, values = collect_trajectories(env, model, max_timesteps_per_update)\n",
    "    \n",
    "    # Tính advantage và returns sử dụng GAE\n",
    "    advantages, returns = compute_gae(rewards, masks, values, gamma, lmbda)\n",
    "    \n",
    "    # Cập nhật PPO qua nhiều epoch trên minibatch\n",
    "    ppo_update(model, optimizer, states, actions, log_probs_old, returns, advantages, clip_epsilon, ppo_epochs, minibatch_size)\n",
    "    \n",
    "    # Tính toán và in ra phần thưởng trung bình của các episode trong batch\n",
    "    if update % 10 == 0:\n",
    "        # Giả sử mỗi episode có tối đa 500 bước, ta tính số episode = len(rewards)/500\n",
    "        num_episodes = len(rewards) / 500\n",
    "        avg_reward = np.sum(rewards) / (num_episodes if num_episodes > 0 else 1)\n",
    "        all_episode_rewards.append(avg_reward)\n",
    "        print(f'Update {update}, Average Reward: {avg_reward:.2f}')\n",
    "\n",
    "# --- 8. Sau khi huấn luyện, visualize hoạt ảnh của agent ---\n",
    "# Chạy một vài episode và render hoạt ảnh\n",
    "for ep in range(5):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        env.render()  # Hiển thị hoạt ảnh\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            probs, _ = model(state_tensor)\n",
    "        action = torch.argmax(probs, dim=1).item()  # Chọn hành động có xác suất cao nhất\n",
    "        next_state, reward, done, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        time.sleep(0.02)\n",
    "        if done or truncated:\n",
    "            break\n",
    "    print(f'Episode {ep+1}: Total Reward: {total_reward}')\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
