{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naaive Bayes Classifier are a family of probabilistic classifiers based on Bayes' theorem with strong (naive) independence assumptions between the features. However, in spite of their apparently over-simplified assumptions, naive Bayes classifiers have worked quite well in many real-world situations, famously document classification and spam filtering. They require a small amount of training data to estimate the necessary parameters. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features/predictors) in a learning problem.\n",
    "\n",
    "NBC models often provide generalization performance that not as good as discriminative models like logistic regression or LinearSVC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main reason that naive Bayes model are so efficient is that they learn parameters by looking at each feature individually and collect simple per-class statistics from each feature.\n",
    "\n",
    "There are three kinds of Naive Bayes Classifiers implemented in sci-kit learn library:\n",
    "- GaussianNB: It is used in classification and it assumes that features follow a normal distribution (continuous values) and is the only one suitable for continuous data.\n",
    "- BernoulliNB: It is used for discrete data (binary values) and assumes that all our features are binary such that they take only two values.\n",
    "- MultinomialNB: It is used for discrete data (count data) and assumes that all our features are multinomially distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will use GaussianNB classifier for this problem.\n",
    "While BernoulliNB and MultinomialNB are mostly used in text classification (where the data are typically represented as word vector counts, although tf-idf vectors are also known to work well in practice)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dive into the code and see how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like every other classifier, we need to import the GaussianNB class from the sklearn.naive_bayes module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y ,test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB().fit(X_train, y_train)\n",
    "gnb_partial = GaussianNB().fit(X_train[:, :2], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb_partial.score(X_test[:, :2], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are two methods that we can use to train our model: fit and partial_fit. The difference between them is that partial_fit can be used when the whole training dataset doesn't fit into the memory. We will use the fit method to train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a prediction, a data point is compared to the statistics for each of the classes, and the best matching class is predicted. The mathematics behind this is to compute the probability of a data point belonging to a class, given the data point. The class that maximizes this probability is the predicted class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and Cons of Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MultinomialNB and BernoulliNB have a single parameter alpha which is a smoothing parameter. The way alpha work is that the algorithm adds to the data alpha many virtual data points that have positive values for all the features. This results in a “smoothing” of the statistics and prevents zero probabilities in further computations. Beside, tuning alpha is often helpful to get better generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GauusianNB is mostly used on very high-dimensional data, while other two variants of naive Bayes are widely used for sparse count data such as text. MultiomialNB usually performs better than BinaryNB, particularly on datasets with a relatively large number of nonzero features (i.e. large documents)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes learners and classifiers can be extremely fast compared to more sophisticated methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so this is the end of NBC, not too long but we're focusing on machine learning for simple task only. In the next notebook, we will learn about Decision Trees.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
