{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70d9579c",
   "metadata": {},
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c1fb561c",
   "metadata": {},
   "source": [
    "# Linear Regression:\n",
    "\n",
    "**Theory:** Linear regression is a supervised machine learning algorithm used for modeling the relationship between a dependent variable (target) and one or more independent variables (features) by fitting a linear equation. It assumes that this relationship can be represented as a straight line, which is the simplest form of regression.\n",
    "\n",
    "**Formula:** The formula for a simple linear regression (with one feature) is:\n",
    "\n",
    "$y = mx + b$\n",
    "\n",
    "Where:\n",
    "- $y$ is the target variable.\n",
    "- $x$ is the feature variable.\n",
    "- $m$ is the slope (coefficient) of the line.\n",
    "- $b$ is the y-intercept.\n",
    "\n",
    "For multiple linear regression (with more than one feature), the equation extends to:\n",
    "\n",
    "$y = b_0 + b_1x_1 + b_2x_2 + \\ldots + b_px_p$\n",
    "\n",
    "Where:\n",
    "- $y$ is the target variable.\n",
    "- $x_1, x_2, \\ldots, x_p$ are the feature variables.\n",
    "- $b_0, b_1, b_2, \\ldots, b_p$ are the coefficients.\n",
    "\n",
    "Cost Function (aka Lost function):\n",
    "\n",
    "$J_{(w)} = \\frac{1}{2m} \\sum_{i=1}^{m} (h_{w}(x^{(i)}) - y^{(i)})^2$\n",
    "\n",
    "There is a formula to calculate the right coefficients for the linear regression model. It is called the Normal Equation:\n",
    "\n",
    "$\\hat{\\theta} = (X^T \\cdot X)^{-1} \\cdot X^T \\cdot y$\n",
    "\n",
    "Where:\n",
    "- $\\hat{\\theta}$ is the value of $\\theta$ that minimizes the cost function.\n",
    "- $y$ is the vector of target values containing $y^{(1)}$ to $y^{(m)}$.\n",
    "\n",
    "**Support Functions:** Linear regression typically uses the least squares method to find the coefficients that minimize the sum of squared errors between the predicted and actual values.\n",
    "\n",
    "**Pros:**\n",
    "- Simplicity and interoperability: Easy to understand and explain.\n",
    "- Works well for linear relationships in data.\n",
    "- Quick to train and apply.\n",
    "\n",
    "**Cons:**\n",
    "- Assumes a linear relationship, which may not hold in all cases.\n",
    "- Sensitive to outliers.\n",
    "- Limited to modeling linear relationships.\n",
    "\n",
    "**When to Use:**\n",
    "- Use linear regression when you suspect a linear relationship between the independent and dependent variables.\n",
    "- It's commonly used for tasks like predicting house prices, stock prices, or any continuous numerical value based on input features.\n",
    "\n",
    "Feel free to ask if you'd like more details or have specific questions about linear regression.\n",
    "\n",
    "Certainly! Here's how you can import and use Linear Regression from TensorFlow, PyTorch, and scikit-learn:\n",
    "\n",
    "**Using scikit-learn (sklearn):**\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create a Linear Regression model\n",
    "model = LinearRegression() # You can also pass parameters here, example: solver, alpha, random_state, shuffle, penalty, etc.\n",
    "\n",
    "# Fit the model to your data (X_train and y_train are your feature and target data)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "```\n",
    "\n",
    "**Using TensorFlow:**\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define your model (Sequential API)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=1, input_shape=(num_features,))\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
    "\n",
    "# Fit the model to your data (X_train and y_train are your feature and target data)\n",
    "model.fit(X_train, y_train, epochs=num_epochs)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "```\n",
    "\n",
    "**Using PyTorch:**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a custom Linear Regression model class\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Create an instance of the model\n",
    "model = LinearRegressionModel(input_dim=num_features)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop (X_train and y_train are your feature and target data)\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test)\n",
    "```\n",
    "\n",
    "Please replace `X_train`, `y_train`, `X_test`, `num_features`, `num_epochs`, and any other placeholders with your actual data and parameters. These examples demonstrate how to create, train, and use a simple Linear Regression model in each of these libraries."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7ca657a",
   "metadata": {},
   "source": [
    "# Logistic Regression:\n",
    "\n",
    "**Theory:** Logistic Regression is a supervised machine learning algorithm used for binary and multi-class classification tasks. Despite its name, it's a classification algorithm rather than a regression algorithm. Logistic Regression models the probability that a given input belongs to a particular class.\n",
    "\n",
    "**How It Works:**\n",
    "- Logistic Regression uses the logistic (sigmoid) function to model the probability of the positive class (class 1).\n",
    "- The logistic function maps any real-valued number to a value between 0 and 1, representing the probability.\n",
    "- During training, it learns the weights and bias that best fit the data to maximize the likelihood of the observed labels.\n",
    "\n",
    "**Formula:** The formula for Logistic Regression's prediction is:\n",
    "\n",
    "$$P(Y=1|X) = \\frac{1}{1 + e^{-(\\mathbf{w} \\cdot \\mathbf{x} + b)}}$$\n",
    "\n",
    "Where:\n",
    "- $P(Y=1|X)$ is the probability of class 1.\n",
    "- $\\mathbf{w}$ is the weight vector.\n",
    "- $\\mathbf{x}$ is the input feature vector.\n",
    "- $b$ is the bias term.\n",
    "- $e$ is the base of the natural logarithm.\n",
    "\n",
    "**Basic-concept:**\n",
    "- We turn a normal Linear regression into a Logistic regression by using a sigmoid function., fit them in a range of 0 to 1.\n",
    "\n",
    "**Deep math explanation:**\n",
    "- Why we not using a linear function for classification?\n",
    "- Because the linear function can give us a value that is not in the range of 0 to 1. When we derivative the linear function, we will get a constant value. So, we can't use the gradient descent to find the minimum value of the cost function.\n",
    "- Why we use a sigmoid function?\n",
    "- Because the sigmoid function can give us a value that is in the range of 0 to 1. When we derivative the sigmoid function, we will get a variable value. So, we can use the gradient descent to find the minimum value of the cost function.\n",
    "\n",
    "**Cost function:**\n",
    "\n",
    "$$Cost(h_\\theta(x), y) = \\begin{cases} -log(h_\\theta(x)) & \\text{if } y = 1 \\\\ -log(1 - h_\\theta(x)) & \\text{if } y = 0 \\end{cases}$$\n",
    "\n",
    "Full cost function:\n",
    "$$ L(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} y^{(i)} \\log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)}))$$\n",
    "\n",
    "Break down each time derivative:\n",
    "\n",
    "$ L(\\theta) \\cong - \\sum_{i=1}^{m} [y^{(i)} \\log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)}))]$\n",
    "\n",
    "$ \\frac{\\partial L(\\theta)}{\\partial \\theta_j} = -y^{(i)} \\frac{1}{h_\\theta(x^{(i)})} \\frac{\\partial h_\\theta(x^{(i)})}{\\partial \\theta_j} - (1 - y^{(i)}) \\frac{1}{1 - h_\\theta(x^{(i)})} \\frac{\\partial h_\\theta(x^{(i)})}{\\partial \\theta_j}$\n",
    "\n",
    "$ = -y^{(i)} \\sum_{i=1}^{m} \\frac{1}{h_\\theta(x^{(i)})} \\frac{\\partial h_\\theta(x^{(i)})}{\\partial \\theta_j} + (1 - y^{(i)}) \\sum_{i=1}^{m} \\frac{1}{1 - h_\\theta(x^{(i)})} \\frac{\\partial h_\\theta(x^{(i)})}{\\partial \\theta_j}$\n",
    "\n",
    "$ = \\text{...at the end you will get Sigmoid function}$\n",
    "\n",
    "So why it should be $-\\log$ at the cost function instead of $\\log$?\n",
    "- Because log() in graph is a negative convex function, so we need to use -log() to make it a positive convex function.\n",
    "- For more calculation details, reading Gradient Descent.\n",
    "\n",
    "**Support Functions:** Logistic Regression relies on the logistic function for predictions and gradient descent for parameter updates.\n",
    "\n",
    "Now, let's see how to import and use Logistic Regression in TensorFlow, PyTorch, and scikit-learn:\n",
    "\n",
    "**Using scikit-learn (sklearn):**\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "model = LogisticRegression() # You can also pass parameters here, example: solver, cv, Cs, penalty, etc.\n",
    "\n",
    "# Fit the model to your data (X_train and y_train are your feature and target data)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "```\n",
    "\n",
    "**Using TensorFlow:**\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define your model (Sequential API)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=1, input_shape=(num_features,), activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model to your data (X_train and y_train are your feature and target data)\n",
    "model.fit(X_train, y_train, epochs=num_epochs)\n",
    "\n",
    "# Make predictions\n",
    "predictions = (model.predict(X_test) > 0.5).astype(int)\n",
    "```\n",
    "\n",
    "**Using PyTorch:**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a custom Logistic Regression model class\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "# Create an instance of the model\n",
    "model = LogisticRegressionModel(input_dim=num_features)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop (X_train and y_train are your feature and target data)\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train.view(-1, 1).float())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "In these code examples, I've shown how to create, train, and use a Logistic Regression model in each of the mentioned libraries."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab8ef954",
   "metadata": {},
   "source": [
    "# Softmax Regression (Multinomial Logistic Regression):\n",
    "\n",
    "**Theory:** Softmax Regression, also known as Multinomial Logistic Regression, is a supervised machine learning algorithm used for multi-class classification. Unlike binary classifiers, which classify data into two classes, Softmax Regression classifies data into multiple mutually exclusive classes.\n",
    "\n",
    "**How It Works:**\n",
    "- Softmax Regression extends logistic regression to handle multiple classes by using the softmax function to calculate class probabilities.\n",
    "- It assigns a probability to each class and selects the class with the highest probability as the prediction.\n",
    "- During training, it learns the weights and biases for each class to maximize the likelihood of the observed labels.\n",
    "\n",
    "**Formula (Softmax Function):**\n",
    "The softmax function computes the probability distribution over multiple classes for a given input:\n",
    "\n",
    "$$P(Y=j|X) = \\frac{e^{(\\mathbf{w_j} \\cdot \\mathbf{x} + b_j)}}{\\sum_{k=1}^{K} e^{(\\mathbf{w_k} \\cdot \\mathbf{x} + b_k)}}$$\n",
    "\n",
    "Where:\n",
    "- $P(Y=j|X)$ is the probability that the input belongs to class $j$.\n",
    "- $\\mathbf{w_j}$ is the weight vector for class $j$.\n",
    "- $\\mathbf{x}$ is the input feature vector.\n",
    "- $b_j$ is the bias term for class $j$.\n",
    "- $K$ is the total number of classes.\n",
    "\n",
    "**Cost Function:**\n",
    "The cost function for Softmax Regression is the cross-entropy loss:\n",
    "$$ a_1 = \\frac{e^{z_1}}{\\sum_{i=1}^{K} e^{z_i}} $$\n",
    "\n",
    "$$ a_n = \\frac{e^{z_n}}{\\sum_{i=1}^{K} e^{z_i}} $$\n",
    "\n",
    "$$ \\text{loss}(a_1,...,a_n, y) = \\begin{cases} -log(a_1) \\ \\text{if y = 1} \\\\ -log(a_2) \\ \\text{if y = 2} \\\\ \\text{...} \\\\ -log(a_n) \\ \\text{if y = n} \\end{cases} $$\n",
    "\n",
    "- Finally we have the cost function:\n",
    "$$ \\text{J}(w,b) = -\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{j=1}^{K} 1\\{y^{(i)} = j\\} log \\frac{e^{w_j^T x^{(i)} + b_j}}{\\sum_{l=1}^{K} e^{w_l^T x^{(i)} + b_l}} $$\n",
    "\n",
    "**Support Functions:** Softmax Regression relies on the softmax function for probability calculation and gradient descent for parameter updates.\n",
    "\n",
    "Now, let's see code examples for Softmax Regression using TensorFlow, PyTorch, and scikit-learn:\n",
    "\n",
    "**Using scikit-learn (sklearn):**\n",
    "\n",
    "Scikit-learn provides a straightforward way to perform Softmax Regression, also known as Multinomial Logistic Regression:\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a Softmax Regression model with the desired number of classes (K)\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "\n",
    "# Fit the model to your data (X_train is your feature data, y_train are class labels)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "```\n",
    "\n",
    "**Using TensorFlow:**\n",
    "\n",
    "In TensorFlow, you can use the built-in `tf.keras` API to create a Softmax Regression model for multi-class classification:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Define a Sequential model for Softmax Regression\n",
    "model = Sequential([\n",
    "    Dense(units=num_classes, input_dim=num_features, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model to your training data (X_train and y_train are feature and label data)\n",
    "model.fit(X_train, y_train, epochs=num_epochs, batch_size=batch_size)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "```\n",
    "\n",
    "**Using PyTorch:**\n",
    "\n",
    "In PyTorch, you can create a custom Softmax Regression model for multi-class classification and define your training loop:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a custom Softmax Regression model class\n",
    "class SoftmaxRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(SoftmaxRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.softmax(self.linear(x), dim=1)\n",
    "\n",
    "# Create an instance of the model\n",
    "model = SoftmaxRegressionModel(input_dim=num_features, num_classes=num_classes)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop (X_train is your feature data, y_train is class labels)\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test)\n",
    "    \n",
    "```\n",
    "\n",
    "In these code examples, I've shown how to create, train, and use a Softmax Regression model for multi-class classification in each of the mentioned libraries. Replace placeholders with your actual data and parameters. If you have more questions or need further details, feel free to ask."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c1665c3",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors (KNN):\n",
    "\n",
    "**Theory:** K-Nearest Neighbors is a supervised machine learning algorithm used for classification and regression tasks. It's based on the principle that data points with similar features tend to be close to each other in the feature space.\n",
    "\n",
    "**How It Works:**\n",
    "- For classification: When you want to classify a new data point, KNN looks at the K nearest data points in the training dataset, we calculate the distance, try to minimize the distance, and then we group all the data point into a group then assigns the majority class among those K neighbors to the new point.\n",
    "- For regression: KNN predicts a continuous target variable by averaging or weighting the target values of the K nearest neighbors.\n",
    "\n",
    "**Support Functions:** KNN doesn't rely on specific formulas or equations. Instead, it involves distance metrics (typically Euclidean distance) to measure the similarity between data points and identify the nearest neighbors.\n",
    "\n",
    "**Pros:**\n",
    "- Simple and easy to understand.\n",
    "- No training phase; it's instance-based.\n",
    "- Works well for non-linear relationships.\n",
    "- Can be used for both classification and regression tasks.\n",
    "\n",
    "**Cons:**\n",
    "- Computationally expensive, especially with large datasets.\n",
    "- Sensitivity to the choice of the number of neighbors (K).\n",
    "- Doesn't perform well when feature scales are significantly different.\n",
    "- May not handle high-dimensional data effectively (curse of dimensionality).\n",
    "\n",
    "**Formula:**\n",
    "- Euclidean Distance: The most commonly used distance metric in KNN is Euclidean distance. It is defined as the square root of the sum of squared differences between two data points.\n",
    "\n",
    "$$ d(x, y) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2} $$\n",
    "\n",
    "- Manhattan Distance: Another distance metric that can be used in KNN is Manhattan distance. It is defined as the sum of absolute differences between two data points.\n",
    "\n",
    "$$ d(x, y) = \\sum_{i=1}^{n} |x_i - y_i| $$\n",
    "\n",
    "- Minkowski Distance: Minkowski distance is a generalized distance metric that includes Euclidean distance and Manhattan distance as special cases. It is defined as:\n",
    "\n",
    "$$ d(x, y) = \\sqrt[p]{\\sum_{i=1}^{n} |x_i - y_i|^p} $$\n",
    "\n",
    "Where:\n",
    "- $p$ is a parameter that determines the type of distance metric. When $p=1$, it is equivalent to Manhattan distance. When $p=2$, it is equivalent to Euclidean distance.\n",
    "\n",
    "**When to Use:**\n",
    "- Use KNN when you have a small to moderately sized dataset and you want to make predictions based on similarity to existing data points.\n",
    "- It's commonly used in recommendation systems, image recognition, and anomaly detection.\n",
    "\n",
    "Choosing the right value of K is important; a smaller K makes the model more sensitive to noise, while a larger K can smooth out the decision boundaries. Cross-validation can help determine an appropriate K value.\n",
    "\n",
    "Certainly! Let's discuss K-Nearest Neighbors (KNN) once more and include code examples for importing and using it in TensorFlow, PyTorch, and scikit-learn.\n",
    "\n",
    "**Support Functions:** KNN relies on the calculation of distances between data points and the selection of the K nearest neighbors.\n",
    "\n",
    "Now, let's see how to import and use K-Nearest Neighbors (KNN) in TensorFlow, PyTorch, and scikit-learn:\n",
    "\n",
    "**Using scikit-learn (sklearn):**\n",
    "\n",
    "For scikit-learn, KNN is straightforward to implement for both classification and regression tasks:\n",
    "\n",
    "**Classification:**\n",
    "\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create a KNN Classifier model with the desired number of neighbors (K)\n",
    "model = KNeighborsClassifier(n_neighbors=K)\n",
    "\n",
    "# Fit the model to your training data (X_train and y_train are your feature and target data)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions for classification\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "```\n",
    "\n",
    "**Regression:**\n",
    "\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Create a KNN Regressor model with the desired number of neighbors (K)\n",
    "model = KNeighborsRegressor(n_neighbors=K)\n",
    "\n",
    "# Fit the model to your training data (X_train and y_train are your feature and target data)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions for regression\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8a1eff1",
   "metadata": {},
   "source": [
    "# Decision Tree:\n",
    "\n",
    "**Theory:** Decision Trees are supervised machine learning algorithms used for classification and regression tasks. They learn a series of hierarchical decision rules that classify the data based on the feature values.\n",
    "\n",
    "**How It Works:**\n",
    "- Decision Trees learn a series of hierarchical decision rules that classify the data based on the feature values.\n",
    "\n",
    "**Support Functions:** \n",
    "- Decision Trees rely on the calculation of information gain and entropy to determine the best split at each node.\n",
    "\n",
    "**Pros:**\n",
    "- Simple and easy to understand.\n",
    "- Can handle both numerical and categorical data.\n",
    "- Can be used for both classification and regression tasks.\n",
    "- Can be used for feature selection.\n",
    "- Robust to outliers and missing values.\n",
    "\n",
    "**Cons:**\n",
    "- Prone to overfitting.\n",
    "- Sensitive to small variations in the data.\n",
    "- Unstable; a small change in the data can lead to a large change in the structure of the tree.\n",
    "\n",
    "## Information Gain and Entropy\n",
    "**Entropy**\n",
    "- A measure of uncertainty or randomness in the data.\n",
    "    - The entropy of a dataset is 0 if all of its records belong to the same class.\n",
    "    $$ H(D) = -\\sum_{i=1}^{n} p_i \\log_2 p_i $$\n",
    "\n",
    "    Where:\n",
    "    - $H(D)$ is the entropy of the dataset $D$.\n",
    "    - $p_i$ is the probability of the $i$-th class.\n",
    "\n",
    "    - Condition Entropy:\n",
    "    $$ H(D|A) = \\sum_{v=1}^{V} p(x)H(D|A = v) $$\n",
    "\n",
    "    Where:\n",
    "    - $H(D|A)$ is the conditional entropy of the dataset $D$ given the attribute $A$.\n",
    "    - $p(x)$ is the probability of the attribute $A$ having the value $v$.\n",
    "    - $H(D|A = v)$ is the entropy of the dataset $D$ for which the attribute $A$ has the value $v$.\n",
    "\n",
    "**Formula:**\n",
    "- Gini Impurity (CART - Classification and Regression Trees): The Gini Impurity is a measure of the probability of misclassifying a randomly chosen element in the dataset if it were randomly labeled according to the class distribution in the dataset.\n",
    "        - For a node with multiple classes (classification):\n",
    "            $$ \\text{Gini}(p) = 1 - \\sum_{i=1}^{n} p_i^2 $$\n",
    "            - Where:\n",
    "                - $\\text{Gini}(p)$ is the Gini Impurity of the node.\n",
    "                - $p_i$ is the probability of the $i$-th class.\n",
    "- Mean Squared Error (MSE - CART for Regression):\n",
    "        - For a node in regression task:\n",
    "            $$\\text{MSE(D)} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\bar{y})^2$$\n",
    "            - Where:\n",
    "                - $\\text{MSE(D)}$ is the mean squared error of the dataset $D$.\n",
    "                - $y_i$ is the target value of the $i$-th record.\n",
    "                - $\\bar{y}$ is the mean target value of the dataset $D$.\n",
    "\n",
    "- Information Gain (ID3 and C4.5):\n",
    "- For a node with multiple classes (classification):\n",
    "    $$\\text{IG(D,A)} = \\text{H(D)} - \\text{H(D|A)}$$\n",
    "- For a node in regression task:\n",
    "    $$\\text{IG(D,A)} = \\text{MSE(D)} - \\text{MSE(D|A)}$$\n",
    "- Where:\n",
    "    - $\\text{IG(D,A)}$ is the information gain of the node.\n",
    "    - $\\text{H(D)}$ is the entropy of the dataset $D$.\n",
    "    - $\\text{H(D|A)}$ is the conditional entropy of the dataset $D$ given the attribute $A$.\n",
    "    - $\\text{MSE(D)}$ is the mean squared error of the dataset $D$.\n",
    "    - $\\text{MSE(D|A)}$ is the conditional mean squared error of the dataset $D$ given the attribute $A$.\n",
    "\n",
    "- SplitInfo (C4.5):\n",
    "    $$\\text{SplitInfo(D,A)} = -\\sum_{v=1}^{V} \\frac{|D_A| A = v|}{|D|} \\log_2 \\frac{|D_A| A = v|}{|D|}$$\n",
    "    - Where:\n",
    "        - $\\text{SplitInfo(D,A)}$ is the split information of the node.\n",
    "        - $p(x)$ is the probability of the attribute $A$ having the value $v$.\n",
    "\n",
    "- Gain Ratio (C4.5):\n",
    "    $$\\text{GainRatio(D,A)} = \\frac{\\text{IG(D,A)}}{\\text{SplitInfo(D,A)}}$$\n",
    "    - Where:\n",
    "        - $\\text{GainRatio(D,A)}$ is the gain ratio of the node.\n",
    "        - $\\text{IG(D,A)}$ is the information gain of the node.\n",
    "        - $\\text{SplitInfo(D,A)}$ is the split information of the node.\n",
    "\n",
    "**Remember** This will work good when your data already in best split (Perfect separation)\n",
    "        \n",
    "**When to Use:**\n",
    "- Use Decision Trees when you want to discover the underlying structure in your data and don't have prior knowledge about the relationships between the features.\n",
    "- It's commonly used in customer segmentation, fraud detection, and recommendation systems.\n",
    "\n",
    "**Support Functions:** Decision Trees rely on the calculation of information gain and entropy to determine the best split at each node.\n",
    "\n",
    "Now, let's see how to import and use Decision Trees in TensorFlow, PyTorch, and scikit-learn:\n",
    "\n",
    "**Using scikit-learn (sklearn):**\n",
    "- **For Classification Task**\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create a Decision Tree Classifier model\n",
    "model = DecisionTreeClassifier() # You can also pass parameters here, example: criterion, max_depth, min_samples_split, etc.\n",
    "\n",
    "# Fit the model to your data (X_train and y_train are your feature and target data)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "```\n",
    "\n",
    "**Using TensorFlow:**\n",
    "```python\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split # A little help with sklearn\n",
    "\n",
    "# Generate synthetic data\n",
    "num_samples = 1000\n",
    "num_features = 2\n",
    "X = np.random.randn(num_samples, num_features)\n",
    "y = (X[:, 0] + X[:, 1] > 0).astype(int)  # Binary label based on a simple condition\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define your model (Sequential API)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=1, input_shape=(num_features,), activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model to your data\n",
    "num_epochs = 50\n",
    "model.fit(X_train, y_train, epochs=num_epochs, verbose=1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "```\n",
    "\n",
    "**Using PyTorch:**\n",
    "```python\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Generate synthetic data\n",
    "num_samples = 1000\n",
    "num_features = 2\n",
    "X = np.random.randn(num_samples, num_features)\n",
    "y = (X[:, 0] + X[:, 1] > 0).astype(np.float32)  # Binary label based on a simple condition\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create DataLoader for PyTorch\n",
    "train_dataset = TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train))\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Define your model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(num_features, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleModel()\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "with torch.no_grad():\n",
    "    test_inputs = torch.Tensor(X_test)\n",
    "    test_labels = torch.Tensor(y_test).view(-1, 1)\n",
    "    test_outputs = model(test_inputs)\n",
    "    test_loss = criterion(test_outputs, test_labels)\n",
    "    predicted = (test_outputs > 0.5).numpy()\n",
    "    accuracy = np.mean((predicted == test_labels.numpy()).astype(float))\n",
    "\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy:.4f}')\n",
    "```\n",
    "\n",
    "- ***For Regression Task**\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Create a Decision Tree Regressor model\n",
    "model = DecisionTreeRegressor() # You can also pass parameters here, example: criterion, max_depth, min_samples_split, etc.\n",
    "\n",
    "# Fit the model to your data (X_train and y_train are your feature and target data)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "```\n",
    "\n",
    "**Using TensorFlow:**\n",
    "```python\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split # A little help with sklearn\n",
    "\n",
    "# Generate synthetic data\n",
    "num_samples = 1000\n",
    "num_features = 2\n",
    "X = np.random.randn(num_samples, num_features)\n",
    "y = X[:, 0] + X[:, 1]  # Target value based on a simple condition\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define your model (Sequential API)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=1, input_shape=(num_features,))\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
    "\n",
    "# Fit the model to your data\n",
    "num_epochs = 50\n",
    "model.fit(X_train, y_train, epochs=num_epochs, verbose=1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "```\n",
    "\n",
    "**Using PyTorch:**\n",
    "```python\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate synthetic data\n",
    "num_samples = 1000\n",
    "num_features = 2\n",
    "X = np.random.randn(num_samples, num_features)\n",
    "y = X[:, 0] + X[:, 1]  # Target value based on a simple condition\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define your model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(num_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleModel()\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(torch.Tensor(X_train))\n",
    "    loss = criterion(outputs, torch.Tensor(y_train).view(-1, 1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "with torch.no_grad():\n",
    "    test_inputs = torch.Tensor(X_test)\n",
    "    test_labels = torch.Tensor(y_test).view(-1, 1)\n",
    "    test_outputs = model(test_inputs)\n",
    "    test_loss = criterion(test_outputs, test_labels)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f093e23",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "**Theory:** Random Forest is an ensemble machine learning algorithm used for classification and regression tasks. It's an ensemble of Decision Trees that combines the predictions of multiple decision trees to produce a more accurate prediction than a single decision tree.\n",
    "\n",
    "**How It Works:**\n",
    "- Random Forest combines the predictions of multiple decision trees to produce a more accurate prediction than a single decision tree.\n",
    "- It randomly selects a subset of features at each node to determine the best split.\n",
    "- During training, it learns the weights and biases for each class to maximize the likelihood of the observed labels.\n",
    "\n",
    "**Support Functions:** Random Forest relies on the calculation of information gain and entropy to determine the best split at each node.\n",
    "\n",
    "**Pros:**\n",
    "- Simple and easy to understand.\n",
    "- Can handle both numerical and categorical data.\n",
    "- Can be used for both classification and regression tasks.\n",
    "\n",
    "**Cons:**\n",
    "- Prone to overfitting.\n",
    "- Sensitive to small variations in the data.\n",
    "- Unstable; a small change in the data can lead to a large change in the structure of the tree.\n",
    "\n",
    "**Formula:**\n",
    "- The same as Decision Tree but instead of using one tree, we use multiple trees and then we use the majority vote to get the final result.\n",
    "- It will using sampling with replacement (Bootstrap Aggregating) to get the data for each tree with RFS (Random Feature Selection). \n",
    "\n",
    "**When to Use:**\n",
    "- Use Random Forest when you want to discover the underlying structure in your data and don't have prior knowledge about the relationships between the features.\n",
    "- It's commonly used in customer segmentation, fraud detection, and recommendation systems.\n",
    "\n",
    "**Support Functions:** Random Forest relies on the calculation of information gain and entropy to determine the best split at each node.\n",
    "\n",
    "Now, let's see how to import and use Random Forest in TensorFlow, PyTorch, and scikit-learn:\n",
    "\n",
    "**Using scikit-learn (sklearn):**\n",
    "- **For Classification Task**\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a Random Forest Classifier model with the desired number of trees (n_estimators)\n",
    "model = RandomForestClassifier(n_estimators=100) # You can also pass parameters here, example: criterion, max_depth, min_samples_split, etc.\n",
    "\n",
    "# Fit the model to your data (X_train and y_train are your feature and target data)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "```\n",
    "\n",
    "**Using TensorFlow:**\n",
    "```python\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split # A little help with sklearn\n",
    "\n",
    "# Generate synthetic data\n",
    "num_samples = 1000\n",
    "num_features = 2\n",
    "X = np.random.randn(num_samples, num_features)\n",
    "y = (X[:, 0] + X[:, 1] > 0).astype(int)  # Binary label based on a simple condition\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define your model (Sequential API)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=1, input_shape=(num_features,), activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model to your data\n",
    "num_epochs = 50\n",
    "model.fit(X_train, y_train, epochs=num_epochs, verbose=1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "```\n",
    "\n",
    "**Using PyTorch:**\n",
    "```python\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate synthetic data\n",
    "num_samples = 1000\n",
    "num_features = 2\n",
    "X = np.random.randn(num_samples, num_features)\n",
    "y = (X[:, 0] + X[:, 1] > 0).astype(np.float32)  # Binary label based on a simple condition\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define your model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(num_features, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleModel()\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = model(torch.Tensor(X_train))\n",
    "    loss = criterion(outputs, torch.Tensor(y_train).view(-1, 1))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "with torch.no_grad():\n",
    "    test_inputs = torch.Tensor(X_test)\n",
    "    test_labels = torch.Tensor(y_test).view(-1, 1)\n",
    "    test_outputs = model(test_inputs)\n",
    "    test_loss = criterion(test_outputs, test_labels)\n",
    "    predicted = (test_outputs > 0.5).numpy()\n",
    "    accuracy = np.mean((predicted == test_labels.numpy()).astype(float))\n",
    "\n",
    "# Print the results\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy:.4f}')\n",
    "```\n",
    "\n",
    "- ***For Regression Task**\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Create a Random Forest Regressor model with the desired number of trees (n_estimators)\n",
    "model = RandomForestRegressor(n_estimators=100) # You can also pass parameters here, example: criterion, max_depth, min_samples_split, etc.\n",
    "\n",
    "# Fit the model to your data (X_train and y_train are your feature and target data)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "```\n",
    "\n",
    "**Using TensorFlow:**\n",
    "```python\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split # A little help with sklearn\n",
    "\n",
    "# Generate synthetic data\n",
    "num_samples = 1000\n",
    "num_features = 2\n",
    "X = np.random.randn(num_samples, num_features)\n",
    "y = X[:, 0] + X[:, 1]  # Target value based on a simple condition\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define your model (Sequential API)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=1, input_shape=(num_features,))\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
    "\n",
    "# Fit the model to your data\n",
    "num_epochs = 50\n",
    "model.fit(X_train, y_train, epochs=num_epochs, verbose=1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "```\n",
    "\n",
    "**Using PyTorch:**\n",
    "```python\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Generate synthetic data\n",
    "num_samples = 1000\n",
    "num_features = 2\n",
    "X = np.random.randn(num_samples, num_features)\n",
    "y = X[:, 0] + X[:, 1]  # Target value based on a simple condition\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define your model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(num_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleModel()\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(torch.Tensor(X_train))\n",
    "    loss = criterion(outputs, torch.Tensor(y_train).view(-1, 1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "with torch.no_grad():\n",
    "    test_inputs = torch.Tensor(X_test)\n",
    "    test_labels = torch.Tensor(y_test).view(-1, 1)\n",
    "    test_outputs = model(test_inputs)\n",
    "    test_loss = criterion(test_outputs, test_labels)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dba0ed2",
   "metadata": {},
   "source": [
    "# Ensemble Learning\n",
    "\n",
    "**Theory:** Ensemble Learning is a machine learning technique that combines the predictions of multiple models to produce a more accurate prediction than a single model.\n",
    "\n",
    "**How It Works:**\n",
    "- Ensemble Learning combines the predictions of multiple models to produce a more accurate prediction than a single model.\n",
    "- It can be used for both classification and regression tasks.\n",
    "\n",
    "**Support Functions:** Ensemble Learning relies on the calculation of information gain and entropy to determine the best split at each node.\n",
    "\n",
    "**Pros:**\n",
    "- Simple and easy to understand.\n",
    "- Can handle both numerical and categorical data.\n",
    "- Can be used for both classification and regression tasks.\n",
    "\n",
    "**Cons:**\n",
    "- Prone to overfitting.\n",
    "- Sensitive to small variations in the data.\n",
    "- Unstable; a small change in the data can lead to a large change in the structure\n",
    "\n",
    "1. Bagging (Bootstrap Aggregating)\n",
    "- Bagging is an ensemble learning technique that uses the bootstrap sampling method to generate multiple training datasets from the original dataset.\n",
    "\n",
    "\n",
    "2. Boosting\n",
    "- Boosting is an ensemble learning technique that uses the weighted average of multiple weak learners to produce a strong learner.\n",
    "- Included in AdaBoost, Gradient Boosting, and XGBoost.\n",
    "\n",
    "3. Voting\n",
    "- Voting is an ensemble learning technique that combines the predictions of multiple models to produce a more accurate prediction than a single model.\n",
    "- Included in Hard Voting and Soft Voting.\n",
    "\n",
    "4. Stacking\n",
    "- Stacking is an ensemble learning technique that have some familiar with Neural Network, it's like a Neural Network but instead of using the output of each layer as input for the next layer, we use the output of each model as input for the next model.\n",
    "\n",
    "**Formula:**\n",
    "- The same as Decision Tree but instead of using one model(or tree in DT), we use multiple models and then we use the majority vote to get the final result.\n",
    "\n",
    "**Code**\n",
    "\n",
    "**Using scikit-learn (sklearn):**\n",
    "- **For Classification Task**\n",
    "```python\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create a Voting Classifier model with the desired estimators\n",
    "model = VotingClassifier(estimators=[\n",
    "    ('lr', LogisticRegression()),\n",
    "    ('svm', SVC()),\n",
    "    ('dt', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "# Fit the model to your data (X_train and y_train are your feature and target data)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "```\n",
    "\n",
    "**Using TensorFlow:**\n",
    "```python\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split # A little help with sklearn\n",
    "\n",
    "# Generate synthetic data\n",
    "num_samples = 1000\n",
    "num_features = 2\n",
    "X = np.random.randn(num_samples, num_features)\n",
    "y = (X[:, 0] + X[:, 1] > 0).astype(int)  # Binary label based on a simple condition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf04ccc",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n",
    "- Gradient Boosting is an ensemble learning technique that uses the weighted average of multiple weak learners to produce a strong learner.\n",
    "- It's included in AdaBoost, Gradient Boosting, and XGBoost.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b9ab1c6",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVMs)\n",
    "\n",
    "Support Vector Machine (SVMs) is a powerful and widely used supervised machine learning algorithm that is primarily used for classification tasks. SVMs are known for their ability to handle both linear and non-linear classification problems efficiently. They work by finding the optimal hyperplane (or hyperplanes) that best separates data points of different classes in feature space. Here are the key components and concepts related to Support Vector Machines:\n",
    "\n",
    "**Theory**\n",
    "Define the hyperplanes H such that:\n",
    "$$ w^T x + b \\geq 1 \\ \\text{if} \\ y = 1 $$\n",
    "$$ w^T x + b \\leq -1 \\ \\text{if} \\ y = -1 $$\n",
    "And $H_1$ and $H_2$ are the planes:\n",
    "$$ H_1: w^T x + b = 1 $$\n",
    "$$ H_2: w^T x + b = -1 $$\n",
    "Then we have the plane between $H_1$ and $H_2$ is the median between, where we call it $H_0$:\n",
    "$$ H_0: w^T x + b = 0 $$\n",
    "\n",
    "**Support Vectors**\n",
    "- Support vectors are data points that lie closest to the hyperplane(s). They are the ones that significantly influence the positioning and orientation of the hyperplane. In other words, they are the data points that are used in the margin calculation and contribute to finding the optimal hyperplane.\n",
    "\n",
    "**Cost Function**\n",
    "- In summary, Support Vector Machines (SVMs) are versatile and powerful machine learning algorithms used for classification tasks. They are known for their ability to find optimal decision boundaries while maximizing the margin between classes, even in high-dimensional feature spaces. The kernel trick allows SVMs to handle non-linear data, making them suitable for a wide range of applications.\n",
    "$$ min_{\\theta} C \\sum_{i=1}^{m} \\left[ y^{(i)} cost_1(\\theta^T x^{(i)}) + (1 - y^{(i)}) cost_0(\\theta^T x^{(i)}) \\right] + \\frac{1}{2} \\sum_{j=1}^{n} \\theta_j^2 $$\n",
    "- Also we can use thing call Hinge Loss:\n",
    "$$ cost_1(\\theta^T x^{(i)}) = \\max(0, 1 - \\theta^T x^{(i)}) $$\n",
    "\n",
    "**Linear Programming in SVMs**\n",
    "- Linear programming is a mathematical technique used to find the best possible outcome or solution from a given set of constraints or conditions. It is widely used in the field of optimization. In SVMs, linear programming is used to find the optimal hyperplane that maximizes the margin between classes.\n",
    "- What we do is making the data points as far as possible from the hyperplane, so we can get the best hyperplane that can separate the data points.\n",
    "- Example we call that $H_1$ and $H_2$ then:\n",
    "$$ \\frac{\\|w*x+b\\|}{\\|w\\|} = \\frac{1}{\\|w\\|} $$\n",
    "or\n",
    "$$ \\text{dist} = \\frac{\\left| Ax + By + C \\right|}{\\sqrt{A^2 + B^2}} = \\frac{1}{\\sqrt{A^2 + B^2}} $$\n",
    "=> We have to maximizing the margin, so we have to minimize $\\|w\\|$ in condition that no data points between $H_1$ and $H_2$.\n",
    "$$\\begin{cases} w*x^{(i)}+b \\geq 1 \\ \\text{if} \\ y^{(i)} = 1 \\\\ w*x^{(i)}+b \\leq 1 \\ \\text{if} \\ y^{(i)} = -1 \\end{cases}$$\n",
    "\n",
    "**Special:**\n",
    "- There're some useful trick to training this model is using Lagrange Multiplier and KKT (Karush-Kuhn-Tucker) Conditions. \n",
    "- Sometime your data can be in low dimension but it's not linear separable, so you can use something call Kernel Trick to make it linear separable. (RBF Kernel, Polynomial Kernel, Nystrm Kernel, etc.)\n",
    "\n",
    "**Using scikit-learn for SVM:**\n",
    "\n",
    "```python\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load a dataset (e.g., the Iris dataset)\n",
    "data = datasets.load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVM classifier with a linear kernel\n",
    "clf = svm.SVC(kernel='linear')\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "```\n",
    "\n",
    "**Using PyTorch for SVM:**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load a dataset (e.g., the Iris dataset)\n",
    "data = datasets.load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a custom SVM model class\n",
    "class SVM(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SVM, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Create an instance of the model\n",
    "model = SVM(input_dim=X_train.shape[1])\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.HingeEmbeddingLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    inputs = torch.Tensor(X_train)\n",
    "    labels = torch.Tensor(y_train).view(-1, 1)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    test_inputs = torch.Tensor(X_test)\n",
    "    test_labels = torch.Tensor(y_test).view(-1, 1)\n",
    "    test_outputs = model(test_inputs)\n",
    "    predicted = (test_outputs > 0).numpy()\n",
    "    accuracy = accuracy_score(test_labels.numpy(), predicted)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "```\n",
    "\n",
    "In this scikit-learn example, we load the Iris dataset, split it into training and testing sets, create an SVM classifier with a linear kernel, fit the classifier to the training data, make predictions on the test data, and calculate the accuracy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "219a0080",
   "metadata": {},
   "source": [
    "# Soft Margin SVM\n",
    "\n",
    "A Soft Margin SVM is a variant of the Support Vector Machine (SVM) algorithm that allows for some level of misclassification in order to find a more flexible decision boundary. In traditional (hard margin) SVM, the objective is to find a hyperplane that perfectly separates the two classes, which may not always be possible or desirable in real-world scenarios due to noisy or overlapping data. The Soft Margin SVM introduces a regularization parameter $C$ to control the trade-off between maximizing the margin and allowing some misclassification.\n",
    "\n",
    "Here's a code example using scikit-learn to implement a Soft Margin SVM:\n",
    "\n",
    "```python\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load a dataset (e.g., the Iris dataset)\n",
    "data = datasets.load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Soft Margin SVM classifier with a linear kernel and regularization parameter C\n",
    "# The higher the value of C, the less tolerance for misclassification\n",
    "clf = svm.SVC(kernel='linear', C=1.0)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "```\n",
    "\n",
    "In this code example, we use the `svm.SVC` class from scikit-learn to create a Soft Margin SVM classifier with a linear kernel. The `C` parameter controls the regularization strength. A smaller `C` value allows for a wider margin but may tolerate some misclassification, while a larger `C` value enforces stricter classification.\n",
    "\n",
    "You can adjust the value of `C` based on your specific problem and the trade-off you want between maximizing the margin and allowing for misclassification."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1915608f",
   "metadata": {},
   "source": [
    "# Kernel SVM\n",
    "\n",
    "Kernel SVM, also known as Support Vector Machines with Kernels, is an extension of the traditional SVM algorithm that allows for the classification of non-linearly separable data. It achieves this by mapping the original feature space into a higher-dimensional space, where the data may become linearly separable. This transformation is performed using kernel functions. Kernel SVMs are a powerful tool for handling complex decision boundaries in various machine learning tasks. Here's a code example using scikit-learn to implement Kernel SVM:\n",
    "\n",
    "```python\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load a dataset (e.g., the Iris dataset)\n",
    "data = datasets.load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Kernel SVM classifier with a radial basis function (RBF) kernel\n",
    "clf = svm.SVC(kernel='rbf') # Kernel can be 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "```\n",
    "\n",
    "In this code example, we use the `svm.SVC` class from scikit-learn to create a Kernel SVM classifier with an RBF kernel. The RBF kernel is one of the most commonly used kernel functions, but scikit-learn also supports other kernel functions like polynomial and sigmoid kernels.\n",
    "\n",
    "Kernel SVMs are particularly effective when dealing with non-linearly separable data, as they can implicitly map the data into a higher-dimensional space where it becomes linearly separable. The choice of the kernel and its hyperparameters can significantly impact the SVM's performance, so it's important to experiment and choose the appropriate kernel function for your specific problem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7886a046",
   "metadata": {},
   "source": [
    "# Multi-class SVM\n",
    "\n",
    "**Theory:**\n",
    "A Multi-class Support Vector Machine (SVM) is an extension of the traditional binary SVM to handle classification problems with more than two classes. In multi-class SVM, the goal is to assign each data point to one of several possible classes or categories. Several strategies can be employed to extend binary SVM to multi-class problems, such as one-vs-all (OvA), one-vs-one (OvO), and softmax-based approaches.\n",
    "\n",
    "**Pro:**\n",
    "- Multi-class SVMs are a powerful tool for multi-class classification tasks, as they can handle complex decision boundaries and non-linearly separable data.\n",
    "\n",
    "**Con:**\n",
    "- Multi-class SVMs can be computationally expensive, especially when using the OvO strategy, which trains a binary classifier for each pair of classes.\n",
    "\n",
    "**Code Example (Multi-class SVM in scikit-learn):**\n",
    "Here's a code example using scikit-learn to implement a Multi-class SVM using the OvA strategy:\n",
    "\n",
    "```python\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load a dataset (e.g., the Iris dataset)\n",
    "data = datasets.load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Multi-class SVM classifier with a linear kernel using the One-vs-All (OvA) strategy\n",
    "clf = svm.SVC(kernel='linear', decision_function_shape='ovr') # decision_function_shape can be 'ovr' or 'ovo'\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "```\n",
    "\n",
    "In this code example, we use the `svm.SVC` class from scikit-learn to create a Multi-class SVM classifier with a linear kernel. We specify the `decision_function_shape` parameter as `'ovr'`, which stands for One-vs-Rest or One-vs-All. This strategy trains a binary classifier for each class, where each classifier separates one class from the rest of the classes. The final prediction is made by selecting the class with the highest confidence score among all binary classifiers.\n",
    "\n",
    "You can adjust the choice of kernel, kernel parameters, and the multi-class strategy based on your specific problem and data characteristics. Scikit-learn provides a flexible and convenient way to implement multi-class SVMs for various applications."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d9e5d01",
   "metadata": {},
   "source": [
    "# Perceptron Learning Algorithm (PLA):\n",
    "\n",
    "**Theory:** The Perceptron Learning Algorithm is a simple supervised machine learning algorithm used for binary classification tasks. It's based on the concept of a perceptron, which is a linear classifier. The algorithm learns a linear decision boundary that separates two classes in the feature space.\n",
    "\n",
    "**How It Works:**\n",
    "- The PLA takes a set of input features and assigns weights to each feature.\n",
    "- It calculates the weighted sum of the input features and applies a step function to make a binary prediction (usually 0 or 1).\n",
    "- During training, if the prediction is incorrect, it updates the weights to correct the error.\n",
    "\n",
    "**Formula:** The formula for the Perceptron's prediction is:\n",
    "\n",
    "$$y = \\text{step}(\\mathbf{w} \\cdot \\mathbf{x} + b)$$\n",
    "\n",
    "Where:\n",
    "- $y$ is the predicted class (0 or 1).\n",
    "- $\\mathbf{w}$ is the weight vector.\n",
    "- $\\mathbf{x}$ is the input feature vector.\n",
    "- $b$ is the bias term.\n",
    "- $\\text{step}()$ is the step function that returns 0 for values less than or equal to 0 and 1 otherwise.\n",
    "\n",
    "**Support Functions:** The key operation in the PLA is updating the weight vector and bias term based on misclassifications.\n",
    "\n",
    "Now, let's see how to import and use the Perceptron Learning Algorithm in TensorFlow, PyTorch, and scikit-learn:\n",
    "\n",
    "**Using scikit-learn (sklearn):**\n",
    "\n",
    "Scikit-learn provides a `Perceptron` class for implementing the Perceptron Learning Algorithm:\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "# Create a Perceptron model\n",
    "model = Perceptron()\n",
    "\n",
    "# Fit the model to your data (X_train and y_train are your feature and target data)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "```\n",
    "\n",
    "**Using TensorFlow:**\n",
    "\n",
    "TensorFlow doesn't have a specific class for the Perceptron, but you can create a custom one-layer neural network:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define a custom Perceptron-like model (Sequential API)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=1, input_shape=(num_features,), activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model to your data (X_train and y_train are your feature and target data)\n",
    "model.fit(X_train, y_train, epochs=num_epochs)\n",
    "\n",
    "# Make predictions\n",
    "predictions = (model.predict(X_test) > 0.5).astype(int)\n",
    "\n",
    "```\n",
    "\n",
    "**Using PyTorch:**\n",
    "\n",
    "In PyTorch, you can create a custom Perceptron model as follows:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a custom Perceptron-like model class\n",
    "class PerceptronModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(PerceptronModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "# Create an instance of the model\n",
    "model = PerceptronModel(input_dim=num_features)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop (X_train and y_train are your feature and target data)\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train.view(-1, 1).float())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "```\n",
    "\n",
    "In these code examples, I've shown how to create, train, and use a Perceptron model, which implements the Perceptron Learning Algorithm, in each of the mentioned libraries. Please replace placeholders with your actual data and parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbcbc8d",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier\n",
    "\n",
    "**Theory:** The Naive Bayes Classifier is a simple supervised machine learning algorithm that is used for classification tasks. It's based on the Bayes Theorem, which describes the probability of an event based on prior knowledge of conditions that might be related to the event. The Naive Bayes Classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. In other words, it assumes that the features are independent of each other. This is a strong assumption, which is why it's called \"naive\". Despite this simplifying assumption, the Naive Bayes Classifier has been shown to perform well in many real-world applications.\n",
    "\n",
    "**How It Works:**\n",
    "- The Naive Bayes Classifier calculates the probability of a data point belonging to a particular class using Bayes Theorem.\n",
    "- It then assigns the data point to the class with the highest probability.\n",
    "\n",
    "**Formula:** \n",
    "- The formula for the Naive Bayes Classifier is:\n",
    "    - Gaussian Naive Bayes: \n",
    "        $$ P(y|x) = P(y| \\mu , \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left( -\\frac{(x-\\mu)^2}{2\\sigma^2} \\right) $$\n",
    "\n",
    "        Where the the mean $\\mu$ and variance $\\sigma^2$ are calculated by using Maximum Likelihood:\n",
    "        $$ (\\mu, \\sigma^2) = \\text{argmax} \\prod_{i=1}^{n} P(x_i | \\mu, \\sigma^2) $$\n",
    "\n",
    "    - Multinomial Naive Bayes: This mainly using in text classification task with feature vector got calculated by BOW (Bag of Words)\n",
    "        - Not fully understand so I don't write any formula here\n",
    "    - Bernoulli Naive Bayes\n",
    "        $$ P(x_i|y) = \\begin{cases} P(x_i|y) & \\text{if} \\ x_i = 1 \\\\ 1 - P(x_i|y) & \\text{if} \\ x_i = 0 \\end{cases} $$\n",
    "        - So we have a fully Bernoulli Naive Bayes:\n",
    "        $$ P(x|y) = \\prod_{i=1}^{n} P(x_i|y) $$\n",
    "        \n",
    "        Where:\n",
    "        - $P(x_i|y)$ is the probability of the $i$-th feature given the class $y$.\n",
    "        - $P(x|y)$ is the probability of the feature vector $x$ given the class $y$.\n",
    "\n",
    "**Support Functions:** The Naive Bayes Classifier relies on the calculation of probabilities using Bayes Theorem.\n",
    "\n",
    "Now, let's see how to import and use the Naive Bayes Classifier in TensorFlow, PyTorch, and scikit-learn:\n",
    "\n",
    "**Using scikit-learn (sklearn):**\n",
    "\n",
    "Scikit-learn provides several classes for implementing the Naive Bayes Classifier, including `GaussianNB`, `MultinomialNB`, and `BernoulliNB`:\n",
    "\n",
    "```python\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Create a Gaussian Naive Bayes Classifier model\n",
    "model = GaussianNB()\n",
    "\n",
    "# Fit the model to your data (X_train and y_train are your feature and target data)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "```\n",
    "\n",
    "**Using TensorFlow:**\n",
    "\n",
    "TensorFlow doesn't have a specific class for the Naive Bayes Classifier, but you can create a custom one-layer neural network:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define a custom Naive Bayes Classifier-like model (Sequential API)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=1, input_shape=(num_features,), activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model to your data (X_train and y_train are your feature and target data)\n",
    "model.fit(X_train, y_train, epochs=num_epochs)\n",
    "\n",
    "# Make predictions\n",
    "predictions = (model.predict(X_test) > 0.5).astype(int)\n",
    "\n",
    "```\n",
    "\n",
    "**Using PyTorch:**\n",
    "\n",
    "In PyTorch, you can create a custom Naive Bayes Classifier model as follows:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a custom Naive Bayes Classifier-like model class\n",
    "class NaiveBayesModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(NaiveBayesModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "# Create an instance of the model\n",
    "model = NaiveBayesModel(input_dim=num_features)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop (X_train and y_train are your feature and target data)\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train.view(-1, 1).float())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "```\n",
    "\n",
    "In these code examples, I've shown how to create, train, and use a Naive Bayes Classifier model in each of the mentioned libraries. Please replace placeholders with your actual data and parameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d2334dce",
   "metadata": {},
   "source": [
    "# Unsupervised Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9abc835",
   "metadata": {},
   "source": [
    "# K-Means Clustering:\n",
    "\n",
    "**Theory:** K-Means is an unsupervised machine learning algorithm used for partitioning a dataset into K distinct, non-overlapping clusters. The goal is to group similar data points together based on their feature similarity.\n",
    "\n",
    "**Support Functions:** K-Means clustering relies on two key components:\n",
    "\n",
    "1. **Centroids:** Each cluster is represented by a centroid, which is the mean (average) of all data points in that cluster.\n",
    "\n",
    "2. **Distance Metric:** The algorithm calculates distances (typically Euclidean distance) between data points and centroids to assign each data point to the nearest cluster.\n",
    "\n",
    "**Pros:**\n",
    "- Simple and efficient.\n",
    "- Scalable to large datasets.\n",
    "- Can be applied to a wide range of data types.\n",
    "- Useful for data exploration and segmentation.\n",
    "\n",
    "**Cons:**\n",
    "- Requires specifying the number of clusters (K) in advance, which can be challenging.\n",
    "- Sensitive to the initial placement of centroids, leading to different results for different initializations.\n",
    "- Assumes clusters are spherical and equally sized, which may not hold in all cases.\n",
    "\n",
    "**Formula**\n",
    "- Distance Metric: Euclidean distance is the most commonly used distance metric in K-Means clustering. It is defined as the square root of the sum of squared differences between two data points.\n",
    "\n",
    "$$ d(x, y) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2} $$\n",
    "\n",
    "- Cluster Centroids: The centroid of a cluster is the mean (average) of all data points in that cluster.\n",
    "\n",
    "$$ c = \\frac{1}{|S|} \\sum_{x \\in S} x $$\n",
    "\n",
    "Where:\n",
    "- $c$ is the centroid of the cluster.\n",
    "- $S$ is the set of data points in the cluster.\n",
    "\n",
    "- Optimization: The goal of K-Means clustering is to minimize the sum of squared distances between data points and their assigned clusters (also known as inertia).\n",
    "\n",
    "$$ Inertia = \\sum_{i=0}^{n} \\min_{\\mu_j \\in C} (||x_i - \\mu_j||^2) $$\n",
    "\n",
    "Where:\n",
    "- $n$ is the number of data points.\n",
    "- $C$ is the set of clusters.\n",
    "- $\\mu_j$ is the centroid of cluster $j$.\n",
    "\n",
    "**When to Use:**\n",
    "- Use K-Means when you want to discover natural groupings in your data and don't have prior knowledge about the clusters.\n",
    "- It's commonly used in customer segmentation, image compression, and recommendation systems.\n",
    "\n",
    "Remember that choosing the right value of K (number of clusters) is critical for the success of K-Means, and various methods, such as the elbow method or silhouette score, can help determine an appropriate K value.\n",
    "\n",
    "Let's discuss K-Means Clustering again and include code examples for importing and using it in TensorFlow, PyTorch, and scikit-learn.\n",
    "\n",
    "**Support Functions:** K-Means relies on the calculation of distances between data points and centroids and centroid updates.\n",
    "\n",
    "Now, let's see how to import and use K-Means Clustering in TensorFlow, PyTorch, and scikit-learn:\n",
    "\n",
    "**Using scikit-learn (sklearn):**\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create a K-Means model with the desired number of clusters (K)\n",
    "model = KMeans(n_clusters=K)\n",
    "\n",
    "# Fit the model to your data (X is your feature data)\n",
    "model.fit(X)\n",
    "\n",
    "# Get cluster assignments for each data point\n",
    "cluster_assignments = model.labels_\n",
    "\n",
    "# Get cluster centroids\n",
    "cluster_centers = model.cluster_centers_\n",
    "```\n",
    "\n",
    "**Using TensorFlow:**\n",
    "\n",
    "TensorFlow doesn't have a specific K-Means implementation in its core library, but you can use the TensorFlow K-Means Clustering API for TensorFlow 2:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops.cluster import cluster_ops\n",
    "\n",
    "# Create a TensorFlow K-Means model with the desired number of clusters (K)\n",
    "kmeans = tf.compat.v1.estimator.experimental.KMeans(\n",
    "    num_clusters=K, use_mini_batch=False)\n",
    "\n",
    "# Define input function (X is your feature data)\n",
    "def input_fn():\n",
    "    return tf.convert_to_tensor(X, dtype=tf.float32), None\n",
    "\n",
    "# Train the model\n",
    "kmeans.train(input_fn)\n",
    "\n",
    "# Get cluster assignments for each data point\n",
    "cluster_assignments = list(kmeans.predict_cluster_index(input_fn))\n",
    "\n",
    "# Get cluster centroids\n",
    "cluster_centers = kmeans.cluster_centers()\n",
    "```\n",
    "\n",
    "**Using PyTorch:**\n",
    "\n",
    "PyTorch does not have a built-in K-Means implementation in its core library. You can use external libraries or implement K-Means from scratch. Here's an example using the `KMeans` implementation from the `scikit-learn` library:\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create a K-Means model with the desired number of clusters (K)\n",
    "model = KMeans(n_clusters=K)\n",
    "\n",
    "# Fit the model to your data (X is your feature data)\n",
    "model.fit(X)\n",
    "\n",
    "# Get cluster assignments for each data point\n",
    "cluster_assignments = model.labels_\n",
    "\n",
    "# Get cluster centroids\n",
    "cluster_centers = model.cluster_centers_\n",
    "```\n",
    "\n",
    "In these code examples, I've shown how to create, train, and use a K-Means Clustering model in each of the mentioned libraries. Please replace placeholders with your actual data and parameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c0907a3",
   "metadata": {},
   "source": [
    "# Hirenchical Clustering\n",
    "\n",
    "**Theory:** Hierarchical Clustering is a clustering algorithm that groups similar objects into groups called clusters. The algorithm builds a hierarchy of clusters, where each node is a cluster consisting of the clusters of its children nodes. The algorithm can be represented using a dendrogram, which is a tree-like diagram that shows the hierarchical relationship between clusters.\n",
    "\n",
    "**How It Works:**\n",
    "- The algorithm starts by treating each data point as a separate cluster.\n",
    "- It then repeatedly merges the two closest clusters until only one cluster remains.\n",
    "\n",
    "**Pros:**\n",
    "- Simple and easy to implement.\n",
    "- Produces a hierarchy of clusters that can be visualized using a dendrogram.\n",
    "\n",
    "**Cons:**\n",
    "- Computationally expensive.\n",
    "- Difficult to determine the optimal number of clusters.\n",
    "\n",
    "**When to Use:**\n",
    "- Use Hierarchical Clustering when you want to discover natural groupings in your data and don't have prior knowledge about the clusters.\n",
    "- It's commonly used in customer segmentation, image segmentation, and recommendation systems.\n",
    "\n",
    "**Deep math behind Hierarchical Clustering:**\n",
    "- Distance Metric: Hierarchical Clustering relies on a distance metric to measure the similarity between data points. The most commonly used distance metric is Euclidean distance, which is defined as the square root of the sum of squared differences between two data points.\n",
    "$$ d(x, y) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2} $$\n",
    "\n",
    "- Linkage Criteria: The algorithm uses a linkage criterion to determine the distance between two clusters. The most commonly used linkage criteria are:\n",
    "  - **Single Linkage:** The distance between two clusters is defined as the shortest distance between any two points in the two clusters.\n",
    "  - **Complete Linkage:** The distance between two clusters is defined as the longest distance between any two points in the two clusters.\n",
    "  - **Average Linkage:** The distance between two clusters is defined as the average distance between all pairs of points in the two clusters.\n",
    "\n",
    "- Dendrogram: A dendrogram is a tree-like diagram that shows the hierarchical relationship between clusters. It can be used to visualize the results of Hierarchical Clustering.\n",
    "\n",
    "**Code Example for Hierarchical Clustering using scikit-learn:**\n",
    "Here's a code example demonstrating Hierarchical Clustering using Python's scikit-learn library:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "# Sample data (replace with your dataset)\n",
    "X = np.array([\n",
    "    [1.2, 2.3, 3.0],\n",
    "    [2.8, 5.1, 4.7],\n",
    "    [3.5, 6.4, 6.0],\n",
    "    [4.3, 8.1, 7.8],\n",
    "    [5.2, 9.7, 9.5]\n",
    "])\n",
    "\n",
    "# Create an Agglomerative Clustering model with the desired number of clusters (K)\n",
    "n_clusters = 2\n",
    "model = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "\n",
    "# Fit the model to the data\n",
    "model.fit(X)\n",
    "\n",
    "# Get cluster assignments\n",
    "cluster_assignments = model.labels_\n",
    "\n",
    "# Get cluster centroids\n",
    "cluster_centers = model.cluster_centers_\n",
    "\n",
    "# Get the number of clusters\n",
    "n_clusters = model.n_clusters_\n",
    "\n",
    "# Get the children and the distances for each cluster\n",
    "children = model.children_\n",
    "\n",
    "# Get a dendrogram\n",
    "dendrogram = dendrogram(model)\n",
    "```\n",
    "\n",
    "In this example, we create sample data $X$ and use scikit-learn's `AgglomerativeClustering` class to perform Hierarchical Clustering. We specify the desired number of clusters $k$ and fit the model to the data. The `labels_` attribute is used to obtain the cluster assignments for each data point. Additionally, we print the cluster centroids, the number of clusters, and the children and distances for each cluster. Finally, we use the `dendrogram` function from `scipy.cluster.hierarchy` to obtain a dendrogram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81f5346",
   "metadata": {},
   "source": [
    "# Latent Variable Model\n",
    "\n",
    "**Theory:** Latent Variable Models are a class of statistical models that are used to describe and explain relationships between observed variables and latent variables. They are commonly used in machine learning for dimensionality reduction, clustering, and data visualization.\n",
    "\n",
    "**How It Works:**\n",
    "- Latent Variable Models assume that there are hidden (latent) variables that are not directly observed but can be inferred from the observed variables.\n",
    "\n",
    "**Pros:**\n",
    "- Can be used for dimensionality reduction, clustering, and data visualization.\n",
    "- Can be used to discover hidden relationships between observed variables.\n",
    "\n",
    "**Cons:**\n",
    "- Can be difficult to interpret.\n",
    "- Can be computationally expensive.\n",
    "\n",
    "**When to Use:**\n",
    "- Use Latent Variable Models when you want to discover hidden relationships between observed variables and don't have prior knowledge about the relationships.\n",
    "\n",
    "**Deep math behind Latent Variable Model:**\n",
    "- Latent Variable Models are a class of statistical models that are used to describe and explain relationships between observed variables and latent variables. They are commonly used in machine learning for dimensionality reduction, clustering, and data visualization.\n",
    "\n",
    "**Code Example for Latent Variable Model using scikit-learn:**\n",
    "Here's a code example demonstrating Latent Variable Models using Python's scikit-learn library:\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a PCA model with the desired number of components (K)\n",
    "model = PCA(n_components=K)\n",
    "\n",
    "# Fit the model to the data\n",
    "model.fit(X)\n",
    "\n",
    "# Get the transformed data (X is your feature data)\n",
    "transformed_data = model.transform(X)\n",
    "\n",
    "# Get the number of components\n",
    "n_components = model.n_components_\n",
    "\n",
    "# Get the explained variance ratio\n",
    "explained_variance_ratio = model.explained_variance_ratio_\n",
    "\n",
    "# Get the singular values\n",
    "singular_values = model.singular_values_\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b2ac50",
   "metadata": {},
   "source": [
    "# t-SNE (t-Distributed Stochastic Neighbor Embedding)\n",
    "\n",
    "**Theory:** t-SNE is a dimensionality reduction technique used for data visualization. It's based on the concept of a manifold, which is a lower-dimensional space that captures the essence of a higher-dimensional space. The goal of t-SNE is to map high-dimensional data to a low-dimensional space while preserving the structure of the data as much as possible.\n",
    "\n",
    "**How It Works:**\n",
    "- t-SNE starts by calculating the probability of similarity between pairs of data points in the high-dimensional space.\n",
    "- It then calculates the probability of similarity between the same pairs of data points in the low-dimensional space.\n",
    "\n",
    "**Pros:**\n",
    "- Can be used for data visualization.\n",
    "- Can be used to discover hidden relationships between observed variables.\n",
    "\n",
    "**Cons:**\n",
    "- Can be difficult to interpret.\n",
    "- Can be computationally expensive.\n",
    "\n",
    "**When to Use:**\n",
    "- Use t-SNE when you want to visualize high-dimensional data in a low-dimensional space and don't have prior knowledge about the data.\n",
    "\n",
    "**Deep math behind t-SNE:**\n",
    "- t-SNE is a dimensionality reduction technique used for data visualization. It's based on the concept of a manifold, which is a lower-dimensional space that captures the essence of a higher-dimensional space. The goal of t-SNE is to map high-dimensional data to a low-dimensional space while preserving the structure of the data as much as possible.\n",
    "\n",
    "**Code Example for t-SNE using scikit-learn:**\n",
    "Here's a code example demonstrating t-SNE using Python's scikit-learn library:\n",
    "\n",
    "```python\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Create a t-SNE model with the desired number of components (K)\n",
    "model = TSNE(n_components=K)\n",
    "\n",
    "# Fit the model to the data\n",
    "model.fit(X)\n",
    "\n",
    "# Get the transformed data (X is your feature data)\n",
    "transformed_data = model.transform(X)\n",
    "\n",
    "# Get the number of components\n",
    "n_components = model.n_components_\n",
    "\n",
    "# Get the KL divergence\n",
    "kl_divergence = model.kl_divergence_\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97064c10",
   "metadata": {},
   "source": [
    "# Isolation Forest\n",
    "\n",
    "**Theory:** Isolation Forest is an unsupervised machine learning algorithm used for anomaly detection. It's based on the concept of an isolation tree, which is a binary tree that recursively partitions the data into subsets until each subset contains only one data point. The algorithm then calculates the average path length for each data point, which is the average number of edges that must be traversed in order to reach that data point. The intuition behind this is that anomalies are isolated and have a shorter average path length than normal data points.\n",
    "\n",
    "**How It Works:**\n",
    "- The algorithm starts by randomly selecting a feature and a split value for that feature.\n",
    "- It then splits the data into two subsets based on the selected feature and split value.\n",
    "- It then recursively splits the subsets until each subset contains only one data point.\n",
    "- It then calculates the average path length for each data point.\n",
    "- It then calculates an anomaly score for each data point based on its average path length.\n",
    "\n",
    "**Pros:**\n",
    "- Can be used for anomaly detection.\n",
    "- Can be used to discover hidden relationships between observed variables.\n",
    "\n",
    "**Cons:**\n",
    "- Can be difficult to interpret.\n",
    "- Can be computationally expensive.\n",
    "\n",
    "**When to Use:**\n",
    "- Use Isolation Forest when you want to detect anomalies in your data and don't have prior knowledge about the data.\n",
    "\n",
    "**Deep math behind Isolation Forest:**\n",
    "- Isolation Forest is an unsupervised machine learning algorithm used for anomaly detection. It's based on the concept of an isolation tree, which is a binary tree that recursively partitions the data into subsets until each subset contains only one data point. The algorithm then calculates the average path length for each data point, which is the average number of edges that must be traversed in order to reach that data point. The intuition behind this is that anomalies are isolated and have a shorter average path length than normal data points.\n",
    "\n",
    "**Code Example for Isolation Forest using scikit-learn:**\n",
    "Here's a code example demonstrating Isolation Forest using Python's scikit-learn library:\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Create an Isolation Forest model with the desired number of estimators (K)\n",
    "model = IsolationForest(n_estimators=K)\n",
    "\n",
    "# Fit the model to the data\n",
    "model.fit(X)\n",
    "\n",
    "# Get the anomaly scores (X is your feature data)\n",
    "anomaly_scores = model.decision_function(X)\n",
    "\n",
    "# Get the number of estimators\n",
    "n_estimators = model.n_estimators\n",
    "\n",
    "# Get the maximum number of samples\n",
    "max_samples = model.max_samples\n",
    "\n",
    "# Get the contamination\n",
    "contamination = model.contamination\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72e00ba",
   "metadata": {},
   "source": [
    "# Self-Organizing Maps\n",
    "\n",
    "**Theory:** Self-Organizing Maps (SOMs) are a type of artificial neural network that is used for dimensionality reduction and data visualization. They are commonly used in machine learning for clustering and data visualization.\n",
    "\n",
    "**How It Works:**\n",
    "- SOMs consist of a grid of nodes, where each node is connected to all other nodes.\n",
    "- Each node has a weight vector that is randomly initialized.\n",
    "- During training, the weights of the nodes are updated based on the similarity between the input data and the weight vectors.\n",
    "- The goal is to preserve the topological properties of the input data in the SOM.\n",
    "\n",
    "**Pros:**\n",
    "- Can be used for dimensionality reduction and data visualization.\n",
    "- Can be used to discover hidden relationships between observed variables.\n",
    "\n",
    "**Cons:**\n",
    "- Can be difficult to interpret.\n",
    "- Can be computationally expensive.\n",
    "\n",
    "**When to Use:**\n",
    "- Use SOMs when you want to visualize high-dimensional data in a low-dimensional space and don't have prior knowledge about the data.\n",
    "\n",
    "**Deep math behind Self-Organizing Maps:**\n",
    "- Self-Organizing Maps (SOMs) are a type of artificial neural network that is used for dimensionality reduction and data visualization. They are commonly used in machine learning for clustering and data visualization.\n",
    "\n",
    "**Code Example for Self-Organizing Maps using scikit-learn:**\n",
    "Here's a code example demonstrating Self-Organizing Maps using Python's scikit-learn library:\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Create sample data (replace with your dataset)\n",
    "X, y = make_blobs(n_samples=1000, centers=8, n_features=2, random_state=42)\n",
    "\n",
    "# Create a MiniBatchKMeans model with the desired number of clusters (K)\n",
    "model = MiniBatchKMeans(n_clusters=K)\n",
    "\n",
    "# Fit the model to the data\n",
    "model.fit(X)\n",
    "\n",
    "# Get cluster assignments\n",
    "cluster_assignments = model.labels_\n",
    "\n",
    "# Get cluster centroids\n",
    "cluster_centers = model.cluster_centers_\n",
    "\n",
    "# Get the number of clusters\n",
    "n_clusters = model.n_clusters_\n",
    "\n",
    "# Get the number of iterations\n",
    "n_iter = model.n_iter_\n",
    "\n",
    "# Get the inertia\n",
    "inertia = model.inertia_\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c1b5d5",
   "metadata": {},
   "source": [
    "# DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n",
    "\n",
    "**Theory:** DBSCAN is a density-based clustering algorithm that is used for clustering and anomaly detection. It's based on the concept of a neighborhood, which is a set of data points that are close to each other. The algorithm starts by randomly selecting a data point and finding its neighborhood. It then recursively expands the neighborhood to include all data points that are within a specified distance of the data points in the neighborhood. The algorithm then repeats this process until all data points have been assigned to a cluster or marked as noise.\n",
    "\n",
    "**How It Works:**\n",
    "- The algorithm starts by randomly selecting a data point and finding its neighborhood.\n",
    "- It then recursively expands the neighborhood to include all data points that are within a specified distance of the data points in the neighborhood.\n",
    "- The algorithm then repeats this process until all data points have been assigned to a cluster or marked as noise.\n",
    "\n",
    "**Pros:**\n",
    "- Can be used for clustering and anomaly detection.\n",
    "- Can be used to discover hidden relationships between observed variables.\n",
    "\n",
    "**Cons:**\n",
    "- Can be difficult to interpret.\n",
    "- Can be computationally expensive.\n",
    "\n",
    "**When to Use:**\n",
    "- Use DBSCAN when you want to cluster your data and don't have prior knowledge about the data.\n",
    "\n",
    "**Deep math behind DBSCAN:**\n",
    "- DBSCAN is a density-based clustering algorithm that is used for clustering and anomaly detection. It's based on the concept of a neighborhood, which is a set of data points that are close to each other. The algorithm starts by randomly selecting a data point and finding its neighborhood. It then recursively expands the neighborhood to include all data points that are within a specified distance of the data points in the neighborhood. The algorithm then repeats this process until all data points have been assigned to a cluster or marked as noise.\n",
    "\n",
    "**Code Example for DBSCAN using scikit-learn:**\n",
    "Here's a code example demonstrating DBSCAN using Python's scikit-learn library:\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Create sample data (replace with your dataset)\n",
    "X, y = make_blobs(n_samples=1000, centers=8, n_features=2, random_state=42)\n",
    "\n",
    "# Create a DBSCAN model with the desired neighborhood radius (eps) and minimum number of samples (min_samples)\n",
    "model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "\n",
    "# Fit the model to the data (X is your feature data)\n",
    "model.fit(X)\n",
    "\n",
    "# Get cluster assignments\n",
    "cluster_assignments = model.labels_\n",
    "\n",
    "# Get the number of clusters\n",
    "n_clusters = model.n_clusters_\n",
    "\n",
    "# Get the number of noise points\n",
    "n_noise = model.n_noise_\n",
    "\n",
    "# Get the core sample indices\n",
    "core_sample_indices = model.core_sample_indices_\n",
    "\n",
    "# Get the neighborhood radius\n",
    "eps = model.eps\n",
    "\n",
    "# Get the minimum number of samples\n",
    "min_samples = model.min_samples\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c6860ee",
   "metadata": {},
   "source": [
    "# Gradient Descent:\n",
    "\n",
    "**Theory:** Gradient Descent is an optimization algorithm used to minimize a loss function in machine learning and deep learning models. It iteratively adjusts the model parameters (weights) to find the minimum of the loss function, which corresponds to the best-fitting model.\n",
    "\n",
    "**Formula:** The basic update rule in Gradient Descent is as follows:\n",
    "\n",
    "$\\theta = \\theta - \\alpha \\cdot \\nabla J(\\theta)$\n",
    "\n",
    "Where:\n",
    "- $\\theta$ represents the model parameters (weights).\n",
    "- $\\alpha$ is the learning rate, which controls the step size.\n",
    "- $\\nabla J(\\theta)$ is the gradient of the loss function with respect to $\\theta$, indicating the direction of steepest ascent.\n",
    "\n",
    "Updating method loop formula:\n",
    "\n",
    "$temp_0 = \\theta_{t+1} = \\theta_{t} - \\alpha \\cdot \\nabla J(\\theta_{t})$\n",
    "\n",
    "$temp_1 = \\theta_{t+2} = \\theta_{t+1} - \\alpha \\cdot \\nabla J(\\theta_{t+1})$\n",
    "\n",
    "$\\theta_0 = temp_0$\n",
    "\n",
    "$\\theta_1 = temp_1$\n",
    "\n",
    "**Support Functions:** Gradient Descent relies on the calculation of gradients (partial derivatives) of the loss function with respect to the model parameters. Most machine learning libraries handle this computation automatically.\n",
    "\n",
    "**Some Types of Gradient Descent:**\n",
    "- **Batch Gradient Descent:** Uses the entire training dataset to compute the gradient of the loss function for each step. It's slow and computationally expensive, but it's guaranteed to converge to the global minimum for convex functions.\n",
    "$$ w_n = w_n - \\alpha \\cdot \\frac{\\partial J}{\\partial w_n}(\\hat{w}, b) $$\n",
    "\n",
    "- **Stochastic Gradient Descent (SGD):** Uses a single training example to compute the gradient of the loss function for each step. It's fast and computationally efficient, but it's sensitive to noisy data and may never converge to the global minimum.\n",
    "$$ w_n = w_n - \\alpha \\cdot \\frac{\\partial J(x_i,y_i)}{\\partial w_n} $$\n",
    "\n",
    "- **Mini-Batch Gradient Descent:** Uses a mini-batch of training examples to compute the gradient of the loss function for each step. It's a compromise between batch and stochastic gradient descent and is the most commonly used method in practice.\n",
    "\n",
    "- **Momentum:** Uses a momentum term to accelerate SGD in the relevant direction and dampens oscillations. It's faster than standard SGD and can be used to escape local minima.\n",
    "\n",
    "- **Nesterov Accelerated Gradient (NAG):** Uses a momentum term to accelerate SGD in the relevant direction and dampens oscillations. It's faster than standard SGD and can be used to escape local minima.\n",
    "$$v_prev = v$$\n",
    "$$v = momentum * v - \\alpha \\cdot \\frac{\\partial J}{\\partial w_n}(w + momentum \\cdot v_prep)$$\n",
    "$$w = w + v$$\n",
    "\n",
    "- **Adagrad:** Adapts the learning rate to the parameters, performing larger updates for infrequent and smaller updates for frequent parameters. It's well-suited for dealing with sparse data.\n",
    "$$ G = G + (\\frac{\\partial J}{\\partial w_n})^2 $$\n",
    "$$ w_n = w_n - \\alpha \\cdot \\frac{\\partial J}{\\partial w_n}(\\hat{w}, b) \\cdot \\frac{1}{\\sqrt{G + \\epsilon}} $$\n",
    "\n",
    "- **Adadelta:** Same as Adagrad, but with a moving average of the squared gradient instead of the accumulated gradient. It's well-suited for dealing with sparse data.\n",
    "$$ E[\\Delta w^2] = \\rho E[\\Delta w^2] + (1 - \\rho) \\Delta w^2 $$\n",
    "$$ w_n = w_n - \\alpha \\cdot \\frac{\\partial J}{\\partial w_n}(\\hat{w}, b) \\cdot \\frac{1}{\\sqrt{E[\\Delta w^2] + \\epsilon}} $$\n",
    "\n",
    "- **RMSprop:** Same as Adadelta, but with a moving average of the squared gradient instead of the accumulated gradient. It's well-suited for dealing with sparse data.\n",
    "$$ E[\\nabla^2] = decay_rate \\cdot E[\\nabla^2] + (1 - decay_rate) \\nabla^2 $$\n",
    "$$ w_n = w_n - \\alpha \\cdot \\frac{\\partial J}{\\partial w_n}(\\hat{w}, b) \\cdot \\frac{1}{\\sqrt{E[\\nabla^2] + \\epsilon}} $$\n",
    "\n",
    "- **Adam:** Combines the advantages of RMSprop and momentum. It's the most commonly used optimization algorithm in deep learning.\n",
    "$$m = \\beta_1 \\cdot m + (1 - \\beta_1) \\cdot \\nabla J(\\theta)$$\n",
    "$$v = \\beta_2 \\cdot v + (1 - \\beta_2) \\cdot \\nabla J(\\theta)^2$$\n",
    "$$\\hat{m} = \\frac{m}{1 - \\beta_1^t}$$\n",
    "$$\\hat{v} = \\frac{v}{1 - \\beta_2^t}$$\n",
    "$$\\theta = \\theta - \\alpha \\cdot \\frac{\\hat{m}}{\\sqrt{\\hat{v}} + \\epsilon}$$\n",
    "\n",
    "Remember the b only update after all the w update.\n",
    "\n",
    "**When Gradient Descent can be stop in process**\n",
    "- **Early Stopping:** Stop training when the validation loss starts to increase, which indicates that the model is starting to overfit the training data.\n",
    "- **Learning Rate Scheduling:** Reduce the learning rate over time to make smaller updates as the training progresses.\n",
    "- **Batch Normalization:** Normalize the inputs to each layer to ensure that the distribution of inputs remains stable as the training progresses.\n",
    "\n",
    "Now, let's see how to import and use Gradient Descent in TensorFlow, PyTorch, and scikit-learn:\n",
    "\n",
    "**Using scikit-learn (sklearn):**\n",
    "\n",
    "Scikit-learn doesn't explicitly expose Gradient Descent as an algorithm, but it's used internally in various models. For example, you can use it with the `SGDRegressor` for linear regression:\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# Create a SGDRegressor model (Stochastic Gradient Descent)\n",
    "model = SGDRegressor(learning_rate='constant', eta0=0.01, max_iter=num_epochs)\n",
    "\n",
    "# Fit the model to your data (X_train and y_train are your feature and target data)\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "**Using TensorFlow:**\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define your model (Sequential API)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=1, input_shape=(num_features,))\n",
    "])\n",
    "\n",
    "# Compile the model with a custom optimizer (e.g., SGD)\n",
    "custom_optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "model.compile(optimizer=custom_optimizer, loss='mean_squared_error')\n",
    "\n",
    "# Fit the model to your data (X_train and y_train are your feature and target data)\n",
    "model.fit(X_train, y_train, epochs=num_epochs)\n",
    "```\n",
    "\n",
    "**Using PyTorch:**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a custom Linear Regression model class\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Create an instance of the model\n",
    "model = LinearRegressionModel(input_dim=num_features)\n",
    "\n",
    "# Define loss and optimizer (e.g., SGD)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop (X_train and y_train are your feature and target data)\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "In these code examples, I've shown how to use Gradient Descent as the optimization algorithm when training a Linear Regression model in TensorFlow, PyTorch, and scikit-learn.\n",
    "\n",
    "**Also all the method above don't mention anything like advance Optimization method, so I'll complete it all by coding out below, for shorter version or using library, please research on Google**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12d06fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Coding and compare all algorithms without library\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class GradientDescentComparison:\n",
    "    def __init__(self, num_features, learning_rate=0.01, n_iterations=1000, batch_size=32):\n",
    "        self.num_features = num_features\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def generate_synthetic_data(self, num_samples=100):\n",
    "        np.random.seed(0)\n",
    "        X = 2 * np.random.rand(num_samples, self.num_features)\n",
    "        true_theta = np.random.randn(self.num_features, 1)\n",
    "        noise = np.random.randn(num_samples, 1)\n",
    "        y = X.dot(true_theta) + noise\n",
    "        return X, y, true_theta\n",
    "\n",
    "    def loss(self, X, y, theta):\n",
    "        num_samples = len(y)\n",
    "        error = X.dot(theta) - y\n",
    "        return (1 / (2 * num_samples)) * np.sum(error**2)\n",
    "\n",
    "    def gradient_descent(self, X, y, theta):\n",
    "        num_samples = len(y)\n",
    "        gradients = (1 / num_samples) * X.T.dot(X.dot(theta) - y)\n",
    "        return gradients\n",
    "\n",
    "    def run_optimization(self, optimizer):\n",
    "        X, y, true_theta = self.generate_synthetic_data()\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        theta = np.random.randn(self.num_features + 1, 1)\n",
    "        losses = []\n",
    "\n",
    "        for iteration in range(self.n_iterations):\n",
    "            indices = np.random.permutation(X.shape[0])\n",
    "            X_shuffled = X_b[indices]\n",
    "            y_shuffled = y[indices]\n",
    "\n",
    "            for i in range(0, X.shape[0], self.batch_size):\n",
    "                xi = X_shuffled[i:i + self.batch_size]\n",
    "                yi = y_shuffled[i:i + self.batch_size]\n",
    "\n",
    "                gradients = self.gradient_descent(xi, yi, theta)\n",
    "                theta = optimizer.update(theta, gradients)\n",
    "\n",
    "            loss_value = self.loss(X_b, y, theta)\n",
    "            losses.append(loss_value)\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def plot_comparison(self):\n",
    "        from collections import OrderedDict\n",
    "\n",
    "        optimizers = OrderedDict()\n",
    "        optimizers['Gradient Descent'] = GradientDescentOptimizer(self.learning_rate)\n",
    "        optimizers['Momentum'] = MomentumOptimizer(self.learning_rate, momentum=0.9)\n",
    "        optimizers['Nesterov Accelerated Gradient'] = NesterovOptimizer(self.learning_rate, momentum=0.9)\n",
    "        optimizers['RMSprop'] = RMSpropOptimizer(self.learning_rate, gamma=0.9)\n",
    "        optimizers['Adagrad'] = AdagradOptimizer(self.learning_rate, epsilon=1e-7)\n",
    "        optimizers['Adam'] = AdamOptimizer(self.learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-7)\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for optimizer_name, optimizer in optimizers.items():\n",
    "            losses = self.run_optimization(optimizer)\n",
    "            plt.plot(losses, label=optimizer_name)\n",
    "\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Gradient Descent Optimization Algorithms Comparison')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "class GradientDescentOptimizer:\n",
    "    def __init__(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def update(self, theta, gradients):\n",
    "        return theta - self.learning_rate * gradients\n",
    "\n",
    "class MomentumOptimizer:\n",
    "    def __init__(self, learning_rate, momentum):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.velocity = None\n",
    "\n",
    "    def update(self, theta, gradients):\n",
    "        if self.velocity is None:\n",
    "            self.velocity = np.zeros_like(theta)\n",
    "\n",
    "        self.velocity = self.momentum * self.velocity + self.learning_rate * gradients\n",
    "        return theta - self.velocity\n",
    "\n",
    "class NesterovOptimizer:\n",
    "    def __init__(self, learning_rate, momentum):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.velocity = None\n",
    "\n",
    "    def update(self, theta, gradients):\n",
    "        if self.velocity is None:\n",
    "            self.velocity = np.zeros_like(theta)\n",
    "\n",
    "        lookahead_theta = theta - self.momentum * self.velocity\n",
    "        lookahead_gradients = gradients\n",
    "\n",
    "        self.velocity = self.momentum * self.velocity + self.learning_rate * lookahead_gradients\n",
    "        return lookahead_theta - self.velocity\n",
    "\n",
    "class RMSpropOptimizer:\n",
    "    def __init__(self, learning_rate, gamma, epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.r = None\n",
    "\n",
    "    def update(self, theta, gradients):\n",
    "        if self.r is None:\n",
    "            self.r = np.zeros_like(theta)\n",
    "\n",
    "        self.r = self.gamma * self.r + (1 - self.gamma) * gradients**2\n",
    "        return theta - (self.learning_rate / (np.sqrt(self.r) + self.epsilon)) * gradients\n",
    "\n",
    "class AdagradOptimizer:\n",
    "    def __init__(self, learning_rate, epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.G = None\n",
    "\n",
    "    def update(self, theta, gradients):\n",
    "        if self.G is None:\n",
    "            self.G = np.zeros_like(theta)\n",
    "\n",
    "        self.G += gradients**2\n",
    "        return theta - (self.learning_rate / (np.sqrt(self.G) + self.epsilon)) * gradients\n",
    "\n",
    "class AdamOptimizer:\n",
    "    def __init__(self, learning_rate, beta1, beta2, epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.t = 0\n",
    "\n",
    "    def update(self, theta, gradients):\n",
    "        self.t += 1\n",
    "        if self.m is None:\n",
    "            self.m = np.zeros_like(theta)\n",
    "            self.v = np.zeros_like(theta)\n",
    "\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * gradients\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * (gradients**2)\n",
    "\n",
    "        m_hat = self.m / (1 - self.beta1**self.t)\n",
    "        v_hat = self.v / (1 - self.beta2**self.t)\n",
    "\n",
    "        return theta - (self.learning_rate / (np.sqrt(v_hat) + self.epsilon)) * m_hat\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    comparison = GradientDescentComparison(num_features=3, learning_rate=0.01, n_iterations=500, batch_size=32)\n",
    "    comparison.plot_comparison()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f95fea85",
   "metadata": {},
   "source": [
    "# Method in Machine Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "623c50d1",
   "metadata": {},
   "source": [
    "# Feature Engineering:\n",
    "\n",
    "Feature engineering is a crucial step in machine learning, and it involves transforming or creating new features from the existing dataset to improve model performance. While TensorFlow and PyTorch are primarily deep learning frameworks and don't have dedicated feature engineering modules, scikit-learn is a versatile library that provides tools for feature engineering. Let's discuss feature engineering and include code examples for scikit-learn:\n",
    "\n",
    "**Theory:** Feature engineering is the process of selecting, transforming, and creating features from the raw data to make it more suitable for machine learning algorithms. It helps improve model accuracy, reduce overfitting, and discover hidden patterns.\n",
    "\n",
    "**Common Techniques:**\n",
    "- **Feature Scaling:** Scaling features to have similar ranges (e.g., Min-Max scaling or Z-score normalization).\n",
    "- **One-Hot Encoding:** Converting categorical variables into binary (0/1) vectors.\n",
    "- **Feature Extraction:** Reducing dimensionality through techniques like Principal Component Analysis (PCA) or feature selection.\n",
    "- **Feature Creation:** Creating new features based on domain knowledge or mathematical operations.\n",
    "- **Handling Missing Data:** Imputing or removing missing values.\n",
    "- **Text Data Processing:** Converting text data into numerical features using techniques like TF-IDF or word embeddings.\n",
    "\n",
    "Now, let's see code examples for common feature engineering techniques using scikit-learn:\n",
    "\n",
    "**Feature Scaling:**\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler to your data (X_train is your feature data)\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transform your data\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "```\n",
    "\n",
    "**One-Hot Encoding:**\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create a OneHotEncoder object\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Fit and transform the encoder on your categorical data (X_cat is your categorical feature data)\n",
    "X_cat_encoded = encoder.fit_transform(X_cat)\n",
    "\n",
    "# If needed, you can concatenate the encoded data with the original numerical data\n",
    "X_combined = hstack((X_num, X_cat_encoded))\n",
    "```\n",
    "\n",
    "**Feature Extraction (PCA):**\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a PCA object with the desired number of components\n",
    "pca = PCA(n_components=3)\n",
    "\n",
    "# Fit and transform the PCA on your data (X is your feature data)\n",
    "X_pca = pca.fit_transform(X)\n",
    "```\n",
    "\n",
    "These are just a few examples of feature engineering techniques in scikit-learn. Depending on your dataset and problem, you may need to apply other techniques or custom transformations to create meaningful features for your machine learning models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ebd3dcd",
   "metadata": {},
   "source": [
    "# Computational Graph:\n",
    "\n",
    "**Theory:** A computational graph is a directed graph that represents the flow of data in a neural network. It's a visual representation of the mathematical operations performed by the network.\n",
    "\n",
    "**How It Works:**\n",
    "- A computational graph is a directed graph that represents the flow of data in a neural network.\n",
    "- It's a visual representation of the mathematical operations performed by the network.\n",
    "- Each node in the graph represents a mathematical operation, and each edge represents the flow of data between operations.\n",
    "- The graph is evaluated during the forward pass to compute the predicted output.\n",
    "- During the backward pass, the gradients are calculated by traversing the graph in reverse order.\n",
    "\n",
    "- Basically, a computational graph is a visual representation of the mathematical operations performed by a neural network. It's evaluated during the forward pass to compute the predicted output and during the backward pass to calculate the gradients.\n",
    "    - You can apply the chain rule to compute the gradients layer by layer, starting from the output layer and moving backward through the network. (backpropagation)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed4f522a",
   "metadata": {},
   "source": [
    "# Convex Sets:\n",
    "\n",
    "Convex sets and convex functions are important concepts in mathematics and optimization theory, and they have applications in various fields, including machine learning and convex optimization. Let's explore these concepts and their significance.\n",
    "\n",
    "**Definition:** A set $S$ in a vector space is considered convex if, for any two points $x$ and $y$ in the set $S$, the line segment connecting $x$ and $y$ lies entirely within $S$. In other words, a set is convex if, for all $x, y \\in S$ and for all $\\lambda \\in [0, 1]$, the point $\\lambda x + (1 - \\lambda) y$ is also in $S$.\n",
    "\n",
    "**Properties:**\n",
    "- Convex sets can be thought of as \"bulging outwards\" and do not have any indentations or holes.\n",
    "- Examples of convex sets include intervals, polygons, and the entire space of real numbers.\n",
    "- The intersection of convex sets is also a convex set.\n",
    "\n",
    "# Convex Functions:\n",
    "\n",
    "**Definition:** A real-valued function $f: \\mathbb{R}^n \\to \\mathbb{R}$ is considered convex if, for any two points $x$ and $y$ in the domain of $f$ and for all $\\lambda \\in [0, 1]$, the following inequality holds:\n",
    "\n",
    "$$f(\\lambda x + (1 - \\lambda) y) \\leq \\lambda f(x) + (1 - \\lambda) f(y)$$\n",
    "\n",
    "In other words, a function is convex if the line segment connecting any two points on its graph lies above the graph.\n",
    "\n",
    "**Properties:**\n",
    "- Convex functions are continuous and have derivatives almost everywhere in their domain.\n",
    "- The sum of convex functions is convex, and positive scalar multiples of convex functions are also convex.\n",
    "- Examples of convex functions include linear functions, quadratic functions, and the negative log-likelihood function used in logistic regression.\n",
    "\n",
    "**Significance in Optimization:**\n",
    "\n",
    "Convex sets and convex functions play a significant role in optimization, particularly in convex optimization. Here's why they are important:\n",
    "\n",
    "1. **Optimality:** Convex optimization problems have a unique global minimum, making it easier to find the best solution.\n",
    "\n",
    "2. **Efficiency:** Many algorithms for convex optimization problems are guaranteed to converge to the global minimum in a finite number of iterations.\n",
    "\n",
    "3. **Applications:** Convex optimization is widely used in machine learning for tasks such as linear regression, logistic regression, support vector machines, and neural network training.\n",
    "\n",
    "4. **Robustness:** Convex optimization problems are often more robust and less sensitive to initial conditions compared to non-convex problems.\n",
    "\n",
    "Overall, the concept of convexity simplifies the optimization process and is a fundamental tool in various mathematical and machine learning applications."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "693e1493",
   "metadata": {},
   "source": [
    "# Overfitting\n",
    "\n",
    "Overfitting is a common problem in machine learning and statistical modeling where a model learns the training data too well, capturing noise and random fluctuations rather than the underlying patterns or relationships in the data. This can lead to poor generalization performance, where the model performs well on the training data but poorly on unseen or new data. Let's explore overfitting in more detail:\n",
    "\n",
    "**Causes of Overfitting:**\n",
    "\n",
    "1. **Complex Models:** Models with a high number of parameters (e.g., deep neural networks) have a higher capacity to fit the training data precisely. While this can be beneficial, it also increases the risk of overfitting.\n",
    "\n",
    "2. **Insufficient Data:** When the training dataset is small, models may struggle to generalize because they have limited examples to learn from.\n",
    "\n",
    "3. **Noise in Data:** Noisy data or outliers can mislead the model, causing it to fit the noise rather than the underlying patterns.\n",
    "\n",
    "4. **Feature Overload:** Including too many features or irrelevant features in the model can lead to overfitting, as the model tries to make sense of noise or unrelated information.\n",
    "\n",
    "**Effects of Overfitting:**\n",
    "\n",
    "1. **High Training Accuracy, Low Test Accuracy:** Overfit models typically achieve high accuracy on the training data but perform poorly on new or unseen data.\n",
    "\n",
    "2. **Poor Generalization:** The model may fail to generalize to different datasets, real-world scenarios, or unseen examples.\n",
    "\n",
    "3. **Unstable Predictions:** Small changes in the input data can lead to significant variations in the model's predictions.\n",
    "\n",
    "**Methods to Prevent or Mitigate Overfitting:**\n",
    "\n",
    "1. **Cross-Validation:** Use techniques like k-fold cross-validation to assess the model's performance on different subsets of the data and detect overfitting.\n",
    "\n",
    "2. **Regularization:** Apply regularization techniques like L1 (Lasso) or L2 (Ridge) regularization to penalize large coefficients in linear models or use dropout in neural networks.\n",
    "\n",
    "3. **Simpler Models:** Choose simpler model architectures or reduce the complexity of existing models if the problem doesn't warrant complex models.\n",
    "\n",
    "4. **More Data:** Collect more data if possible, as larger datasets can help the model generalize better.\n",
    "\n",
    "5. **Feature Selection:** Carefully select and preprocess features, removing irrelevant or redundant ones.\n",
    "\n",
    "6. **Early Stopping:** Monitor the model's performance on a validation set during training and stop when the performance starts to degrade.\n",
    "\n",
    "7. **Ensemble Methods:** Use ensemble methods like bagging or boosting to combine multiple models, which can help reduce overfitting.\n",
    "\n",
    "8. **Data Augmentation:** Augment the training data by applying transformations or adding noise to create additional training examples.\n",
    "\n",
    "9. **Pruning (Decision Trees):** Prune decision trees to remove branches that do not contribute significantly to the model's performance.\n",
    "\n",
    "10. **Apply feature engineering techniques:** Feature engineering is the process of transforming or creating new features from the existing dataset to improve model performance. In example: polynomial, log, square root, etc.\n",
    "\n",
    "\n",
    "**Regularization Techniques:**\n",
    "\n",
    "**L1 Regularization (Lasso):** Adds the absolute values of the model's coefficients as a penalty term to the loss function.\n",
    "\n",
    "$Loss = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y_i})^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j|$\n",
    "\n",
    "- Large $\\lambda$ values lead to more regularization, which can reduce overfitting but may also underfit the data.\n",
    "\n",
    "**L2 Regularization (Ridge):** Adds the squares of the model's coefficients as a penalty term to the loss function.\n",
    "\n",
    "$Loss = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y_i})^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2$\n",
    "\n",
    "**Dropout (Neural Networks):** During training, randomly deactivates a fraction of neurons in each layer, preventing the network from relying too heavily on specific neurons.\n",
    "\n",
    "**Early Stopping:** Monitors the validation error during training and stops training when the validation error starts increasing, indicating overfitting.\n",
    "\n",
    "**In summary, overfitting is a common challenge in machine learning, but there are several strategies and techniques to prevent or mitigate it. It's crucial to strike a balance between model complexity and generalization performance to build models that perform well on both training and test data.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "859ecc03",
   "metadata": {},
   "source": [
    "# Underfitting\n",
    "\n",
    "Underfitting is another common problem in machine learning and statistical modeling. It occurs when a model is too simple to capture the underlying patterns in the training data. As a result, the model's performance is poor not only on the training data but also on new or unseen data. Let's explore underfitting in more detail:\n",
    "\n",
    "**Causes of Underfitting:**\n",
    "\n",
    "1. **Model Complexity:** Models that are too simple, with low capacity or insufficient parameters, may not have the capacity to capture complex relationships in the data.\n",
    "\n",
    "2. **Insufficient Training:** If the model is not trained for a sufficient number of epochs or with inadequate data, it may not have learned the underlying patterns.\n",
    "\n",
    "3. **Inappropriate Features:** If important features are missing or not appropriately preprocessed, the model may struggle to make accurate predictions.\n",
    "\n",
    "**Effects of Underfitting:**\n",
    "\n",
    "1. **Low Training and Test Accuracy:** Underfit models typically have low accuracy on both the training data and test data.\n",
    "\n",
    "2. **Poor Generalization:** The model fails to generalize to different datasets or real-world scenarios.\n",
    "\n",
    "3. **Biased Predictions:** The model may provide biased or inaccurate predictions because it lacks the capacity to capture essential information.\n",
    "\n",
    "**Methods to Prevent or Mitigate Underfitting:**\n",
    "\n",
    "1. **Increase Model Complexity:** Use more complex models with a higher number of parameters, layers, or features, especially when the data has complex relationships.\n",
    "\n",
    "2. **Collect More Data:** If possible, gather more data to provide the model with more examples to learn from.\n",
    "\n",
    "3. **Feature Engineering:** Carefully select and preprocess features, ensuring that relevant information is included and properly transformed.\n",
    "\n",
    "4. **Hyperparameter Tuning:** Experiment with different hyperparameters, such as learning rate, batch size, and the number of epochs, to optimize the model's performance.\n",
    "\n",
    "5. **Ensemble Methods:** Combine multiple models using ensemble methods like bagging or boosting to improve predictive accuracy.\n",
    "\n",
    "6. **Data Augmentation:** Augment the training data by applying transformations or adding noise to create additional training examples.\n",
    "\n",
    "7. **Use More Advanced Models:** Consider using more sophisticated models like deep neural networks or ensemble models when appropriate.\n",
    "\n",
    "8. **Cross-Validation:** Employ cross-validation techniques to assess the model's performance and identify underfitting early in the development process.\n",
    "\n",
    "**In summary, underfitting occurs when a model is too simple to capture the underlying patterns in the data. To address underfitting, it's essential to adjust the model's complexity, gather more data if possible, and carefully preprocess features. The goal is to strike the right balance between model complexity and generalization performance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9ec4ab",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique commonly used in data analysis and machine learning. PCA aims to transform high-dimensional data into a lower-dimensional representation while preserving as much of the original variance as possible. It accomplishes this by identifying a set of orthogonal axes, called principal components, along which the data exhibits the most variation.\n",
    "\n",
    "Here's a high-level overview of PCA and a code example using Python's scikit-learn library:\n",
    "\n",
    "**PCA Process:**\n",
    "\n",
    "1. **Standardization:** If the data is not already standardized (mean-centered and scaled), it's a good practice to standardize it so that all features have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "2. **Covariance Matrix:** Compute the covariance matrix of the standardized data. The covariance matrix represents the relationships and variances among different features.\n",
    "\n",
    "3. **Eigenvalue Decomposition:** Perform eigenvalue decomposition on the covariance matrix. This yields a set of eigenvalues and corresponding eigenvectors.\n",
    "\n",
    "4. **Select Principal Components:** Sort the eigenvalues in descending order and select the top \\(k\\) eigenvectors (principal components) corresponding to the largest eigenvalues, where \\(k\\) is the desired lower dimensionality.\n",
    "\n",
    "5. **Projection:** Project the original data onto the selected principal components to obtain a lower-dimensional representation.\n",
    "\n",
    "**Code Example for PCA using scikit-learn:**\n",
    "\n",
    "Here's a code example demonstrating PCA using Python's scikit-learn library:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Sample data (replace with your dataset)\n",
    "X = np.array([\n",
    "    [1.2, 2.3, 3.0],\n",
    "    [2.8, 5.1, 4.7],\n",
    "    [3.5, 6.4, 6.0],\n",
    "    [4.3, 8.1, 7.8],\n",
    "    [5.2, 9.7, 9.5]\n",
    "])\n",
    "\n",
    "# Create a PCA model with the desired number of components\n",
    "n_components = 2\n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "# Fit the PCA model to the data\n",
    "pca.fit(X)\n",
    "\n",
    "# Transform the data to the lower-dimensional space\n",
    "X_reduced = pca.transform(X)\n",
    "\n",
    "# Explained variance ratio (proportion of total variance explained by each component)\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "print(\"Reduced Data ({}-dimensional):\".format(n_components))\n",
    "print(X_reduced)\n",
    "print(\"\\nExplained Variance Ratio:\")\n",
    "print(explained_variance_ratio)\n",
    "\n",
    "```\n",
    "\n",
    "In this example, we create sample data \\(X\\) and use scikit-learn's `PCA` class to perform PCA. We specify the desired number of components (\\(k\\)) and fit the PCA model to the data. The `transform` method is used to obtain the lower-dimensional representation. Additionally, we print the explained variance ratio, which indicates the proportion of total variance explained by each principal component.\n",
    "\n",
    "PCA is commonly used for dimensionality reduction, visualization, and noise reduction in data preprocessing tasks. It can help simplify complex datasets while preserving essential information."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f0a1527",
   "metadata": {},
   "source": [
    "# Convex Optimization\n",
    "\n",
    "Convex optimization is a subfield of mathematical optimization focused on solving optimization problems where both the objective function and the constraint set are convex. Convexity is a crucial property that makes optimization problems tractable, as it ensures that local optima are also global optima. Convex optimization has wide-ranging applications in various fields, including machine learning, engineering, economics, and operations research.\n",
    "\n",
    "Here are some key aspects and concepts related to convex optimization:\n",
    "\n",
    "**1. Convex Sets:**\n",
    "   - A set is convex if, for any two points in the set, the line segment connecting them lies entirely within the set.\n",
    "   - Convex sets have a critical property: if a local minimum exists within a convex set, it is also a global minimum.\n",
    "\n",
    "**2. Convex Functions:**\n",
    "   - A function $f(x)$ is convex if, for any two points $x$ and $y$ in its domain and for all $\\lambda$ in the interval $[0, 1]$, the following inequality holds:\n",
    "   \n",
    "     $$f(\\lambda x + (1 - \\lambda) y) \\leq \\lambda f(x) + (1 - \\lambda) f(y)$$\n",
    "   \n",
    "   - Convex functions have properties like non-negative second derivatives, non-decreasing gradients, and their epigraphs (the region above the function's graph) are convex sets.\n",
    "\n",
    "**3. Convex Optimization Problems:**\n",
    "   - A convex optimization problem is defined as follows:\n",
    "   \n",
    "     $$\\text{Minimize } f(x) \\text{ subject to } g_i(x) \\leq 0 \\text{ for } i = 1, 2, \\ldots, m$$\n",
    "   \n",
    "   - Here, $f(x)$ is a convex objective function, and $g_i(x)$ are convex inequality constraints.\n",
    "   - Convex optimization problems can also include equality constraints.\n",
    "\n",
    "**4. Algorithms for Convex Optimization:**\n",
    "   - There are various algorithms for solving convex optimization problems, including gradient descent, interior-point methods, and sub gradient methods.\n",
    "   - These algorithms exploit the structure of convexity to efficiently find global optima.\n",
    "   - Interior-point methods, in particular, are popular for solving large-scale convex problems.\n",
    "\n",
    "**5. Applications:**\n",
    "   - Convex optimization has applications in diverse areas, such as linear programming (LP), quadratic programming (QP), support vector machines (SVMs), portfolio optimization, and robust control.\n",
    "   - In machine learning, convex optimization is used for training linear models (e.g., linear regression and logistic regression) and support vector machines.\n",
    "\n",
    "**6. Duality:**\n",
    "   - Convex optimization problems often have associated dual problems, which provide a lower bound on the optimal value of the original problem.\n",
    "   - Strong duality holds for convex problems, meaning that the optimal value of the dual problem is equal to the optimal value of the primal problem.\n",
    "\n",
    "**7. Computational Complexity:**\n",
    "   - Convex optimization problems are generally efficiently solvable in polynomial time.\n",
    "   - This computational tractability distinguishes convex optimization from non-convex optimization, where finding global optima can be NP-hard.\n",
    "\n",
    "Convex optimization is a powerful framework for solving a wide range of real-world problems. Its applications extend beyond optimization theory to fields where optimization plays a crucial role in decision-making and resource allocation. Researchers and practitioners leverage convexity to design efficient algorithms and models for solving complex problems."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2cc70df1",
   "metadata": {},
   "source": [
    "# Duality\n",
    "\n",
    "Duality is a fundamental concept in optimization theory that is particularly important in convex optimization. It provides a powerful tool for understanding and solving optimization problems, especially when dealing with convex optimization problems. Duality introduces the idea of creating a \"dual\" problem associated with the original \"primal\" problem, which can provide valuable insights and solutions. Let's explore the concept of duality in more detail:\n",
    "\n",
    "**Primal Problem:**\n",
    "- The original optimization problem, which we aim to solve, is referred to as the \"primal\" problem.\n",
    "- In the primal problem, we have an objective function to minimize or maximize, subject to certain constraints.\n",
    "\n",
    "**Dual Problem:**\n",
    "- The \"dual\" problem is a related optimization problem that is derived from the primal problem.\n",
    "- It is constructed in such a way that it provides a lower bound (in minimization) or an upper bound (in maximization) on the optimal value of the primal problem.\n",
    "- The dual problem involves introducing Lagrange multipliers or dual variables for each constraint in the primal problem.\n",
    "\n",
    "**Lagrangian and Lagrange Dual Function:**\n",
    "- The Lagrangian is a function that combines the objective function of the primal problem with the constraints, using Lagrange multipliers.\n",
    "- The Lagrange dual function is the infimum (the greatest lower bound) of the Lagrangian over all feasible points.\n",
    "\n",
    "**Weak Duality:**\n",
    "- Weak duality is a fundamental property that states that the optimal value of the dual problem is always a lower bound on the optimal value of the primal problem.\n",
    "- Mathematically, if \\(p^*\\) is the optimal value of the primal problem, and \\(d^*\\) is the optimal value of the dual problem, then \\(d^* \\leq p^*\\).\n",
    "\n",
    "**Strong Duality:**\n",
    "- Strong duality is a more specific property that holds when the primal and dual problems have the same optimal value.\n",
    "- In other words, if \\(p^* = d^*\\), then strong duality is satisfied.\n",
    "- Strong duality is a special property that holds for convex optimization problems but may not hold for non-convex problems.\n",
    "\n",
    "**Karush-Kuhn-Tucker (KKT) Conditions:**\n",
    "- The KKT conditions are a set of necessary conditions for optimality in both the primal and dual problems.\n",
    "- These conditions include the gradient of the Lagrangian being zero, complementary slackness, and feasibility constraints.\n",
    "- Satisfying the KKT conditions is a requirement for optimality in convex optimization.\n",
    "\n",
    "**Applications of Duality:**\n",
    "- Duality is a powerful tool for sensitivity analysis in optimization, allowing us to understand how changes in problem parameters affect the optimal solution.\n",
    "- It is used in various optimization algorithms, including interior-point methods for convex optimization.\n",
    "- Duality is also applied in economics, game theory, and engineering for modeling and solving complex problems.\n",
    "\n",
    "In summary, duality is a central concept in optimization that allows us to derive a related dual problem from a primal problem. It provides valuable insights into the nature of optimization problems, establishes bounds on solutions, and plays a crucial role in solving and analyzing convex optimization problems. Strong duality is a key property that distinguishes convex optimization from non-convex optimization, where finding the global optimum can be more challenging."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9eec12a5",
   "metadata": {},
   "source": [
    "# Neighborhood-Based Collaborative Filtering\n",
    "\n",
    "Neighborhood-Based Collaborative Filtering (CF) is a popular recommendation algorithm used in recommendation systems. It's based on the idea that users who have interacted with or rated items similarly in the past will continue to do so in the future. Collaborative Filtering methods make personalized recommendations by leveraging the preferences and behaviors of similar users or items. Neighborhood-Based CF specifically focuses on finding these \"neighborhoods\" of users or items to make recommendations. Here's an overview of Neighborhood-Based Collaborative Filtering and a code example using Python's scikit-learn library:\n",
    "\n",
    "**Types of Neighborhood-Based Collaborative Filtering:**\n",
    "\n",
    "1. **User-Based CF:** This approach finds users who are similar to the target user and recommends items that those similar users have liked or interacted with. It relies on user-item interactions and is based on the assumption that users who have similar preferences will like similar items.\n",
    "\n",
    "2. **Item-Based CF:** In this approach, the similarity between items is calculated based on the ratings or interactions of users. When a user expresses interest in an item, the algorithm recommends other items that are similar to the ones the user has already interacted with.\n",
    "\n",
    "**Code Example for User-Based Collaborative Filtering with scikit-learn:**\n",
    "\n",
    "Here's a simplified example using scikit-learn's Nearest Neighbors module for User-Based Collaborative Filtering. In practice, more advanced techniques and data preprocessing would be used for real-world recommendation systems.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Sample user-item interaction matrix (user rows, item columns)\n",
    "# Each row represents a user's ratings on items (e.g., movies)\n",
    "user_item_matrix = np.array([\n",
    "    [5, 4, 0, 0, 1, 0],\n",
    "    [0, 0, 5, 0, 2, 3],\n",
    "    [1, 0, 0, 4, 0, 5],\n",
    "    [0, 2, 3, 0, 0, 4],\n",
    "    [4, 0, 0, 3, 5, 0],\n",
    "])\n",
    "\n",
    "# Define the number of neighbors to consider\n",
    "k_neighbors = 3\n",
    "\n",
    "# Create a Nearest Neighbors model\n",
    "model = NearestNeighbors(n_neighbors=k_neighbors, metric='cosine', algorithm='brute')\n",
    "model.fit(user_item_matrix)\n",
    "\n",
    "# Query a user's nearest neighbors (e.g., user 0)\n",
    "query_user = user_item_matrix[0].reshape(1, -1)  # Reshape to make it a 2D array\n",
    "distances, indices = model.kneighbors(query_user)\n",
    "\n",
    "# Recommend items based on the nearest neighbors\n",
    "recommended_items = np.sum(user_item_matrix[indices], axis=0)\n",
    "print(\"Recommended items:\", recommended_items)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "In this example, we create a simplified user-item interaction matrix and use scikit-learn's Nearest Neighbors module to find the nearest neighbors (users) for a target user (user 0) based on cosine similarity. We then recommend items based on the ratings of the nearest neighbors.\n",
    "\n",
    "In a real-world application, you would typically work with larger datasets and more sophisticated techniques for neighborhood-based collaborative filtering. Additionally, you may need to handle sparsity and cold-start problems."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6df9a2f",
   "metadata": {},
   "source": [
    "# Matrix Factorization Collaborative Filtering\n",
    "\n",
    "Matrix Factorization Collaborative Filtering is a recommendation system technique that factorizes a user-item interaction matrix into two lower-dimensional matrices: one representing users and the other representing items. The goal is to approximate the original matrix by multiplying these two lower-dimensional matrices. This approach can capture latent factors or features that influence user-item interactions and can make personalized recommendations based on these learned latent factors. Matrix Factorization is widely used in recommendation systems and has been popularized by algorithms like Singular Value Decomposition (SVD) and its variations, including Matrix Factorization with Stochastic Gradient Descent (SGD). Below is a code example for Matrix Factorization Collaborative Filtering using Python's scikit-learn library:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Sample user-item interaction matrix (user rows, item columns)\n",
    "# Each row represents a user's ratings on items (e.g., movies)\n",
    "user_item_matrix = np.array([\n",
    "    [5, 4, 0, 0, 1, 0],\n",
    "    [0, 0, 5, 0, 2, 3],\n",
    "    [1, 0, 0, 4, 0, 5],\n",
    "    [0, 2, 3, 0, 0, 4],\n",
    "    [4, 0, 0, 3, 5, 0],\n",
    "])\n",
    "\n",
    "# Number of latent factors to learn\n",
    "n_latent_factors = 2\n",
    "\n",
    "# Create a Truncated SVD (Matrix Factorization) model\n",
    "model = TruncatedSVD(n_components=n_latent_factors)\n",
    "\n",
    "# Fit the model to the user-item matrix\n",
    "model.fit(user_item_matrix)\n",
    "\n",
    "# Transform the user-item matrix to a lower-dimensional approximation\n",
    "user_item_approx = model.transform(user_item_matrix)\n",
    "\n",
    "# Reconstruct the original user-item matrix from the lower-dimensional approximation\n",
    "user_item_predicted = np.dot(user_item_approx, model.components_)\n",
    "\n",
    "# Calculate root mean squared error (RMSE) between the original and reconstructed matrix\n",
    "rmse = sqrt(mean_squared_error(user_item_matrix, user_item_predicted))\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "\n",
    "# Make recommendations based on the reconstructed matrix\n",
    "# For example, recommend items with the highest predicted ratings for a user\n",
    "user_idx = 0  # Index of the target user\n",
    "recommended_items = np.argsort(user_item_predicted[user_idx])[::-1]  # Sort in descending order\n",
    "print(\"Recommended items:\", recommended_items)\n",
    "\n",
    "```\n",
    "\n",
    "In this example, we use scikit-learn's TruncatedSVD, which is an implementation of Matrix Factorization (SVD), to learn latent factors from the user-item interaction matrix. We fit the model to the data, transform the data into a lower-dimensional approximation, and then reconstruct the original matrix. The RMSE is calculated to measure the quality of the approximation. Finally, we can make recommendations based on the reconstructed matrix by sorting items based on predicted ratings for a target user.\n",
    "\n",
    "In practice, real-world datasets are often sparse, and Matrix Factorization techniques are used to handle these sparse matrices. Various optimization algorithms and regularization techniques are applied to improve the accuracy of recommendations and handle scalability issues."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cdcde9d5",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition\n",
    "\n",
    "Singular Value Decomposition (SVD) is a matrix factorization technique used in linear algebra and data analysis. It decomposes a matrix into three other matrices, which can help extract hidden patterns or features from data. SVD is widely used in various applications, including recommendation systems, image compression, and dimensionality reduction. Here's an overview of SVD and a code example using Python's NumPy library:\n",
    "\n",
    "**SVD Decomposition:**\n",
    "Given a matrix $A$, SVD decomposes it into three matrices:\n",
    "\n",
    "1. $U$ (left singular vectors): A unitary matrix containing information about the relationships between rows in the original matrix.\n",
    "2. $S$ (singular values): A diagonal matrix containing singular values, which represent the strength of each singular vector's contribution to the original matrix.\n",
    "3. $V^T$ (right singular vectors): The transpose of a unitary matrix containing information about the relationships between columns in the original matrix.\n",
    "\n",
    "Mathematically, the decomposition is expressed as $A = U \\cdot S \\cdot V^T$.\n",
    "\n",
    "**Code Example for SVD using NumPy:**\n",
    "\n",
    "Here's a code example demonstrating SVD using Python's NumPy library:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample matrix\n",
    "A = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]\n",
    "])\n",
    "\n",
    "# Perform SVD decomposition\n",
    "U, S, VT = np.linalg.svd(A)\n",
    "\n",
    "# U, S, and VT are the left singular vectors, singular values, and right singular vectors, respectively\n",
    "\n",
    "# Print the matrices\n",
    "print(\"U (left singular vectors):\")\n",
    "print(U)\n",
    "print(\"\\nS (singular values):\")\n",
    "print(S)\n",
    "print(\"\\nVT (right singular vectors):\")\n",
    "print(VT)\n",
    "\n",
    "# Reconstruct the original matrix using the decomposition\n",
    "reconstructed_A = np.dot(U, np.dot(np.diag(S), VT))\n",
    "\n",
    "print(\"\\nReconstructed Matrix:\")\n",
    "print(reconstructed_A)\n",
    "\n",
    "```\n",
    "\n",
    "In this example, we create a sample matrix \\(A\\) and perform SVD decomposition using NumPy's `np.linalg.svd` function. The matrices \\(U\\), \\(S\\), and \\(VT\\) are printed, and then the original matrix is reconstructed by multiplying these matrices.\n",
    "\n",
    "SVD is a powerful technique for dimensionality reduction and feature extraction. In recommendation systems, for example, it can be used to reduce the dimensionality of user-item interaction data, making it easier to find similar users or items."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba08c351",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis\n",
    "\n",
    "Linear Discriminant Analysis (LDA) is a dimensionality reduction and classification technique used in the field of machine learning and pattern recognition. LDA is particularly useful for solving classification problems and is closely related to Principal Component Analysis (PCA). However, while PCA focuses on maximizing variance, LDA aims to maximize the separation between different classes in the data. It does so by finding a linear combination of features (discriminants) that best separates the classes while reducing the dimensionality of the data. Here's an overview of LDA and a code example using Python's scikit-learn library:\n",
    "\n",
    "**Key Concepts in LDA:**\n",
    "\n",
    "1. **Between-Class Scatter and Within-Class Scatter:** LDA aims to maximize the ratio of the between-class scatter to the within-class scatter. This ratio quantifies how well the classes are separated in the transformed space.\n",
    "\n",
    "2. **Eigenvalue Decomposition:** LDA involves eigenvalue decomposition of the scatter matrices to find the discriminant vectors (linear combinations of features) that maximize the objective function.\n",
    "\n",
    "3. **Number of Components:** Like PCA, you can specify the number of discriminant components you want to retain, effectively reducing the dimensionality of the data.\n",
    "\n",
    "**Code Example for LDA using scikit-learn:**\n",
    "\n",
    "Here's a code example demonstrating LDA using Python's scikit-learn library:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# Sample data (replace with your dataset)\n",
    "X = np.array([\n",
    "    [2.1, 3.2],\n",
    "    [1.9, 2.5],\n",
    "    [3.2, 4.2],\n",
    "    [2.4, 3.5],\n",
    "    [2.8, 2.7]\n",
    "])\n",
    "\n",
    "# Corresponding class labels\n",
    "y = np.array([0, 0, 1, 1, 2])\n",
    "\n",
    "# Create an LDA model with the desired number of components\n",
    "n_components = 2\n",
    "lda = LinearDiscriminantAnalysis(n_components=n_components)\n",
    "\n",
    "# Fit the LDA model to the data and transform it\n",
    "X_lda = lda.fit_transform(X, y)\n",
    "\n",
    "# View the transformed data\n",
    "print(\"Transformed Data ({}-dimensional):\".format(n_components))\n",
    "print(X_lda)\n",
    "```\n",
    "\n",
    "In this example, we create sample data \\(X\\) and corresponding class labels \\(y\\). We use scikit-learn's `LinearDiscriminantAnalysis` class to perform LDA. We specify the desired number of components (\\(k\\)) and fit the LDA model to the data using the `fit_transform` method. The resulting transformed data (\\(X_{\\text{lda}}\\)) contains \\(k\\) discriminant components.\n",
    "\n",
    "LDA is useful when you want to reduce the dimensionality of your data while preserving class separability, making it a valuable tool for classification tasks. It can help improve the performance of classification algorithms by reducing the risk of overfitting and increasing the separability of different classes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2cc99aa5",
   "metadata": {},
   "source": [
    "# Maximum Likelihood and Maximum A Posteriori\n",
    "\n",
    "Maximum Likelihood (ML) and Maximum A Posteriori (MAP) are two common estimation techniques used in statistics and machine learning to find the most likely values of parameters in a statistical model. They are often used in the context of parameter estimation for probability distributions. Here's an overview of both concepts:\n",
    "\n",
    "**Maximum Likelihood (ML):**\n",
    "Maximum Likelihood is an estimation method that seeks to find the set of parameter values that maximizes the likelihood function of the observed data. In other words, it finds the parameter values that make the observed data most probable under the assumed statistical model.\n",
    "\n",
    "Mathematically, for a parameter vector $\\theta$ and observed data $X$, the likelihood function is denoted as $L(\\theta | X)$, and ML estimation aims to find the value of $\\theta$ that maximizes this likelihood:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_{\\text{ML}} = \\arg \\max_\\theta L(\\theta | X)\n",
    "$$\n",
    "\n",
    "ML estimation is widely used and has desirable properties, such as consistency and asymptotic efficiency. However, it does not take into account any prior information or beliefs about the parameter values.\n",
    "\n",
    "**Maximum A Posteriori (MAP):**\n",
    "Maximum A Posteriori estimation is an extension of ML that incorporates prior information or beliefs about the parameter values. It aims to find the set of parameter values that maximizes the posterior probability distribution of the parameters given the observed data. In other words, it combines the likelihood of the data and the prior information to estimate the most probable parameter values.\n",
    "\n",
    "Mathematically, for a parameter vector $\\theta$, observed data $X$, and a prior distribution $P(\\theta)$, the posterior probability distribution is denoted as $P(\\theta | X)$, and MAP estimation finds the value of $\\theta$ that maximizes this posterior:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_{\\text{MAP}} = \\arg \\max_\\theta P(\\theta | X) = \\arg \\max_\\theta \\frac{P(X | \\theta) \\cdot P(\\theta)}{P(X)}\n",
    "$$\n",
    "\n",
    "MAP estimation is particularly useful when you have prior information about the parameter values or when you want to regularize the estimation by incorporating prior beliefs. It strikes a balance between the likelihood of the data and the prior, which can be especially helpful in cases with limited data.\n",
    "\n",
    "In summary, Maximum Likelihood (ML) and Maximum A Posteriori (MAP) are estimation techniques used to find the most likely values of parameters in statistical models. ML focuses solely on the likelihood of the data, while MAP combines the likelihood with prior information to provide a more informed estimate. The choice between ML and MAP depends on the availability of prior information and the specific goals of the estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668369f9",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "- Hyperparameter tuning is the process of finding the best hyperparameters for a machine learning model. It is an important step in the machine learning pipeline, since the performance of the model can be significantly improved by tuning the hyperparameters.\n",
    "\n",
    "## Grid Search\n",
    "**Grid Search:**\n",
    "- Grid search is a technique for hyperparameter tuning that works by searching over all possible combinations of hyperparameter values, evaluating each combination using cross-validation, and selecting the best combination of hyperparameter values.\n",
    "- To use this, at least you must know basic parameter of each model you are using. For example, if you are using Linear Regression, you must know the parameter of Linear Regression, such as fit_intercept, normalize, copy_X, etc.\n",
    "\n",
    "**Code Example for Grid Search using scikit-learn:**\n",
    "- We will work with Linear Regression model to see how Grid Search works\n",
    "\n",
    "```python\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Sample data\n",
    "X = [[0, 0], [1, 1], [2, 2]]\n",
    "y = [0, 1, 2]\n",
    "\n",
    "# Create a Linear Regression model\n",
    "reg = linear_model.LinearRegression()\n",
    "\n",
    "# Create a dictionary of hyperparameters to search over\n",
    "hyperparameters = {'fit_intercept': [True, False], 'normalize': [True, False], 'copy_X': [True, False]}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "clf = GridSearchCV(reg, hyperparameters, cv=10)\n",
    "\n",
    "# Fit the model to the data\n",
    "best_model = clf.fit(X, y)\n",
    "\n",
    "# View the best hyperparameters\n",
    "print('Best fit intercept:', best_model.best_estimator_.get_params()['fit_intercept'])\n",
    "print('Best normalize:', best_model.best_estimator_.get_params()['normalize'])\n",
    "print('Best copy X:', best_model.best_estimator_.get_params()['copy_X'])\n",
    "```\n",
    "\n",
    "## Random Search\n",
    "**Random Search:**\n",
    "- Random search is a technique for hyperparameter tuning that works by randomly sampling hyperparameter values from a specified distribution, evaluating each combination using cross-validation, and selecting the best combination of hyperparameter values.\n",
    "\n",
    "**How it work:**\n",
    "The following steps describe the algorithm of random search in machine learning.\n",
    "- Set x to a random place in the search space.\n",
    "- Repeat until a termination requirement, such as multiple iterations completed or appropriate fitness achieved, is met:\n",
    "    - Take a new position y from the hypersphere with a given radius around the current position x.\n",
    "    - If f(y) < f(x), then assign x = y as the new position.\n",
    "\n",
    "**Code Example for Random Search using scikit-learn:**\n",
    "- We will work with Decision Tree model to see how Random Search works\n",
    "\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import train_test_split\n",
    "\n",
    "# Load the data\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Create a Decision Tree model\n",
    "dtclf = DecisionTreeClassifier()\n",
    "dtclf.fit(X_train, y_train)\n",
    "score = dtclf.predict(X_test)\n",
    "\n",
    "# Create a dictionary of hyperparameters to search over\n",
    "hyperparameters = {'max_depth': [1, 2, 3, 4, 5], 'min_samples_leaf': [1, 2, 3, 4, 5], 'min_samples_split': [2, 3, 4, 5, 6]}\n",
    "\n",
    "# Create a RandomizedSearchCV object\n",
    "rs = RandomizedSearchCV(dtclf, param_distributions=hyperparameters,\n",
    "                        n_iter=10, scoring=\"accuracy\", random_state=0,\n",
    "                        n_jobs=-1, cv=10, return_train_score=True)\n",
    "\n",
    "rs.fit(X_train, y_train)\n",
    "\n",
    "# View the best hyperparameters\n",
    "print('Best max_depth:', rs.best_estimator_.get_params()['max_depth'])\n",
    "print('Best min_samples_leaf:', rs.best_estimator_.get_params()['min_samples_leaf'])\n",
    "print('Best min_samples_split:', rs.best_estimator_.get_params()['min_samples_split'])\n",
    "print('Mean cross-validated training accuracy score:', rs.best_score_)\n",
    "rs.best_estimator_.fit(X_train, y_train)\n",
    "score2 = rs.best_estimator_.predict(X_test)\n",
    "\n",
    "# Compare the two models\n",
    "print('Accuracy score with default hyperparameters:', score)\n",
    "print('Accuracy score with best hyperparameters:', score2)\n",
    "```\n",
    "\n",
    "## Bayesian Optimization\n",
    "- Different with Grid Search and Random Search, Bayesian Optimization is a technique for hyperparameter tuning that works by constructing a probabilistic model of the objective function, and then using this model to select the next hyperparameter values to evaluate. It is used to improve the performance of machine learning models when the dataset is imbalanced.\n",
    "\n",
    "**Why Bayesian Optimization:**\n",
    "- Random Search and Grid Search typically work efficiently when the number of hyperparameters is small. However, when the number of hyperparameters is large, these techniques can become computationally expensive. Bayesian Optimization is a technique that can be used to overcome this problem.\n",
    "\n",
    "**Deep math behind Bayesian Optimization:**\n",
    "- BO optimizes the objective function based on machine learning. Write as formula:\n",
    "$ max_{x \\in X} f(x) $\n",
    "- Where $f(x)$ is the objective function and $x$ is the hyperparameter. The goal of BO is to find the best hyperparameter $x$ that maximize the objective function $f(x)$.\n",
    "\n",
    "**Code Example for Bayesian Optimization using scikit-optimize:**\n",
    "- We will work with Decision Tree model to see how Bayesian Optimization works\n",
    "\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import train_test_split\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "# Load the data\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Create a Decision Tree model\n",
    "dtclf = DecisionTreeClassifier()\n",
    "dtclf.fit(X_train, y_train)\n",
    "score = dtclf.predict(X_test)\n",
    "\n",
    "# Create a dictionary of hyperparameters to search over\n",
    "hyperparameters = {'max_depth': Integer(1, 5), 'min_samples_leaf': Integer(1, 5), 'min_samples_split': Integer(2, 6)}\n",
    "\n",
    "# Create a BayesSearchCV object\n",
    "bs = BayesSearchCV(dtclf, hyperparameters, scoring=\"accuracy\", n_iter=10, cv=10, refit=True)\n",
    "\n",
    "bs.fit(X_train, y_train)\n",
    "\n",
    "# View the best hyperparameters\n",
    "print('Best max_depth:', bs.best_estimator_.get_params()['max_depth'])\n",
    "print('Best min_samples_leaf:', bs.best_estimator_.get_params()['min_samples_leaf'])\n",
    "print('Best min_samples_split:', bs.best_estimator_.get_params()['min_samples_split'])\n",
    "print('Mean cross-validated training accuracy score:', bs.best_score_)\n",
    "bs.best_estimator_.fit(X_train, y_train)\n",
    "score2 = bs.best_estimator_.predict(X_test)\n",
    "\n",
    "# Compare the two models\n",
    "print('Accuracy score with default hyperparameters:', score)\n",
    "print('Accuracy score with best hyperparameters:', score2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6b3f3c",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "- A pipeline is a sequence of data processing components. It is used to automate the machine learning workflow by combining multiple steps into a single pipeline. Pipelines are commonly used in machine learning to automate the process of training and evaluating machine learning models.\n",
    "\n",
    "**All pipeline parameters to fit in:**\n",
    "- **steps:** A list of (name, transform) tuples, where name is the name of the step and transform is the transform object.\n",
    "- **memory:** Used to cache the fitted transformers of the pipeline.\n",
    "- **verbose:** If True, the time elapsed while fitting each step will be printed as it is completed.\n",
    "- **return:** The fitted estimator.\n",
    "\n",
    "- When your code has many steps and function calls, it can be difficult to keep track of what is happening. A pipeline is a sequence of data processing components that are chained together. Each component takes the output of the previous component as its input, and produces an output that is passed to the next component. Pipelines are commonly used in machine learning to automate the process of training and evaluating machine learning models.\n",
    "\n",
    "**Code Example for Pipeline using scikit-learn:**\n",
    "- We will work with Linear Regression model to see how Pipeline works\n",
    "\n",
    "```python\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn import pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer # This one for custom function\n",
    "\n",
    "# Sample data\n",
    "X = [[0, 0], [1, 1], [2, 2]]\n",
    "y = [0, 1, 2]\n",
    "\n",
    "def remove_first_column(X): # Custom function\n",
    "    return X[:, 1:]\n",
    "\n",
    "\n",
    "# Create a pipeline object\n",
    "pipe = pipeline.Pipeline([\n",
    "    ('remove_first_column', FunctionTransformer(remove_first_column)),\n",
    "    ('scaler', preprocessing.StandardScaler()),\n",
    "    ('reg', linear_model.LinearRegression())\n",
    "])\n",
    "\n",
    "# Fit the model to the data\n",
    "pipe.fit(X, y)\n",
    "\n",
    "# View the coefficients of the model\n",
    "print(pipe['reg'].coef_)\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8cc8ba2",
   "metadata": {},
   "source": [
    "# All Accuracy Metrics and tricks to calculate them\n",
    "\n",
    "**Truth Table:**\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th>Actual Class</th>\n",
    "    <th>Predicted Class</th>\n",
    "    <th>Outcome</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Positive</td>\n",
    "    <td>Positive</td>\n",
    "    <td>True Positive (TP)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Positive</td>\n",
    "    <td>Negative</td>\n",
    "    <td>False Negative (FN)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Negative</td>\n",
    "    <td>Positive</td>\n",
    "    <td>False Positive (FP)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Negative</td>\n",
    "    <td>Negative</td>\n",
    "    <td>True Negative (TN)</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Or In easy way:\n",
    "<table>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th>Positive</th>\n",
    "    <th>Negative</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Positive</th>\n",
    "    <td>True Positive (TP)</td>\n",
    "    <td>False Negative (FN)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Negative</th>\n",
    "    <td>False Positive (FP)</td>\n",
    "    <td>True Negative (TN)</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "**Accuracy:**\n",
    "- Accuracy is the most intuitive performance measure and it is simply a ratio of correctly predicted observation to the total observations.\n",
    "$$ Accuracy = \\frac{TP+TN}{TP+FP+FN+TN} $$\n",
    "\n",
    "**Precision:**\n",
    "- Precision is the ratio of correctly predicted positive observations to the total predicted positive observations.\n",
    "$$Precision = \\frac{TP}{TP+FP}$$\n",
    "\n",
    "**Recall (Sensitivity):**\n",
    "- Recall is the ratio of correctly predicted positive observations to the all observations in actual class.\n",
    "  - True Positive Rate is the ratio of correctly predicted positive observations to the all observations in actual class.\n",
    "  $$Recall = \\frac{TP}{TP+FN}$$\n",
    "  - False Negative Rate is the ratio of incorrectly predicted negative observations to the all observations in actual class.\n",
    "  $$False Negative Rate = \\frac{FN}{FN+TN} = 1 - Specificity$$\n",
    "\n",
    "**Specificity:**\n",
    "- Specificity is the ratio of correctly predicted negative observations to the all observations in actual class.\n",
    "$$Specificity = \\frac{TN}{TN+FP}$$\n",
    "\n",
    "**Main Accuracy Metrics:**\n",
    "- F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account.\n",
    "$$ F1 Score = 2 * \\frac{Precision * Recall}{Precision + Recall} $$\n",
    "\n",
    "- Support is the number of actual occurrences of the class in the specified dataset.\n",
    "- Macro average is the average performance of each class.\n",
    "$$ Macro Average = (Precision_1 + Precision_2 + Precision_3 + ... + Precision_n) / n $$\n",
    "\n",
    "**ROC, AUC, and Gini:**\n",
    "- ROC curve (Receiver Operating Characteristic curve) is a plot of the true positive rate against the false positive rate.\n",
    "- AUC (Area Under the ROC Curve) score is the measure of the ability of a classifier to distinguish between classes and is used as a summary of the ROC curve.\n",
    "- Gini coefficient is a measure of statistical dispersion intended to represent the income or wealth distribution of a nation's residents, and is the most commonly used measure of inequality.\n",
    "$$ Gini = 2 * AUC - 1 $$\n",
    "\n",
    "**Balanced Accuracy:**\n",
    "- Balanced accuracy is the average of recall obtained on each class.\n",
    "$$ BA = TP/(TP+FN) + TN/(TN+FP) / 2 $$\n",
    "\n",
    "**Mathew Correlation Coefficient:**\n",
    "- The Matthews correlation coefficient is used in machine learning as a measure of the quality of binary and multiclass classifications.\n",
    "$$ MCC = (TP * TN) - (FP * FN) / \\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)} $$\n",
    "\n",
    "\n",
    "**Code Example for Accuracy Metrics using scikit-learn:**\n",
    "\n",
    "```python\n",
    "from sklearn import metrics\n",
    "\n",
    "# Sample labels (actual values)\n",
    "y_true = [1, 1, 0, 1, 0, 0, 1, 0, 0, 0]\n",
    "\n",
    "# Sample predictions (predicted values)\n",
    "y_pred = [1, 0, 0, 1, 0, 0, 1, 1, 1, 0]\n",
    "\n",
    "# Accuracy\n",
    "accuracy = metrics.accuracy_score(y_true, y_pred) # (TP + TN) / (TP + TN + FP + FN)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Precision\n",
    "precision = metrics.precision_score(y_true, y_pred) # TP / (TP + FP)\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Recall\n",
    "recall = metrics.recall_score(y_true, y_pred) # TP / (TP + FN)\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# F1 Score\n",
    "f1_score = metrics.f1_score(y_true, y_pred) # 2 * (Precision * Recall) / (Precision + Recall)\n",
    "print(\"F1 Score:\", f1_score)\n",
    "\n",
    "# Classification Report\n",
    "print(metrics.classification_report(y_true, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(metrics.confusion_matrix(y_true, y_pred))\n",
    "```\n",
    "\n",
    "In this example, we use scikit-learn's `metrics` module to calculate various accuracy metrics. We create sample labels and predictions and then calculate the accuracy, precision, recall, F1 score, classification report, and confusion matrix.\n",
    "\n",
    "Please note that while scikit-learn provides a convenient and powerful implementation of these metrics, TensorFlow and PyTorch are more focused on neural networks and deep learning. If you specifically need these metrics, scikit-learn is the recommended library for this purpose in Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "970d2f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Coding ROC curve\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Data for ROC curve\n",
    "x, y = datasets.make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build Logistic Regression\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train, y_train)\n",
    "y_pred_proba = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "# Plot ROC curve\n",
    "def plot_roc_curve(fpr, tpr):\n",
    "    plt.plot(fpr, tpr, color='orange', label='ROC')\n",
    "    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--', label='Random')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "# Calculate ROC curve\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "plot_roc_curve(fpr, tpr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "384aeb7e",
   "metadata": {},
   "source": [
    "# All normalization techniques and tricks to calculate them\n",
    "\n",
    "**Normalization:**\n",
    "- Normalization is a technique often applied as part of data preparation for machine learning. The goal of normalization is to change the values of numeric columns in the dataset to a common scale, without distorting differences in the ranges of values or losing information.\n",
    "- Normalization is also required for some algorithms to model the data correctly.\n",
    "\n",
    "**Normalization Techniques:**\n",
    "- **Min-Max Normalization:** Min-max normalization scales the values in the range [0, 1]. This is done by subtracting the minimum value in the feature and then dividing the result by the difference between the original maximum and original minimum value in the feature.\n",
    "\n",
    "$ x_{norm} = \\frac{x - x_{min}}{x_{max} - x_{min}} $\n",
    "\n",
    "- **Z-Score Normalization:** Z-score normalization scales the values so that the mean is 0 and the standard deviation is 1. This is done by subtracting the mean value from each data point and then dividing the result by the standard deviation of the feature.\n",
    "\n",
    "$ x_{norm} = \\frac{x - \\mu}{\\sigma} $\n",
    "\n",
    "- **Log Transformation:** Log transformation is used to transform skewed data to approximately conform to normality. It can be used to normalize data that contains negative values, as long as the data is shifted by a constant value (e.g., by adding the absolute value of the most negative value in the data).\n",
    "\n",
    "$ x_{norm} = log(x + c) $\n",
    "\n",
    "- **Exponential Transformation:** Exponential transformation is used to transform skewed data to approximately conform to normality. It can be used to normalize data that contains negative values, as long as the data is shifted by a constant value (e.g., by adding the absolute value of the most negative value in the data).\n",
    "\n",
    "$ x_{norm} = e^{x + c} $\n",
    "\n",
    "**Code Example for Normalization using scikit-learn:**\n",
    "\n",
    "```python\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Sample data\n",
    "data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
    "\n",
    "# Create the scaler object with a range of [0, 1]\n",
    "min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit on data\n",
    "min_max_scaler.fit(data)\n",
    "\n",
    "# Transform the original data\n",
    "normalized_data = min_max_scaler.transform(data)\n",
    "\n",
    "print(\"Original Data:\\n\", data)\n",
    "\n",
    "print(\"Normalized Data:\\n\", normalized_data)\n",
    "\n",
    "```\n",
    "\n",
    "In this example, we use scikit-learn's `preprocessing` module to normalize data using min-max normalization. We create sample data and then use the `MinMaxScaler` class to fit and transform the data. The resulting normalized data is printed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dfc2d87c",
   "metadata": {},
   "source": [
    "# All data preprocessing techniques and tricks\n",
    "\n",
    "**Data Preprocessing:**\n",
    "- Data preprocessing is an important step in the machine learning pipeline. It involves transforming raw data into an understandable format for machine learning models. Data preprocessing prepares raw data for further processing.\n",
    "\n",
    "**Data Preprocessing Techniques:**\n",
    "- **Data Cleaning:** Data cleaning is the process of fixing or removing incorrect, corrupted, incorrectly formatted, duplicate, or incomplete data within a dataset. Data cleaning is important because it can improve the quality of data, which can have a significant effect on the accuracy of machine learning models.\n",
    "\n",
    "- **Data Integration:** Data integration is the process of combining data from multiple sources into a single, unified view. Data integration is important because it can provide more accurate and comprehensive information, which can lead to better decision-making.\n",
    "\n",
    "- **Data Transformation:** Data transformation is the process of converting data from one format or structure into another format or structure. Data transformation is important because it can help prepare data for machine learning models, which can improve the accuracy of predictions.\n",
    "\n",
    "- **Data Reduction:** Data reduction is the process of reducing the amount of data to make it easier to work with. Data reduction is important because it can help improve the efficiency of machine learning algorithms, which can reduce training time and improve performance.\n",
    "\n",
    "**Code Example for Data Preprocessing using scikit-learn:**\n",
    "\n",
    "```python\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Sample data\n",
    "raw_data = [[3, -1.5,  2, -5.4],\n",
    "            [0,  4,  -0.3, 2.1],\n",
    "            [1,  3.3, -1.9, -4.3]]\n",
    "\n",
    "# Binarization\n",
    "binarized_data = preprocessing.Binarizer(threshold=1.4).transform(raw_data)\n",
    "print(\"\\nBinarized data:\\n\", binarized_data)\n",
    "\n",
    "# Mean Removal\n",
    "print(\"\\nBefore:\")\n",
    "print(\"Mean =\", raw_data.mean(axis=0))\n",
    "print(\"Std deviation =\", raw_data.std(axis=0))\n",
    "\n",
    "scaled_data = preprocessing.scale(raw_data)\n",
    "print(\"\\nAfter:\")\n",
    "print(\"Mean =\", scaled_data.mean(axis=0))\n",
    "print(\"Std deviation =\", scaled_data.std(axis=0))\n",
    "\n",
    "# Scaling\n",
    "scaled_data_min_max = preprocessing.MinMaxScaler(feature_range=(0, 1)).fit_transform(raw_data)\n",
    "print(\"\\nMin max scaled data:\\n\", scaled_data_min_max)\n",
    "\n",
    "# Normalization (Read more in the dealing with Overfiting, Underfiting)\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6bde81f9",
   "metadata": {},
   "source": [
    "# Understand your data\n",
    "- Data is the most important part of machine learning. It is the fuel that powers the machine learning engine. Without data, machine learning algorithms would not be able to learn and make predictions. Therefore, it's important to understand your data before you start building machine learning models. This includes understanding the data types, data distributions, data quality, and relationships between variables.\n",
    "\n",
    "**Data Types:**\n",
    "- Numerical data represents quantities and is expressed in numbers. It can be further classified into discrete and continuous data. Discrete data can only take on a finite number of values, while continuous data can take on an infinite number of values.\n",
    "\n",
    "Ex: Age, Height, Weight, Income, etc.\n",
    "    - Also you will notice there is two different types of numerical data, discrete and continuous. Discrete data is data that can only take on a finite number of values (1,2,3,4,...). Continuous data is data that can take on an infinite number of values (1.2049, 2.09948,...).\n",
    "\n",
    "- Categorical data represents characteristics and is expressed in words. It can be further classified into ordinal and nominal data. Ordinal data has a natural ordering, while nominal data does not have a natural ordering.\n",
    "\n",
    "**Data information:**\n",
    "- Pandas is one of the most common libraries used for data analysis and manipulation. It provides a convenient way to store and analyze tabular data, such as data stored in spreadsheets or databases. Pandas is built on top of the NumPy library, which makes it easy to work with multidimensional arrays and matrices.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"data.csv\") # Where data is your file name\n",
    "```\n",
    "\n",
    "- Pandas have high level data manipulation tools so it should be easy to understand and use. There is wide range of file type that pandas can read, such as csv, excel, json, html, sql, etc. Pandas also have a lot of built in functions that can help you understand your data better.\n",
    "\n",
    "**Data Distributions:**\n",
    "- Data distributions are a summary of the frequency of individual values or ranges of values within a dataset. They can be used to understand the shape of the data distribution, as well as to understand the skewness, kurtosis, and other parameters of the data.\n",
    "\n",
    "**Data Quality:**\n",
    "- Data quality refers to the condition of data with respect to its fitness for a specific purpose. High-quality data is accurate, consistent, complete, and up-to-date. It is also relevant, timely, and accessible.\n",
    "\n",
    "**Relationships between Variables:**\n",
    "- The relationship between variables is a key concept in statistics, data analysis, and machine learning. It refers to the statistical association, dependence, or correlation between two or more variables. Understanding the relationships between variables can help you understand the data better and make better predictions.\n",
    "\n",
    "**Data processing method:**\n",
    "- Data Cleaning: the process of fixing or removing incorrect, corrupted, incorrectly formatted, duplicate, or incomplete data within a dataset.\n",
    "```python\n",
    "# Drop duplicates\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Drop missing values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Fill missing values with zeros\n",
    "data.fillna(0, inplace=True)\n",
    "\n",
    "# Fill missing values with mean\n",
    "data.fillna(data.mean(), inplace=True)\n",
    "\n",
    "# Fill missing values with median\n",
    "data.fillna(data.median(), inplace=True)\n",
    "\n",
    "# Fill missing values with mode\n",
    "data.fillna(data.mode(), inplace=True)\n",
    "\n",
    "# Fill missing values with forward fill\n",
    "data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Checking data types\n",
    "data.dtypes\n",
    "\n",
    "# Change data type of a column\n",
    "data['column_name'] = data['column_name'].astype('int64') # The astype function is used to change the data type of a column\n",
    "\n",
    "# Remove noisy data (outliers)\n",
    "data = data[data['column_name'] < 100] # Remove all rows where column_name is greater than 100, the variable 100 act like the threshold where you want to remove the noisy data\n",
    "\n",
    "```\n",
    "\n",
    "- Data Integration: the process of combining data from multiple sources into a single, unified view.\n",
    "```python\n",
    "# Concatenate two dataframes\n",
    "df = pd.concat([df1, df2], axis=0) # axis=0 means concatenate along rows\n",
    "\n",
    "# Merge two dataframes\n",
    "df = pd.merge(df1, df2, on='column_name') # Merge df1 and df2 on column_name\n",
    "\n",
    "# Join two dataframes\n",
    "df = df1.join(df2, how='inner') # Join df1 and df2 on index\n",
    "```\n",
    "\n",
    "- Data Transformation: the process of converting data from one format or structure into another format or structure.\n",
    "- Data Normalization: the process of transforming numeric data into a uniform range, typically [0, 1]. This is done by subtracting the minimum value in the feature and then dividing the result by the difference between the original maximum and original minimum value in the feature.\n",
    "- Data Discretization: the process of transforming continuous data into discrete data. This is done by dividing the range of possible values into intervals and then assigning values to each interval.\n",
    "```python\n",
    "# Apply a function to a column\n",
    "df['column_name'] = df['column_name'].apply(lambda x: x * 2) # Multiply each value in column_name by 2\n",
    "\n",
    "# Apply a function to a row\n",
    "df['column_name'] = df.apply(lambda x: x['column_name'] * 2, axis=1) # Multiply each value in column_name by 2\n",
    "\n",
    "from sklearn import preprocessing # Machine Learning library\n",
    "\n",
    "# Min-Max Normalization\n",
    "min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1)) # Create the scaler object with a range of [0, 1]\n",
    "min_max_scaler.fit(data) # Fit on data\n",
    "normalized_data = min_max_scaler.transform(data) # Transform the original data\n",
    "\n",
    "# Mean Removal\n",
    "scaled_data = preprocessing.scale(data) # Scale the data\n",
    "\n",
    "# Standardization\n",
    "standardized_data = preprocessing.StandardScaler().fit_transform(data) # Standardize the data\n",
    "\n",
    "# Discretization\n",
    "discretized_data = preprocessing.KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform').fit_transform(data) # Discretize the data\n",
    "\n",
    "# Binarization\n",
    "binarized_data = preprocessing.Binarizer(threshold=1.4).transform(data) # Binarize the data\n",
    "\n",
    "# One-Hot Encoding\n",
    "one_hot_encoded_data = pd.get_dummies(data) # One-hot encode the data\n",
    "\n",
    "# Label Encoding\n",
    "label_encoded_data = data.apply(preprocessing.LabelEncoder().fit_transform) # Label encode the data\n",
    "\n",
    "# Log Transformation\n",
    "log_transformed_data = data.apply(lambda x: np.log(x + 1)) # Log transform the data\n",
    "\n",
    "```\n",
    "\n",
    "- Data Reduction: the process of reducing the amount of data to make it easier to work with. (Read more in Feature Selection and Feature Extraction)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e427180",
   "metadata": {},
   "source": [
    "# How to choosing your activation function\n",
    "\n",
    "**Activation Function:**\n",
    "- Activation functions are mathematical equations that determine the output of a neural network. The function is attached to each neuron in the network, and determines whether it should be activated (\"fired\") or not, based on whether each neuron's input is relevant for the model's prediction.\n",
    "\n",
    "**Get used to the activation functions:**\n",
    "- There are several activation functions that are commonly used in neural networks. These include the sigmoid, tanh, ReLU, Leaky ReLU, and softmax functions. Each of these functions has its own pros and cons, and is more suitable for certain types of neural networks and applications.\n",
    "\n",
    "**Sigmoid Function:**\n",
    "- The sigmoid function is a mathematical function that takes any real input and outputs a value between 0 and 1. It is also known as the logistic function, since it is used in logistic regression to model the probability of an event occurring as the value of the input varies. The sigmoid function is commonly used in neural networks as an activation function for artificial neurons.\n",
    "$$ f(x) = \\frac{1}{1 + e^{-x}} $$\n",
    "\n",
    "**Tanh Function:**\n",
    "- The tanh function is a mathematical function that takes any real input and outputs a value between -1 and 1. It is also known as the hyperbolic tangent function. The tanh function is commonly used in neural networks as an activation function for artificial neurons.\n",
    "$$ f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} $$\n",
    "\n",
    "**ReLU Function:**\n",
    "- The ReLU function is a mathematical function that takes any real input and outputs the maximum between 0 and the input value. It is also known as the rectified linear unit function. The ReLU function is commonly used in neural networks as an activation function for artificial neurons.\n",
    "$$ f(x) = max(0, x) $$\n",
    "\n",
    "**Leaky ReLU Function:**\n",
    "- The Leaky ReLU function is a mathematical function that takes any real input and outputs the maximum between 0.01x and x. It is a variant of the ReLU function. The Leaky ReLU function is commonly used in neural networks as an activation function for artificial neurons.\n",
    "$$ f(x) = max(0.01x, x) $$\n",
    "\n",
    "**Softmax Function:**\n",
    "- The softmax function is a mathematical function that takes a vector of real numbers and outputs a vector of values between 0 and 1 that sum to 1. It is also known as the normalized exponential function. The softmax function is commonly used in neural networks as an activation function for the output layer.\n",
    "$$ f(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}} $$\n",
    "\n",
    "**So how to choosing the right activation function:**\n",
    "- When I first started learning about neural networks, I was confused about which activation function to use. There are so many different activation functions, and it's not always clear which one is the best choice for a given problem. After doing some research, I found that there are a few guidelines that can help you choose the right activation function for your neural network.\n",
    "    - **Sigmoid:** The sigmoid function is a good choice for the output layer of a binary classification problem, since it outputs a value between 0 and 1 that can be interpreted as a probability. However, it is not recommended for hidden layers, since it can cause the vanishing gradient problem.\n",
    "    - **Tanh:** The tanh function is a good choice for the output layer of a multiclass classification problem, since it outputs a value between -1 and 1 that can be interpreted as a probability. However, it is not recommended for hidden layers, since it can cause the vanishing gradient problem.\n",
    "    - **ReLU:** The ReLU function is a good choice for hidden layers, since it does not cause the vanishing gradient problem. However, it is not recommended for the output layer, since it outputs a value between 0 and 1 that cannot be interpreted as a probability.\n",
    "    - **Leaky ReLU:** The Leaky ReLU function is a good choice for hidden layers, since it does not cause the vanishing gradient problem. However, it is not recommended for the output layer, since it outputs a value between 0 and 1 that cannot be interpreted as a probability.\n",
    "    - **Softmax:** The softmax function is a good choice for the output layer of a multiclass classification problem, since it outputs a vector of values between 0 and 1 that sum to 1 and can be interpreted as probabilities. However, it is not recommended for hidden layers, since it can cause the vanishing gradient problem.\n",
    "    - **Other:** There are many other activation functions that are not covered in this article, such as the hyperbolic tangent function, the exponential linear unit function, and the parametric ReLU function. These functions are not commonly used in neural networks, but they may be useful in certain situations.\n",
    "\n",
    "**Code Example for Activation Functions using TensorFlow:**\n",
    "    \n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Sample data\n",
    "x = tf.constant([-10, -5, 0, 5, 10], dtype=tf.float32)\n",
    "\n",
    "# Sigmoid\n",
    "print(\"Sigmoid:\\n\", tf.nn.sigmoid(x).numpy())\n",
    "\n",
    "# Tanh\n",
    "print(\"\\nTanh:\\n\", tf.nn.tanh(x).numpy())\n",
    "\n",
    "# ReLU\n",
    "print(\"\\nReLU:\\n\", tf.nn.relu(x).numpy())\n",
    "\n",
    "# Leaky ReLU\n",
    "print(\"\\nLeaky ReLU:\\n\", tf.nn.leaky_relu(x, alpha=0.01).numpy())\n",
    "\n",
    "# Softmax\n",
    "print(\"\\nSoftmax:\\n\", tf.nn.softmax(x).numpy())\n",
    "\n",
    "```\n",
    "\n",
    "In this example, we use TensorFlow's `tf.nn` module to apply various activation functions to sample data. We create sample data and then use the `sigmoid`, `tanh`, `relu`, `leaky_relu`, and `softmax` functions to apply the corresponding activation functions. The resulting outputs are printed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "729b57d4",
   "metadata": {},
   "source": [
    "# Penalty in Machine Learning\n",
    "\n",
    "**Penalty:**\n",
    "- A penalty is a function that is added to the loss function of a machine learning model to encourage the model to prefer certain solutions over others. It is often used to prevent overfitting by penalizing large coefficients in the model.\n",
    "\n",
    "**Penalty Types:**\n",
    "- **L1 Penalty:** The L1 penalty is a regularization technique that adds a penalty equal to the sum of the absolute value of the coefficients. It is also known as Lasso regularization. The L1 penalty is commonly used in linear regression models to encourage sparsity.\n",
    "$$ L1 = \\sum_{i=1}^{n} |x_i| $$\n",
    "\n",
    "- **L2 Penalty:** The L2 penalty is a regularization technique that adds a penalty equal to the sum of the squared value of the coefficients. It is also known as Ridge regularization. The L2 penalty is commonly used in linear regression models to prevent overfitting.\n",
    "$$ L2 = \\sum_{i=1}^{n} x_i^2 $$\n",
    "\n",
    "- **Elastic Net Penalty:** The Elastic Net penalty is a regularization technique that combines the L1 and L2 penalties. It is also known as L1/L2 regularization. The Elastic Net penalty is commonly used in linear regression models to prevent overfitting.\n",
    "$$ Elastic Net = \\alpha L1 + (1 - \\alpha) L2 $$\n",
    "\n",
    "Where $\\alpha$ is the mixing parameter between the L1 and L2 penalties. Base on the range of $\\alpha$ we can have different types of Elastic Net penalty:\n",
    "- $\\alpha = 0$: L2 penalty\n",
    "- $\\alpha = 1$: L1 penalty\n",
    "- $\\alpha = 0.5$: Elastic Net penalty ($\\frac{L1}{L2}$)\n",
    "\n",
    "**Code Example for Penalty using scikit-learn:**\n",
    "- We will work with Linear Regression model to see how penalty works\n",
    "\n",
    "```python\n",
    "from sklearn import linear_model\n",
    "\n",
    "# Sample data\n",
    "X = [[0, 0], [1, 1], [2, 2]]\n",
    "y = [0, 1, 2]\n",
    "\n",
    "# Create a Linear Regression model with L1 penalty\n",
    "reg = linear_model.Lasso(alpha=0.1)\n",
    "\n",
    "# Create a Linear Regression model with L2 penalty\n",
    "# reg = linear_model.Ridge(alpha=0.1)\n",
    "\n",
    "# Create a Linear Regression model with Elastic Net penalty\n",
    "# reg = linear_model.ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "\n",
    "# Fit the model to the data\n",
    "reg.fit(X, y)\n",
    "\n",
    "# View the coefficients of the model\n",
    "print(reg.coef_)\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1210a262",
   "metadata": {},
   "source": [
    "# Learning Curve\n",
    "\n",
    "**Learning Curve:**\n",
    "- A learning curve is a plot of the model's performance on the training set and the validation set as a function of the training set size. It is used to visualize how the model's performance improves as the training set size increases.\n",
    "\n",
    "**Learning Curve Types:** (I have describe this in the Overfitting and Underfitting section)\n",
    "- **Underfitting:** Underfitting occurs when the model is not able to capture the underlying pattern of the data. This can happen if the model is too simple (e.g., a linear model), or if it is trained on too little data. Underfitting can be identified by a low training score and a low validation score. (high bias)\n",
    "- **Overfitting:** Overfitting occurs when the model is able to capture the underlying pattern of the data too well. This can happen if the model is too complex (e.g., a high-degree polynomial model), or if it is trained on too little data. Overfitting can be identified by a high training score and a low validation score. (high variance)\n",
    "\n",
    "**Code Example for Learning Curve using scikit-learn:**\n",
    "- We will work with Linear Regression model to see how learning curve works\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# Sample data\n",
    "X = [[0, 0], [1, 1], [2, 2]]\n",
    "y = [0, 1, 2]\n",
    "\n",
    "# Create a Linear Regression model\n",
    "reg = linear_model.LinearRegression()\n",
    "\n",
    "# Calculate the training and testing scores\n",
    "train_sizes, train_scores, test_scores = learning_curve(reg, X, y, cv=10, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Calculate the mean and standard deviation of the training scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "\n",
    "# Calculate the mean and standard deviation of the testing scores\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.plot(train_sizes, train_mean, label=\"Training score\")\n",
    "plt.plot(train_sizes, test_mean, label=\"Cross-validation score\")\n",
    "\n",
    "# Draw the bands\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n",
    "\n",
    "# Create the plot\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a88962d1",
   "metadata": {},
   "source": [
    "# Error Analysis\n",
    "\n",
    "**Theory:**\n",
    "- Error analysis is the process of analyzing the errors made by a machine learning model. It is used to identify the types of errors made by the model, and to determine the root causes of these errors. Error analysis can be used to improve the performance of the model by addressing the root causes of the errors.\n",
    "\n",
    "**Error Analysis Types:**\n",
    "- **Bias Error:** Bias error is the difference between the expected value of the model's predictions and the true value of the target variable. It is caused by the model's inability to represent the underlying pattern of the data. Bias error can be reduced by increasing the model's complexity (e.g., by adding more features or using a more complex model).\n",
    "- **Variance Error:** Variance error is the variability of the model's predictions. It is caused by the model's sensitivity to small fluctuations in the training set. Variance error can be reduced by increasing the size of the training set (e.g., by collecting more data or using data augmentation).\n",
    "- **Irreducible Error:** Irreducible error is the error that cannot be reduced by improving the model. It is caused by the noise in the data. Irreducible error can be reduced by removing outliers from the training set (e.g., by using outlier detection techniques).\n",
    "\n",
    "**How to solve the error:**\n",
    "- **Bias Error:** Bias error can be reduced by increasing the model's complexity (e.g., by adding more features or using a more complex model).\n",
    "- **Variance Error:** Variance error can be reduced by increasing the size of the training set (e.g., by collecting more data or using data augmentation).\n",
    "- **Irreducible Error:** Irreducible error can be reduced by removing outliers from the training set (e.g., by using outlier detection techniques).\n",
    "\n",
    "**All the problem above not alway work, it depend on the problem you are facing. So please research and prepare carefully before you start to solve the problem.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5ab21c",
   "metadata": {},
   "source": [
    "# Method to dealing with oversampling and undersampling\n",
    "\n",
    "## Oversampling\n",
    "\n",
    "### SMOTE (Synthetic Minority Oversampling Technique)\n",
    "\n",
    "**SMOTE:**\n",
    "- SMOTE is a technique for oversampling the minority class in an imbalanced dataset. It works by creating synthetic samples from the minor class instead of creating copies. It is used to improve the performance of machine learning models when the dataset is imbalanced.\n",
    "\n",
    "**SMOTE Algorithm:**\n",
    "- The SMOTE algorithm works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space, and then creating new examples along this line. The new examples are created by randomly selecting a point along the line and then adding a random amount of noise to the point.\n",
    "\n",
    "**Code Example for SMOTE using imbalanced-learn:**\n",
    "- We will work with Linear Regression model to see how SMOTE works\n",
    "\n",
    "```python\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Sample data\n",
    "X = [[0, 0], [1, 1], [2, 2]]\n",
    "y = [0, 1, 2]\n",
    "\n",
    "# Create a SMOTE object\n",
    "sm = SMOTE()\n",
    "\n",
    "# Fit the model to the data\n",
    "X_sm, y_sm = sm.fit_resample(X, y)\n",
    "\n",
    "# View the data\n",
    "print(X_sm)\n",
    "print(y_sm)\n",
    "```\n",
    "\n",
    "### ADASYN (Adaptive Synthetic Sampling Approach for Imbalanced Learning)\n",
    "\n",
    "**ADASYN:**\n",
    "- ADASYN is a technique for oversampling the minority class in an imbalanced dataset. It works by creating synthetic samples from the minor class instead of creating copies. It is used to improve the performance of machine learning models when the dataset is imbalanced.\n",
    "\n",
    "**ADASYN Algorithm:**\n",
    "- The ADASYN algorithm works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space, and then creating new examples along this line. The new examples are created by randomly selecting a point along the line and then adding a random amount of noise to the point.\n",
    "\n",
    "**Code Example for ADASYN using imbalanced-learn:**\n",
    "- We will work with Linear Regression model to see how ADASYN works\n",
    "\n",
    "```python\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "# Sample data\n",
    "X = [[0, 0], [1, 1], [2, 2]]\n",
    "y = [0, 1, 2]\n",
    "\n",
    "# Create a ADASYN object\n",
    "ada = ADASYN()\n",
    "\n",
    "# Fit the model to the data\n",
    "X_ada, y_ada = ada.fit_resample(X, y)\n",
    "\n",
    "# View the data\n",
    "print(X_ada)\n",
    "print(y_ada)\n",
    "```\n",
    "\n",
    "### Random Oversampling\n",
    "\n",
    "**Random Oversampling:**\n",
    "- Random oversampling is a technique for oversampling the minority class in an imbalanced dataset. It works by randomly selecting examples from the minority class and adding them to the training set. It is used to improve the performance of machine learning models when the dataset is imbalanced.\n",
    "\n",
    "**Code Example for Random Oversampling using imbalanced-learn:**\n",
    "- We will work with Linear Regression model to see how Random Oversampling works\n",
    "\n",
    "```python\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Sample data\n",
    "X = [[0, 0], [1, 1], [2, 2]]\n",
    "y = [0, 1, 2]\n",
    "\n",
    "# Create a RandomOverSampler object\n",
    "ros = RandomOverSampler()\n",
    "\n",
    "# Fit the model to the data\n",
    "X_ros, y_ros = ros.fit_resample(X, y)\n",
    "\n",
    "# View the data\n",
    "print(X_ros)\n",
    "print(y_ros)\n",
    "```\n",
    "\n",
    "## Undersampling\n",
    "\n",
    "### Random Undersampling\n",
    "\n",
    "**Random Undersampling:**\n",
    "- Random undersampling is a technique for undersampling the majority class in an imbalanced dataset. It works by randomly selecting examples from the majority class and removing them from the training set. It is used to improve the performance of machine learning models when the dataset is imbalanced.\n",
    "\n",
    "**Code Example for Random Undersampling using imbalanced-learn:**\n",
    "- We will work with Linear Regression model to see how Random Undersampling works\n",
    "\n",
    "```python\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Sample data\n",
    "X = [[0, 0], [1, 1], [2, 2]]\n",
    "y = [0, 1, 2]\n",
    "\n",
    "# Create a RandomUnderSampler object\n",
    "rus = RandomUnderSampler()\n",
    "\n",
    "# Fit the model to the data\n",
    "X_rus, y_rus = rus.fit_resample(X, y)\n",
    "\n",
    "# View the data\n",
    "print(X_rus)\n",
    "print(y_rus)\n",
    "```\n",
    "\n",
    "### NearMiss\n",
    "\n",
    "**NearMiss:**\n",
    "- NearMiss is a technique for undersampling the majority class in an imbalanced dataset. It works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space, and then removing examples that are on the wrong side of the line. It is used to improve the performance of machine learning models when the dataset is imbalanced.\n",
    "\n",
    "**NearMiss Algorithm:**\n",
    "- The NearMiss algorithm works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space, and then removing examples that are on the wrong side of the line.\n",
    "\n",
    "**Code Example for NearMiss using imbalanced-learn:**\n",
    "- We will work with Linear Regression model to see how NearMiss works\n",
    "\n",
    "```python\n",
    "from imblearn.under_sampling import NearMiss\n",
    "\n",
    "# Sample data\n",
    "X = [[0, 0], [1, 1], [2, 2]]\n",
    "y = [0, 1, 2]\n",
    "\n",
    "# Create a NearMiss object\n",
    "nm = NearMiss()\n",
    "\n",
    "# Fit the model to the data\n",
    "X_nm, y_nm = nm.fit_resample(X, y)\n",
    "\n",
    "# View the data\n",
    "print(X_nm)\n",
    "print(y_nm)\n",
    "```\n",
    "\n",
    "### Tomek Links\n",
    "\n",
    "**Tomek Links:**\n",
    "- Tomek Links is a technique for undersampling the majority class in an imbalanced dataset. It works by removing examples that are close in the feature space to examples from the minority class. It is used to improve the performance of machine learning models when the dataset is imbalanced.\n",
    "\n",
    "**Tomek Links Algorithm:**\n",
    "- The Tomek Links algorithm works by removing examples that are close in the feature space to examples from the minority class.\n",
    "\n",
    "**Code Example for Tomek Links using imbalanced-learn:**\n",
    "- We will work with Linear Regression model to see how Tomek Links works\n",
    "\n",
    "```python\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "# Sample data\n",
    "X = [[0, 0], [1, 1], [2, 2]]\n",
    "y = [0, 1, 2]\n",
    "\n",
    "# Create a TomekLinks object\n",
    "tl = TomekLinks()\n",
    "\n",
    "# Fit the model to the data\n",
    "X_tl, y_tl = tl.fit_resample(X, y)\n",
    "\n",
    "# View the data\n",
    "print(X_tl)\n",
    "print(y_tl)\n",
    "```\n",
    "\n",
    "### Edited Nearest Neighbours\n",
    "\n",
    "**Edited Nearest Neighbours:**\n",
    "- Edited Nearest Neighbours is a technique for undersampling the majority class in an imbalanced dataset. It works by removing examples that are close in the feature space to examples from the minority class. It is used to improve the performance of machine learning models when the dataset is imbalanced.\n",
    "\n",
    "**Edited Nearest Neighbours Algorithm:**\n",
    "- The Edited Nearest Neighbours algorithm works by removing examples that are close in the feature space to examples from the minority class.\n",
    "\n",
    "**Code Example for Edited Nearest Neighbours using imbalanced-learn:**\n",
    "- We will work with Linear Regression model to see how Edited Nearest Neighbours works\n",
    "\n",
    "```python\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "\n",
    "# Sample data\n",
    "X = [[0, 0], [1, 1], [2, 2], [3, 3], [4, 4]]\n",
    "y = [0, 1, 2, 3, 4]\n",
    "\n",
    "# Create a EditedNearestNeighbours object\n",
    "enn = EditedNearestNeighbours()\n",
    "\n",
    "# Fit the model to the data\n",
    "X_enn, y_enn = enn.fit_resample(X, y)\n",
    "\n",
    "# View the data\n",
    "print(X_enn)\n",
    "print(y_enn)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
