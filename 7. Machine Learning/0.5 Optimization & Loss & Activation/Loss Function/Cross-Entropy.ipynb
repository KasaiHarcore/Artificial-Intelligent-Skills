{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-Entropy Loss, also known as Log Loss, comes from information theory and is widely used in classification tasks, especially for neural networks. It measures the dissimilarity between two probability distributions: the true labels and the predicted probabilities from a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-Entropy Loss evaluates the performance of a classification model by comparing the predicted probability distribution to the actual label distribution. It penalizes predictions that are far from the true label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mathematically**, the formula for Cross-Entropy Loss is: (for binary classification)\n",
    "\n",
    "$$\\mathcal{L}(y, \\hat{y}) = -(y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}))$$\n",
    "\n",
    "where:\n",
    "- $y$ is the true label (0 or 1)\n",
    "- $\\hat{y}$ is the predicted probability of the positive class (between 0 and 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multi-class classification, the formula is:\n",
    "\n",
    "$$\\mathcal{L}(y, \\hat{y}) = -\\sum_{i=1}^{C} y_i \\log(\\hat{y}_i)$$\n",
    "\n",
    "where:\n",
    "- $C$ is the number of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-Entropy Loss assigns a high penalty to predictions with high confidence but incorrect classification, and a lower penalty when the model predicts closer to the true label. The loss is minimized when the predicted probabilities match the true labels perfectly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the true label is $y = 1$ and the model predicts $\\hat{y} = 0.9$, the Cross-Entropy Loss is:\n",
    "\n",
    "$$\\mathcal{L}(1, 0.9) = -(1 \\log(0.9) + (1 - 1) \\log(1 - 0.9)) = -\\log(0.9)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performs well, so the loss is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pros and Cons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros:\n",
    "- Works well for both binary and multi-class classification.\n",
    "- Efficient for probabilistic models and outputs calibrated probabilities.\n",
    "- Differentiable, making it ideal for gradient-based optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cons:\n",
    "- Sensitive to noisy labels.\n",
    "- Overconfidence in wrong predictions can cause large penalties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use in classification problems (binary or multi-class).\n",
    "- Works well with models that output probability distributions, such as neural networks with softmax or sigmoid activation.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
