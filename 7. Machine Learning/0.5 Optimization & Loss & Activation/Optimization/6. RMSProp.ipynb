{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While AdaGrad adapts the learning rate based on the gradient accumulation for each parameter, it tends to reduce the learning rate too drastically over time, causing the model to stop learning effectively. RMSProp (Root Mean Square Propagation) is a modification of AdaGrad that tries to resolve this issue by changing the way the learning rate is adapted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSProp **keeps track** of an exponentially decaying aaverage of squared gradients. Instead of accumulating all past squared gradients as AdaGrad does, RMSProp uses a moving average. This means that RMSProp gives more weight to recent gradients, allowing it to adapt more effectively without diminishing the learning rate too quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be surprised to see how simple the RMSProp algorithm is. It is just a small modification to the AdaGrad algorithm combine with Momentum:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "v_t = \\beta v_{t - 1} + (1 - \\beta) \\nabla_{\\theta} J(\\theta_{t - 1})^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $v_t$ is the moving average of squared gradients at time step $t$\n",
    "- $\\beta$ is the decay rate, typically set to 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the update rule is:\n",
    "\n",
    "$$\n",
    "\\theta_{t} = \\theta_{t - 1} - \\frac{\\eta}{\\sqrt{v_t + \\epsilon}} \\cdot \\nabla_{\\theta} J(\\theta_{t - 1})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crazy right? Just a small modification to AdaGrad and we have a new optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This optimization can be valued in many scenarios, especially when you are training deep neural networks. Because:\n",
    "- Efficient with Non-Convex Problems: RMSProp performs well with complex, non-convex optimization problems common in deep learning, unlike AdaGrad, which works best in convex settings.\n",
    "- Stable Learning Rate: RMSProp prevents the learning rate from diminishing too quickly, allowing the optimizer to move steadily towards the minimum.\n",
    "- Empirically Proven Performance: RMSProp is widely used in practice due to its robustness and stability across a variety of neural network architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to all of that advantage, but it's not the best; which will lead us to Adam, a very popular optimizer in the deep learning community."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
