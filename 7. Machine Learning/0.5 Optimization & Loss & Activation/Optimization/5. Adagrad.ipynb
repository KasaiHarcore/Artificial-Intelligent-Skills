{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stands for *Adaptive Gradient*, and it's a modification of gradient descent that adjusts the learning rate dynamically for each parameter. Traditional gradient descent methods keep a fixed learning rate, which doesn't always work well for sparse features. **Sparse features** are those where most values are zero, such as a dataset with lots of missing or inactive features.\n",
    "\n",
    "AdaGrad is especially useful for dealing with this type of data because it **adapts** to the sparse nature of the dataset, making it easier to learn the important features without getting \"stuck\" on the zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the update rule:\n",
    "\n",
    "$$\n",
    "\\theta_{t} = \\theta_{t - 1} - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} \\cdot \\nabla_{\\theta} J(\\theta_{t - 1})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $G_t = G_{t-1} + \\nabla_{\\theta} J(\\theta_{t - 1})^2$: the sum of the squares of the gradients up to time $t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we don't have to tuning the learning rate manually, and it's a good choice for many optimization problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But in the other hand, AdaGrad has some drawbacks, such as:\n",
    "- It's adjust the learning rates based on the sum of past squared gradients, if the learning rate is always decreasing, which can make the learning process too slow.\n",
    "- The accumulation of squared gradients in the denominator can make the learning rate too small, and the algorithm can stop learning."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
