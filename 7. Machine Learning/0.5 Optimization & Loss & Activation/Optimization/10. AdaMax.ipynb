{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaMax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaMax algorithm is an extension to the Adaptive Movement Estimation (Adam) Optimization algorithm. The algorithm was described in the 2014 paper by Diederik Kingma and Jimmy Lei Ba titled “Adam: A Method for Stochastic Optimization.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam can be understood as updating weights inversely proportional to the scaled L2 norm (squared) of past gradients. AdaMax extends this to the so-called infinite norm (max) of past gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, AdaMax automatically adapts a separate step size (learning rate) for each parameter in the optimization problem.\n",
    "\n",
    "Let’s step through the AdaMax algorithm:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_t & = \\beta_1 m_{t-1} + (1 - \\beta_1) \\nabla_{\\theta} J(\\theta_{t-1}) \\\\\n",
    "\n",
    "u_t & = \\max(\\beta_2 u_{t-1}, |g_t|) \\\\\n",
    "\n",
    "\\theta_t & = \\theta_{t-1} - \\frac{\\alpha}{1 - \\beta_1^t} \\frac{m_t}{u_t}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $u_t$ is the infinity norm-based term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\frac{\\alpha}{1 - \\beta_1^t}$ the learning rate correction factor accounts for bias in the estimation of, especially during early iterations.\n",
    "\n",
    "$\\frac{m_t}{u_t}$ Normalizes the smoothed gradient by the infinity norm of the past gradients which helps ensure that updates are stable and well-scaled"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
