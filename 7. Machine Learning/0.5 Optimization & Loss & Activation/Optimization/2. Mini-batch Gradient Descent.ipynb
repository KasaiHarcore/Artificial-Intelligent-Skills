{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike SGD, mini-batch uses a number $n$ that is larger than 1 (but still much smaller than the total data $N$). Like SGD, Mini-batch Gradient Descent starts each epoch by randomly shuffling the data and then dividing the entire data into mini-batches, each mini-batch having $n$ data points (except the last mini-batch which may have less if $N$ is not divisible by $n$). Each update, this algorithm takes a mini-batch to calculate the derivative and then updates. The formula can be written as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta_{t + 1} = \\theta_t - \\eta \\frac{1}{n} \\sum_{i=1}^{n} \\nabla J(\\theta_t, x^{(i)}, y^{(i)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    "- $\\theta_t$: parameters at time $t$\n",
    "- $\\eta$: learning rate\n",
    "- $n$: a mini-batch (a subset of the entire data) containing $n$ data points\n",
    "- $J(\\theta, x^{(i)}, y^{(i)})$: the loss function\n",
    "- $\\nabla J(\\theta, x^{(i)}, y^{(i)})$: the derivative of the loss function over small batch $n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mini-batch GD is mostly used in practice because it is faster than SGD and more stable than Batch GD. The number of mini-batch $n$ is usually chosen to be a power of 2 (e.g., 32, 64, 128, 256, 512, 1024, etc.) to make the computation more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages:\n",
    "- Balances the efficiency of SGD with the stability of Batch GD.\n",
    "- Allows for parallel computation on modern hardware (e.g., GPUs)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
