{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Adam optimization algorithm has become popular for training machine learning and deep learning models due to its efficiency and adaptability. Developed by Diederik Kingma and Jimmy Ba, Adam combines the advantages of the Momentum and RMSprop optimization algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Adam algorithm computes adaptive learning rates for each parameter using the first and second moments of the gradients. And I'm sure you will think why it's so simple to upgrade an optimizer algorithms by spawnming momentum. I don't know why but, it's effective and that is what matters :>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go to full details, I will write full implementation of Adam first before explain:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_t & = \\beta_1 v_{t-1} + (1 - \\beta_1) \\nabla_{\\theta} J(\\theta_{t - 1})\\\\\n",
    "s_t & = \\beta_2 s_{t-1} + (1 - \\beta_2) \\nabla_{\\theta} J(\\theta_{t - 1})^2\\\\ \n",
    "v_t^{corrected} & = \\frac{v_t}{1 - \\beta_1^t}\\\\\n",
    "s_t^{corrected} & = \\frac{s_t}{1 - \\beta_2^t}\\\\\n",
    "\\theta & = \\theta - \\alpha \\frac{v_t^{corrected}}{\\sqrt{s_t^{corrected}} + \\epsilon}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $v_t$: Represent for momentum\n",
    "- $s_t$: For RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will easily understand first two term and the final one, but why there's exist a \"corrected\" term?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer that, we can say it's because of the bias correction. The first two terms are biased toward zero especially at the beginning of the training. So, we need to correct them by dividing them by $1 - \\beta^t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And I think I don't have to fully say about the advantages of Adam (just a bunch of combination and wola, magic), it's just a good optimizer algorithm. But, there's a disadvantage of Adam, it's that it's sensitive to the initial learning rate. So, it's recommended to tune the learning rate frequently to achieve the best performance."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
