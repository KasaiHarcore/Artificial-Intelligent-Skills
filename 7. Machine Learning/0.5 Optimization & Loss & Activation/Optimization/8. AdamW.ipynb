{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam with Weight Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if you reach this notebook, that mean:\n",
    "1. You know what Adam is.\n",
    "2. You know what Weight Decay is.\n",
    "3. You already (or at least) finish half of the ML process and want to explore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if you don't know anything then I hope you will finish all machine learning first with first 8 common optimizer. Above it will be mostly use in Deep Learning or even higher level of machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok Let's continue, so let's us talk about the problem with regular Adam (or we say some variance of it like Adagrad, RMSprop, etc.). In some optimizer, weight decay is often added as an L2 regularization term to the loss function. However, adding this term to the loss affects the adaptive learning rates, which can hinder optimal convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you already apply or build some ML application, you will notice that we cannot using L2 with Adam in normal way. So AdamW is a smarter version of Adam as it decouples weight decay from the gradient update step. Instead of adding weight decay to the loss function, it applies weight decay directly during the parameter update, leading to more consistent regularization and better generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculation still the same, but the update rule is slightly different:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{\\hat{v}_t} + \\epsilon + \\lambda \\theta_t} \\hat{s}_t\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\lambda$ is the weight decay coefficient\n",
    "- $\\bar{v}_t = v_{\\text{corrected}}$; The same thing with $\\bar{s}_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This formulation prevents weight decay from affecting the adaptive learning rates and allows for a more consistent regularization effect.\n",
    "\n",
    "By separating weight decay from the gradient-based updates, AdamW achieves better generalization performance without interfering with the learning rate dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I don't like thing that's too much theory, but it's important to know the background. Let's move on!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
