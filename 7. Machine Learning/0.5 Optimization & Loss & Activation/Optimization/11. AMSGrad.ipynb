{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMSGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so this one is also a extension to the Adam version of gradient descent that attempts to improve the convergence properties of the algorithm, avoiding large abrupt changes in the learning rate for each input variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm was described in the 2018 paper by J. Sashank, et al. titled “On the Convergence of Adam and Beyond.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move into the details of the algorithm:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_t & = \\beta_1 m_{t-1} + (1 - \\beta_1) \\nabla_{\\theta} J(\\theta_{t - 1}) \\\\\n",
    "s_t & = \\beta_2 v_{t-1} + (1 - \\beta_2) (\\nabla_{\\theta} J(\\theta_{t - 1}))^2 \\\\\n",
    "\\hat{s}_t & = \\max(\\hat{s}_{t-1}, s_t) \\\\\n",
    "\\theta_{t} & = \\theta_{t - 1} - \\frac{\\alpha}{\\sqrt{\\hat{s}_t} + \\epsilon} v_t\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The content look the same from all previous versions of Adam, but the difference is in the way the $s_t$ is calculated. In the AMSGrad version, the $s_t$ is calculated as the maximum value between the previous $s_t$ and the current $s_t$."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
