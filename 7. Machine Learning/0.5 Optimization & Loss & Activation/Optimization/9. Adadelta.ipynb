{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adadelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adadelta is yet another variant of Adagrad. The main difference lies in the fact that it decreases the amount by which the learning rate is adaptive to coordinates. Moreover, traditionally it referred to as not having a learning rate since it uses the amount of change itself as calibration for future change. The algorithm was proposed in Zeiler. It is fairly straightforward, given the discussion of previous algorithms so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so instead of accumulating all past squared gradients, Adadelta maintains an exponentially decaying average of past squared gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach prevents the learning rate from diminishing too quickly, ensuring that the algorithm continues to learn effectively over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update rule is:\n",
    "\n",
    "$$\n",
    "\\theta_{t} = \\theta_{t - 1} - g'_{t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$g'_t$ called the \"rescaled gradient\" is defined as:\n",
    "\n",
    "$$\n",
    "g'_{t} = \\frac{\\sqrt{\\Delta \\theta_{t - 1} + \\epsilon}}{\\sqrt{s_{t} + \\epsilon}} \\cdot \\nabla_{\\theta} J(\\theta_{t - 1})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we calculated the $s_t$ as an exponentially decaying average of the squared gradients with:\n",
    "\n",
    "$$\n",
    "s_t = \\rho s_{t-1} + (1 - \\rho) \\nabla_{\\theta} J(\\theta_{t - 1})^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\rho$ is the decay rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Second, $\\Delta \\theta$, called leaky average of the squared rescaled gradient $g'_t$, with the same idea as above, we have:\n",
    "\n",
    "$$\n",
    "\\Delta \\theta_t = \\rho \\Delta \\theta_{t-1} + (1 - \\rho) g^{'2}_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look pretty complicated, but it's not. The algorithm is very similar to RMSprop, but it uses the RMS of parameter updates instead of the RMS of the gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And yes, another invention with under the name of Momentum. (Or we can say Momentum Expansions :>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages:\n",
    "- **Addresses Adagradâ€™s Diminishing Learning Rate**: By using a fixed window of past gradients, Adadelta maintains a consistent learning rate throughout training.\n",
    "- **No Need for Initial Learning Rate**: Unlike Adagrad, Adadelta does not require an initial learning rate, as it dynamically adjusts the step size based on the data.\n",
    "- **Improved Robustness**: The algorithm is more robust to the choice of hyperparameters and can perform well across different datasets without extensive tuning."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
