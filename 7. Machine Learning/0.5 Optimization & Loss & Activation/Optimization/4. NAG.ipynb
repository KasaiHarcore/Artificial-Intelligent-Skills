{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nesterov Accelerated Gradient (NAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesterov Accelerated Gradient is a refined version of momentum-based optimization, designed to further smooth and speed up convergence. It does so by looking ahead at the future gradient and adjusting accordingly, which helps reduce overshooting and improves stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In simple terms, NAG calculates the gradient at a **“look-ahead”** position rather than the current position. This look-ahead step is what makes NAG more effective at navigating tricky error surfaces, especially in non-convex optimization problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NAG](./img/nesterov.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand NAG, let’s explore how it differs from simple momentum:\n",
    "- Momentum Update: Traditional momentum calculates an update based on the current gradient and a weighted sum of past gradients, creating a “velocity” term that guides the update direction.\n",
    "- NAG Update: Instead of calculating the gradient at the current point, NAG takes a look-ahead step using the momentum term. It then computes the gradient at this new look-ahead position and adjusts the update based on this gradient. This approach makes it easier to predict when the optimizer is likely to overshoot, allowing for more controlled updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the update rule for NAG:\n",
    "\n",
    "$$\n",
    "v_t = \\gamma v_{t-1} + \\eta \\nabla_{\\theta} J(\\theta_{t - 1} - \\gamma v_{t-1}) \\\\\n",
    "\\theta_{t} = \\theta_{t - 1} - v_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And its look easy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NAG can take advantage of the momentum by:\n",
    "- **Reduced Oscillations**: By peeking ahead, NAG reduces oscillations around the minimum, which can be especially useful in error surfaces with multiple peaks and valleys.\n",
    "- **Faster Convergence**: NAG often converges faster than simple momentum-based methods because it reduces unnecessary steps around the optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the example of NAG compared to Momentum:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Momentum:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Momentum](./img/LR_momentum_contours.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***NAG:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NAG](./img/LR_nag_contours.gif)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
