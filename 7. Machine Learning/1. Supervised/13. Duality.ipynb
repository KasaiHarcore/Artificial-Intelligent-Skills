{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Notebook 14, we got acquainted with the concepts of convex sets and convex functions. Next, in Notebook 15, I also presented convex optimization problems, how to identify them and how to use libraries to solve basic convex problems. In this lesson, we will continue to approach in more depth: the conditions on the solutions of optimization problems, both convex and non-convex; dual problems and KKT conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we start with simple techniques for basic problems. You may have heard of this technique: the method of Lagrange multipliers. This is a method to find the extreme points of the objective function on the feasible set of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall math problem with constraint:\n",
    "\n",
    "$$x = \\arg \\min_x f_0(x) \\\\\n",
    "\\text{ subject to } f_1(x) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lagrange methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we can reduce this problem to an unconstrained problem, we can find the solution by solving the system of derivative equations with each component equal to 0 (assuming that solving this system of equations is feasible)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is the main motivation for mathematician Joseph-Louis Lagrange to develop the method of Lagrange multipliers $\\mathcal{L}(x, \\lambda) = f_0(x) + \\lambda_1(x)$. Attention that, in this function, we have a new variable $\\lambda$, which is the Lagrange multiplier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathcal{L}(x, \\lambda)$ is called the auxiliary function or the Lagrangian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above problem with constraint already been proved that the *optimal value* already satisfy $\\nabla_{x, \\lambda} \\mathcal{L}(x, \\lambda) = 0$. (I will skip this part). Thus:\n",
    "\n",
    "$$\n",
    "\\nabla_x f_0(x) + \\lambda \\nabla_x f_1(x) = 0 \\\\\n",
    "f_1(x) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving equation above, in general, it's more simple than solving the original problem (with constraint). Let's see an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1**: Find the maximum and minimum values of $f_0(x, y) = x + y$ satisfying the constraint $f_1(x, y) = x^2 + y^2 = 2$. You will quickly notice that this is not a convex optimization problem because feasible set $x^2 + y^2 = 2$ is not a convex set. But we can still use the Lagrange method to solve this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lagragian of this prblem is: $\\mathcal{L}(x, y, \\lambda) = x + y + \\lambda(x^2 + y^2 - 2)$. The extreme points of the Lagrange function must satisfy the condition:\n",
    "\n",
    "$$\n",
    "\\nabla_{x, y, \\lambda} \\mathcal{L}(x, y, \\lambda) = 0 \\iff\n",
    "\\begin{cases}\n",
    "    1 + 2\\lambda x = 0 \\\\\n",
    "    1 + 2\\lambda y = 0 \\\\\n",
    "    x^2 + y^2 = 2\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With first two cases, you can find $x = y = \\frac{-1}{2\\lambda}$, then replace in the last cases, we will solve $\\lambda^2 = \\frac{1}{4}$. Thus, we have two pairs of solution $(x, y) \\in \\{(1, 1), (-1, -1)\\}$. By replacing these values in the original function, we can find the maximum and minimum values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2** Cross-Entropy. In Softmax Regression Notebook, we already know that the cross-entropy loss. We have also seen that the cross entropy function is used to measure the similarity of two probability distributions, with the smaller the value of this function, the closer the two probabilities are to each other. We have also stated that the minimum value of the cross entropy function is achieved when each pair of probabilities is similar. Now, let me restate and prove the above statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given distributions $p = [p_1, p_2, \\cdots, p_n]$ and $q = [q_1, q_2, \\cdots, q_n]$, with $p_i \\in [0, 1]$ and $\\sum_{i=1}^n p_i = 1$ and $q_i \\neq 0, \\forall i$. The cross-entropy function is defined as:\n",
    "\n",
    "$$f_0(q) = -\\sum_{i=1}^n p_i \\log q_i$$\n",
    "\n",
    "Find Q that minimizes $f_0(q)$ subject to $\\sum_{i=1}^n q_i = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, we have a constraint $\\sum_{i=1}^n q_i = 1$. We can use the Lagrange method to solve this problem. The Lagrangian of this problem is:\n",
    "\n",
    "$$\\mathcal{L}(q, \\lambda) = -\\sum_{i=1}^n p_i \\log q_i + \\lambda(\\sum_{i=1}^n q_i - 1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With:\n",
    "\n",
    "$$\n",
    "\\nabla_q \\mathcal{L}(q, \\lambda) = 0 \\iff\n",
    "\\begin{cases}\n",
    "    -\\frac{p_i}{q_i} + \\lambda = 0, \\forall i \\\\\n",
    "    \\sum_{i=1}^n q_i = 1\n",
    "\\end{cases}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From first case, we have $p_i = \\lambda q_i$. Then:\n",
    "\n",
    "$$1 = \\sum_{i=1}^n p_i = \\lambda \\sum_{i=1}^n q_i = \\lambda$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that $q_i = p_i, \\forall i$. Thus, the minimum value of the cross-entropy function is achieved when $q = p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Lagrange dual function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lagrangian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allow me to recall the problem 1 more time (I'm pretty sure you are tired of this problem):\n",
    "\n",
    "$$\\begin{aligned}\n",
    "x = \\arg \\min_x f_0(x) \\\\\n",
    "\\text{ subject to } f_i(x) \\leq 0, i = 1, 2, \\cdots, m \\\\\n",
    "h_j(x) = 0, j = 1, 2, \\cdots, p\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With defined domain $\\mathcal{D} = (\\cap_{i=0}^m \\text{dom} f_i) \\cap (\\cap_{j=1}^p \\text{dom} h_j)$. Attention that, we are not assuming convexity of the optimization function or the constraint functions here. The only assumption here is that $\\mathcal{D} \\neq \\emptyset$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lagrangian is also constructed similarly with each Lagrange multiplier for a (non) constrained equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathcal{L}(x, \\lambda, \\nu) = f_0(x) + \\sum_{i=1}^m \\lambda_i f_i(x) + \\sum_{j=1}^p \\nu_j h_j(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With $\\lambda = [\\lambda_1, \\lambda_2, \\cdots, \\lambda_m]$; $\\nu = [\\nu_1, \\nu_2, \\cdots, \\nu_p]$ is vectors, called by *dual variables* or *Lagrange multipliers vectors*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if $x \\in \\R^n$ then the summary of variable of this function is $n + m + p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lagrangian dual function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dual function Lagrange of optimization problem is a function of dual variables, defined as the smallest value according to $x$ of the Lagrangian:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$g(\\lambda, \\nu) = \\inf_{x \\in \\mathcal{D}} \\mathcal{L}(x, \\lambda, \\nu) \\\\\n",
    "= \\inf_{x \\in \\mathcal{D}} (f_0(x) + \\sum_{i = 1}^{m} \\lambda_i f_i(x) + \\sum_{j = 1}^{p} \\nu_jh_j(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    "- $\\inf$ defined $x \\in \\mathcal{D}$, thus the domain of determination of the problem. This one different from the feasible set. Usually, the feasible set is a subset of the domain $\\mathcal{D}$.\n",
    "- With each $x$, Lagrangian is a affine of $(\\lambda, \\nu)$, which concave function. => dual function is pointwise infimum of concave functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Not Done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
