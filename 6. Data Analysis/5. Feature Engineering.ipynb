{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feauture Engineering Introduction\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "In high-dimensional data sets, model training and prediction require a lot of computational costs. Therefore, feature extraction is a technique that helps reduce the dimensionality of data that allows us to select or combine input variables into predictive features but still accurately and intactly represent the data. original data. Feature extraction is applied in many different machine learning problems.\n",
    "\n",
    "- **Autoendcoder**: A quite effective technique in self - supervised learning. This technique will automatically encode the input data from a high-dimensional space to a low-dimensional space (encoder process). Then decode back from low-dimensional space to high-dimensional space (decoder process) so that the output information of the decoding process and the input must be approximately equal.\n",
    "\n",
    "- **Bag-of-Words**: Also known as the bag-of-words algorithm, it is often used in natural language processing (NLP) and information retrieval. The algorithm allows us to extract information from text segments, news items, and web pages by building a bag of words and trying to encode the text content into a vector of word frequencies without regard to word order and grammatical structure.\n",
    "\n",
    "- **Image Processing**: These are algorithms used to detect features on images such as shapes and edges. These can be manual image feature extraction methods such as HOG and SHIFT or using feature extractors through CNN convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Transformation\n",
    "\n",
    "Feature Transformation are techniques that help transform input data into data suitable for the research model. These data are often highly correlated with respect to the target variable and thus help improve model accuracy. Below are some main methods applied in characteristic variation:\n",
    "- **Variable normalization**: Variable normalization aims to create unit uniformity between input variables and minimize negative impacts on the model due to differences in magnitude between variables. Techniques related to unit standardization for input variables are also known as Feature Scaling including: MinMax normalization (Minmax scaling), unit length normalization (Unit length scaling), normal distribution normalization (Standardization).\n",
    "\n",
    "- **Transform variables by function**: In case the data has heteroscedasticity, we can use some functions to transform the input variable to create variables with stable variance and a distribution close to more normal distribution like logrith, square root, cube root.\n",
    "\n",
    "- **Create interaction variables**: Interaction variables are variables that combine multiple input variables such as $x_1*x_2$, $x_1^2*x2$, $x_1*x_2^2$. The interaction variable can be the product of two or more variables. In a model with few input variables, using interaction variables can help create many new explanatory variables that help the model.\n",
    "\n",
    "- **Creating high-order variables**: High-order variables are variables created from input variables by exponentiation with higher-order values, which can be of order 2, 3, etc. For example, the input variable is $x_1$, the high-order variable is $x_1^2$, $x_1^3$.\n",
    "\n",
    "- **Geographic location data**: From geographical location, it is possible to infer region, urban, rural, average income level, demographic factors, etc.\n",
    "\n",
    "- **Time data**: Time series data often has cyclical and seasonal characteristics. Therefore, transformation techniques that turn time into a characteristic that captures cyclical and seasonal properties will help enhance the model's explanatory ability for the target variable. We can choose the time cycle of morning/afternoon/evening during the day; Day in month; week of the month; month of the year or quarter of the year depending on the seasonal rules expressed in the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "Feature selection is a very important part of Machine Learning with the main goal being to eliminate features that do not really contain useful information for the classification or prediction problem. Feature selection techniques can be used to improve training and prediction speed (fewer features mean the model is trained and predicted faster) and even reduce overfitting.\n",
    "\n",
    "Feature selection techniques are quite diverse:\n",
    "- **Use the correlation coefficient with the target variable**: Variables that are highly correlated with the target variable are variables with good explanatory power. Variable importance can be ranked using Pearson Correlation,\n",
    "\n",
    "- **Using AIC index**: AIC (Akaike information criterion) is an index used to evaluate the quality of a statistical model. This index is calculated through the logarithm value of the likelihood function (Log Likelihood Function). To rank the importance of variables, we will first calculate the AIC for the model regressed on all variables. Then perform training experiments, removing one variable each time to see which model's AIC value is the smallest. The smaller the AIC, the lower the error the model has on the training set and thus the variable ranking.\n",
    "\n",
    "- **Using the IV index**: IV (Information Value) is an index used in binary classification problems in statistics. This index is often measured to evaluate the classification power of the input variable.\n",
    "\n",
    "- **Feature selection using models**: Random Forest, Lasso Regression, Neural Network, SVD.\n",
    "\n",
    "- **Selection based on the level of variance fluctuation**: Variables that have little fluctuation or even no change in value will not have the effect of classification and prediction. Therefore, we can filter out these variables by determining that the magnitude of the variance must be greater than a given threshold.\n",
    "\n",
    "Next, we will analyze these techniques in theory and application cases through practical examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction for Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text data can exist in many different forms such as lowercase letters, uppercase letters, punctuation marks, special characters, etc. Different languages ​​also have different character patterns and different grammatical structures.\n",
    "\n",
    "The main problem with text data is how to encode characters into numbers? Tokenization technique will help us do this. Tokenization is when we divide text into the smallest units and build an index dictionary for these units. There are two main types of encoding: ***word encoding*** and ***character encoding***.\n",
    "\n",
    "- For word-based encoding, the words in the sentence will be the smallest unit. In English, words mainly exist in single word form while in Vietnamese there are compound words. When encoding by word, the size of the dictionary will be very large, depending on the number of different words appearing in the entire text.\n",
    "\n",
    "- Encoding by character, we will use symbols in the alphabet to make a word encoding dictionary. The size of the dictionary when encoding by character will be smaller than when encoding by word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag-of-Words Methods\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bag-of-words, abbreviated as BoW, According to the bag-of-word method, we will encode the words in the sentence into a vector whose length is equal to the number of words in the dictionary and count the frequency of occurrence of the words. The frequency of the $i^{th}$ word in the dictionary will be equal to the $i^{th}$ element in the vector."
   ]
  },
  {
   "attachments": {
    "BoW.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BoW.png](attachment:BoW.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above picture, the text on the left is encoded into the word frequency vector on the right. The words “I” and have are repeated twice so have a frequency of 2. Words that do not appear in the sentence but are in the dictionary such as “deep”, “is”, “this”, “machine”, “learning” have a value of 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, according to the bag-of-words method, each word will become a dimension represented in the space of the output vector. When the number of words is very large, the encoding result can form a vector of very large length. Usually this will be a sparse vector with most values ​​equal to 0. The large number of dimensions makes it difficult to represent coding vectors in space. If we want to represent it on a graph, we must find a way to reduce the vector dimension to 2 or 3 dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code illustrating the bag of words method. To build the bag-of-words method, we go through two steps:\n",
    "- Build a dictionary.\n",
    "\n",
    "- Encode text into word frequency vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 1. 1. 0. 0. 1.]\n",
      "[1. 1. 1. 0. 0. 1. 0. 0.]\n",
      "[1. 2. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "import numpy as np\n",
    "\n",
    "# The input is a text containing 3 sentences:\n",
    "texts = [['i', 'have', 'a', 'cat'],\n",
    "        ['he', 'has', 'a', 'dog'],\n",
    "        ['he', 'has', 'a', 'dog', 'and', 'i', 'have', 'a', 'cat']]\n",
    "\n",
    "#B1: Build a dictionary\n",
    "dictionary = list(enumerate(set(reduce(lambda x, y: x + y, texts))))\n",
    "\n",
    "# B2: Encode sentences into frequency vectors\n",
    "def bag_of_word(sentence):\n",
    "    # Initialize a vector with length equal to the dictionary.\n",
    "    vector = np.zeros(len(dictionary))\n",
    "    # Count the words in a sentence that appear in the dictionary.\n",
    "    for i, word in dictionary:\n",
    "        count = 0\n",
    "        # Count the number of words appearing in a sentence.\n",
    "        for w in sentence:\n",
    "            if w == word:\n",
    "                count += 1\n",
    "            vector[i] = count\n",
    "    return vector\n",
    "\n",
    "for i in texts:\n",
    "    print(bag_of_word(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to use the library to find bag-of-words representations of words, in sklearn we use the package as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words in dictionary:  ['and' 'cat' 'dog' 'has' 'have' 'he']\n",
      "[[0 1 0 0 1 0]\n",
      " [0 0 1 1 0 1]\n",
      " [1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "texts = ['i have a cat', \n",
    "        'he has a dog', \n",
    "        'he has a dog and i have a cat']\n",
    "\n",
    "vect = CountVectorizer()\n",
    "X = vect.fit_transform(texts)\n",
    "print('words in dictionary: ', vect.get_feature_names_out())\n",
    "print(X.toarray())"
   ]
  },
  {
   "attachments": {
    "BoW_diagram.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process can be described by the diagram below:\n",
    "\n",
    "![BoW_diagram.png](attachment:BoW_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The limitation of bag-of-words representations is that we cannot distinguish between two sentences with the same words because bag-of-words do not distinguish between the front and back order of words in a sentence. Phrases like \"you have no dog\" and \"no, you have dog\" are two sentences that have the same performance even though they have opposite meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1],\n",
       "       [1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer(ngram_range = (1, 1))\n",
    "vect.fit_transform(['you have no dog', 'no, you have dog']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's why the bag-of-n-gram method will be used instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag-of-N-grams Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bag-of-n-grams method is an extension of bag-of-words. An n-grams is a string consisting of $n$ tokens. In case $n = 1$ word we call it unigram, for 2 words it is bigram and for 3 words it is trigram. When performing tokenization with n-grams, clusters of $n - grams$ of words will appear in the dictionary if they appear in the documents. For example, the sentence “I have a dog” will be tokenized into “I have”, “have a”, “a dog”. Thus the number of words in the dictionary will increase significantly. If we have $k$ single words then we can have up to $k^2$ words in the bigram. \n",
    "\n",
    "But in reality, not most words can be paired with each other, so the representation vector of a sentence in a bigram is a very sparse vector with a large dimension. This leads to costly computation and storage costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In sklearn, to use bigram, in <code>CountVectorizer</code> we change <code>ngram_range = (2, 2)</code>. The first value is the minimum length and the latter value is the maximum allowed length of *ngrams*. Here we declare the smallest and largest lengths to be 2, so we get *ngrams* as *bigrams*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# bigram\n",
    "bigram = CountVectorizer(ngram_range = (2, 2))\n",
    "n1, n2, n3 = bigram.fit_transform(['you have no dog', 'no, you have dog', 'you have a dog']).toarray()\n",
    "\n",
    "# trigram\n",
    "trigram = CountVectorizer(ngram_range = (3, 3))\n",
    "n1, n2, n3 = trigram.fit_transform(['you have no dog', 'no, you have dog', 'you have a dog']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After encoding the sentences we can also calculate the distance between vectors in euclidean space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0 1.0 1.7320508075688772\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "print(euclidean(n1, n2), euclidean(n2, n3), euclidean(n1, n3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF Method\n",
    "\n",
    "Suppose we have a corpus consisting of many sub-texts. Words that are rarely found in the corpus but are present in certain topics may play a more important role. For example, for the Family topic, words like “parents”, “grandparents”, “children”, “brothers”, and “sisters” appear more often than other topics.\n",
    "\n",
    "There are also words that appear a lot in the text, but they appear in almost every topic, every text, such as “the”, “a”, “an”. Such words are called stopwords because they do not have much significance for text classification. When encoding language, we will find ways to remove stopwords by using a dictionary with important stopwords.\n",
    "\n",
    "The TF-IDF method is a method in which we give greater weight to words that appear in some specific texts through the formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{idf}(t, D) = \\log{\\frac{\\left|D\\right|}{\\left|d \\in D: t \\in d\\right|}} = \\log{\\frac{\\left|D\\right|}{df(d, t) + 1}}$$\n",
    "$$\\text{tfidf}(t, d, D) = \\text{tf}(t, d) \\times \\text{idf}(t, D)$$\n",
    "\n",
    "Where:\n",
    "- $\\left|D\\right|$ is the number of texts in the corpus.\n",
    "- $\\text{df}(d, t) = \\left|{d \\in D; t \\in d}\\right|$ is the number of text $d \\in D$ that contain the word $t$.\n",
    "- $\\text{tf}(t, d)$ is the frequency of the word $t$ in the text $d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus $\\text{idf}(t, D)$ is the inverse document frequency index. This index is equal to the logarithm of the inverse of the number of documents divided by the number of documents containing a specific word $i$ . A particular word has a large $\\text{idf}(t, D)$ indicating that the word appears in only a small number of texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{tfidf}(t, d, D)$ is proportional to the frequency of the word appearing in the text and inversely proportional to the frequency of the text. We can explain the meaning of $\\text{tfidf}$ for assessing the importance of words as follows: The more important a word is, the more often it will appear in a specific text, such as The text term $d$ is large, i.e. $\\text{tf}(t, d)$ is large; At the same time, the word must not be *stopwords*, that is, the number of texts it appears in the entire small text set, meaning $\\text{idf}(t, D)$ must be large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To encode text based on the tfidf method, we use the sklearn package as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words in dictionary:  ['afternoon' 'again' 'an' 'and' 'bread' 'bring' 'eat' 'evening' 'falling'\n",
      " 'forecast' 'future' 'go' 'grow' 'hanoi' 'has' 'in' 'like' 'likes' 'll'\n",
      " 'makes' 'market' 'me' 'meat' 'my' 'near' 'out' 'portfolio' 'rain'\n",
      " 'recover' 'rice' 'sandwiches' 'she' 'sticky' 'stock' 'stocks' 'the' 'to'\n",
      " 'umbrella' 'weather' 'when' 'will' 'worried']\n",
      "X shape:  (5, 42)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"I like to eat meat sandwiches\",\n",
    "    \"She likes to eat bread, and I like to eat sticky rice\",\n",
    "    \"the falling stock market makes me worried\",\n",
    "    \"Stocks will recover in the near future. my portfolio will grow again\",\n",
    "    \"Hanoi weather forecast has rain in the afternoon and evening. I'll bring an umbrella when I go out\"\n",
    "]\n",
    "\n",
    "# Calculate tfidf for each word. max_df to remove stopwords that appear in more than 90% of sentences\n",
    "vectorizer = TfidfVectorizer(max_df = 0.9)\n",
    "# Tokenize and build the vocabulary\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(\"words in dictionary: \", vectorizer.get_feature_names_out())\n",
    "print(\"X shape: \", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the word \"I\" appears in all sentences and does not carry much meaning of the topic of the sentence, so it can be considered a stopword. By filtering the upper bound of the word's frequency in the text at 90%, we have removed this word from the dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pocket methods can find some contests on kaggle such as <a class=\"reference external\" href=\"https://www.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking\">Catch me if you can competition</a>, <a class=\"reference external\" href=\"https://www.kaggle.com/xiaoml/bag-of-app-id-python-2-27392\">bag of app</a>, <a class=\"reference external\" href=\"http://www.interdigital.com/download/58540a46e3b9659c9f000372\">bag of event</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located close to another in the space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec is a family of models used to create embedded representations of words. These models are relatively shallow, consisting only of 2-layer neural networks trained to reconstruct the linguistic context for words. Through the word2vec model, each word in a set of documents is represented through a vector in a high-dimensional space, which can be up to hundreds of dimensions, so that words with the same context will be placed closer together in the word2vec model. space.\n",
    "\n",
    "For example, below is an example that after performing word encoding through the word2vec model, the words \"king\", \"queen\", \"man\", \"woman\" have a relationship according to the formula: king - man + woman = queen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a specific position in the sentence we will identify a target word and context words. The target word is the word in the selected position and the context words are the words in the surrounding position that help create a semantic context for the target word.\n",
    "\n",
    "Suppose we have a sentence like this: “I want a blue cup.” If we select a *context window* that includes 3 adjacent words, we will in turn obtain sets of 3 words: “I want one”, “want one”, “a cup”, “colored cup”, “blue cup”. For these sets of three words, the words in the middle will be the target words and the context words will be the words at the beginning and at the end. So we will have pairs of target words and contexts as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "[(('I', 'one'), 'want'), (('want', 'piece'), 'one'), (('one', 'cup'), 'piece'), ( ('cup', 'color'), 'cup'), (('cup', 'blue'), 'color')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word2vec model has two main methods are skip-grams and CBOW as follows:"
   ]
  },
  {
   "attachments": {
    "CBOW & Skip-gram.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CBOW & Skip-gram.png](<attachment:CBOW & Skip-gram.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CBOW Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the CBOW model, we will build a supervised learning model using context words as input, such as in the picture the words $w_{t - 2}, w_{t - 1}, w_{t + 1}, w_{t + 2}$ to explain the target word in current position as ${w_t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words $w_t$ are encoded as one-hot vectors in a $\\mathbb{R}^d$ dimensional space to be used for training. Here, $d$ is the size of the dictionary. Thus, in the CBOW method, we have 5 input one-hot vectors with the number of dimensions equal to the number of words in the dictionary. These vectors are then reduced in dimensionality through a projection onto a lower-dimensional space, for example, 200 dimensions; this step is called the projection in the image. The result is an embedding vector $e_c \\in \\mathbb{R}^{200}$. Finally, the probability distribution of the target word is predicted through a softmax function applied to the vector $e_c$. The model training process will be based on the cross-entropy form of the softmax function.\n",
    "\n",
    "$$\\mathcal{L}(\\mathbf{y}, \\hat{\\mathbf{y}}) = -\\sum_{i=1}^{d} y_i\\log(\\hat{y}_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $\\hat{\\mathbf{y}}$ is the probability of predicting the target word corresponding to the word at index position $i$ in the dictionary, calculated according to the softmax formula:\n",
    "\n",
    "$$\\hat{y_i} = \\frac{\\exp(\\mathbf{w}_{:i}^{\\intercal}\\mathbf{e}_c)}{\\sum_{i=1}^{d}\\exp(\\mathbf{w}_{:i}^{\\intercal}\\mathbf{e}_c)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{w}_{:i} \\in \\mathbb{R}^{200}$ is the parameter vector connecting all nodes of $\\mathbf{e}_c$ to the $i^{th}$ node position of the last layer.\n",
    "\n",
    "After the forward and back propagation process, the model coefficients will be updated and we will obtain a more accurate word representation. An input word will be represented through the CBOW method as the vector $\\mathbf{e}_c$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\nguye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nguye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 13759\n",
      "Vocabulary Sample: [('PAD', 0), ('[', 1), ('The', 2), ('King', 3), ('James', 4), ('Bible', 5), (']', 6), ('Old', 7), ('Testament', 8), ('of', 9)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from string import punctuation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Download Gutenberg corpus\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')\n",
    "norm_bible = gutenberg.sents('bible-kjv.txt') \n",
    "norm_bible = [' '.join(doc) for doc in norm_bible]\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = nltk.tokenize.word_tokenize\n",
    "tokens = [tokenizer(doc) for doc in norm_bible]\n",
    "\n",
    "# Build vocabulary\n",
    "word2id = {'PAD': 0}\n",
    "for sent in tokens:\n",
    "    for word in sent:\n",
    "        if word not in word2id:\n",
    "            word2id[word] = len(word2id)\n",
    "\n",
    "id2word = {v: k for k, v in word2id.items()}\n",
    "vocab_size = len(word2id)\n",
    "\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Vocabulary Sample:', list(word2id.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding sentence by index:  [[1, 2, 3, 4, 5, 6], [2, 7, 8, 9, 10, 3, 4, 5], [2, 11, 12, 9, 13, 14, 15, 16], [17, 14, 17, 18, 10, 19, 20, 21, 10, 22, 23, 10, 24, 25], [17, 14, 26, 27, 10, 24, 28, 29, 30, 31, 23, 32, 33, 23, 34, 28, 35, 10, 36, 9, 10, 37, 25]]\n"
     ]
    }
   ],
   "source": [
    "# Encode sentences into indices\n",
    "wids = [[word2id[w] for w in doc] for doc in tokens]\n",
    "print('Embedding sentence by index: ', wids[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context (X): ['[', 'The', 'James', 'Bible'] -> Target (Y): King\n",
      "Context (X): ['The', 'King', 'Bible', ']'] -> Target (Y): James\n",
      "Context (X): ['The', 'Old', 'of', 'the'] -> Target (Y): Testament\n",
      "Context (X): ['Old', 'Testament', 'the', 'King'] -> Target (Y): of\n",
      "Context (X): ['Testament', 'of', 'King', 'James'] -> Target (Y): the\n",
      "Context (X): ['of', 'the', 'James', 'Bible'] -> Target (Y): King\n",
      "Context (X): ['The', 'First', 'of', 'Moses'] -> Target (Y): Book\n",
      "Context (X): ['First', 'Book', 'Moses', ':'] -> Target (Y): of\n",
      "Context (X): ['Book', 'of', ':', 'Called'] -> Target (Y): Moses\n",
      "Context (X): ['of', 'Moses', 'Called', 'Genesis'] -> Target (Y): :\n",
      "Context (X): ['1', ':', 'In', 'the'] -> Target (Y): 1\n"
     ]
    }
   ],
   "source": [
    "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
    "    context_length = window_size * 2\n",
    "    for words in corpus:\n",
    "        sentence_length = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            context_words = []\n",
    "            label_word = word\n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "            context_words = [words[i] for i in range(start, end) if 0 <= i < sentence_length and i != index]\n",
    "            x = torch.nn.functional.pad(torch.tensor(context_words), (0, context_length - len(context_words)), 'constant', 0)\n",
    "            y = torch.nn.functional.one_hot(torch.tensor(label_word), num_classes=vocab_size)\n",
    "            yield x, y\n",
    "\n",
    "# Test this out for some samples\n",
    "i = 0\n",
    "window_size = 2  # context window size\n",
    "for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "    if 0 not in x:\n",
    "        print('Context (X):', [id2word[w.item()] for w in x], '-> Target (Y):', id2word[y.argmax().item()])\n",
    "        if i == 10:\n",
    "            break\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 521.7630507946014\n",
      "Epoch 2, Loss: 258.8592987060547\n",
      "Epoch 3, Loss: 175.8490710258484\n",
      "Epoch 4, Loss: 127.66212940216064\n",
      "Epoch 5, Loss: 98.59629130363464\n",
      "Epoch 6, Loss: 79.42006278038025\n",
      "Epoch 7, Loss: 65.96231380105019\n",
      "Epoch 8, Loss: 57.537843093276024\n",
      "Epoch 9, Loss: 51.170799881219864\n",
      "Epoch 10, Loss: 46.32542020827532\n",
      "Epoch 11, Loss: 42.1400792747736\n",
      "Epoch 12, Loss: 39.87142910063267\n",
      "Epoch 13, Loss: 38.94939765334129\n",
      "Epoch 14, Loss: 36.24399475753307\n",
      "Epoch 15, Loss: 35.20298230648041\n",
      "Epoch 16, Loss: 33.837794318795204\n",
      "Epoch 17, Loss: 32.28717716410756\n",
      "Epoch 18, Loss: 31.784702736884356\n",
      "Epoch 19, Loss: 30.50809258967638\n",
      "Epoch 20, Loss: 31.09312973730266\n",
      "Epoch 21, Loss: 29.649346569553018\n",
      "Epoch 22, Loss: 28.88797376677394\n",
      "Epoch 23, Loss: 28.642189282923937\n",
      "Epoch 24, Loss: 28.109546829015017\n",
      "Epoch 25, Loss: 27.68133505806327\n",
      "Epoch 26, Loss: 27.250855254009366\n",
      "Epoch 27, Loss: 27.445099033415318\n",
      "Epoch 28, Loss: 27.152747373096645\n",
      "Epoch 29, Loss: 26.192441053688526\n",
      "Epoch 30, Loss: 25.583230759948492\n",
      "Epoch 31, Loss: 25.82633836567402\n",
      "Epoch 32, Loss: 26.32203298714012\n",
      "Epoch 33, Loss: 25.7085651922971\n",
      "Epoch 34, Loss: 25.584082088433206\n",
      "Epoch 35, Loss: 25.2551154892426\n",
      "Epoch 36, Loss: 25.583719401154667\n",
      "Epoch 37, Loss: 24.536601105704904\n",
      "Epoch 38, Loss: 24.364565581083298\n",
      "Epoch 39, Loss: 24.444814370013773\n",
      "Epoch 40, Loss: 25.083736917935312\n",
      "Epoch 41, Loss: 25.285710211843252\n",
      "Epoch 42, Loss: 25.078666555695236\n",
      "Epoch 43, Loss: 23.605303733609617\n",
      "Epoch 44, Loss: 23.824972248636186\n",
      "Epoch 45, Loss: 24.306566534098238\n",
      "Epoch 46, Loss: 24.33940608194098\n",
      "Epoch 47, Loss: 23.579033307963982\n",
      "Epoch 48, Loss: 23.757173392223194\n",
      "Epoch 49, Loss: 24.126189784146845\n",
      "Epoch 50, Loss: 23.312756803818047\n",
      "Epoch 51, Loss: 23.2806972656399\n",
      "Epoch 52, Loss: 22.97486264584586\n",
      "Epoch 53, Loss: 24.1032747910358\n",
      "Epoch 54, Loss: 23.07327963411808\n",
      "Epoch 55, Loss: 22.86154165584594\n",
      "Epoch 56, Loss: 23.046054095961154\n",
      "Epoch 57, Loss: 22.168097631074488\n",
      "Epoch 58, Loss: 23.206271091476083\n",
      "Epoch 59, Loss: 22.939396440051496\n",
      "Epoch 60, Loss: 22.3398771523498\n",
      "Epoch 61, Loss: 23.105250456370413\n",
      "Epoch 62, Loss: 22.521727619692683\n",
      "Epoch 63, Loss: 22.937966872239485\n",
      "Epoch 64, Loss: 23.019630966708064\n",
      "Epoch 65, Loss: 22.37909762794152\n",
      "Epoch 66, Loss: 22.79073278978467\n",
      "Epoch 67, Loss: 22.236009256914258\n",
      "Epoch 68, Loss: 22.397723596775904\n",
      "Epoch 69, Loss: 22.46013922104612\n",
      "Epoch 70, Loss: 22.20298509299755\n",
      "Epoch 71, Loss: 22.036096598487347\n",
      "Epoch 72, Loss: 21.303327951347455\n",
      "Epoch 73, Loss: 22.379522517323494\n",
      "Epoch 74, Loss: 22.031650170567445\n",
      "Epoch 75, Loss: 22.001350989565253\n",
      "Epoch 76, Loss: 21.755659321555868\n",
      "Epoch 77, Loss: 21.382409617304802\n",
      "Epoch 78, Loss: 22.423442932777107\n",
      "Epoch 79, Loss: 21.57115980051458\n",
      "Epoch 80, Loss: 21.335325894877315\n",
      "Epoch 81, Loss: 22.109425828326494\n",
      "Epoch 82, Loss: 21.05359651055187\n",
      "Epoch 83, Loss: 21.09740994265303\n",
      "Epoch 84, Loss: 21.302807786501944\n",
      "Epoch 85, Loss: 21.159005966037512\n",
      "Epoch 86, Loss: 21.51202405616641\n",
      "Epoch 87, Loss: 21.411255761049688\n",
      "Epoch 88, Loss: 20.711610765894875\n",
      "Epoch 89, Loss: 20.958973811473697\n",
      "Epoch 90, Loss: 20.845837587490678\n",
      "Epoch 91, Loss: 22.110196148976684\n",
      "Epoch 92, Loss: 20.63083148561418\n",
      "Epoch 93, Loss: 21.43801146140322\n",
      "Epoch 94, Loss: 21.140361468540505\n",
      "Epoch 95, Loss: 21.091960958205163\n",
      "Epoch 96, Loss: 21.346855343319476\n",
      "Epoch 97, Loss: 21.54373129364103\n",
      "Epoch 98, Loss: 20.711981161963195\n",
      "Epoch 99, Loss: 20.43649818841368\n",
      "CBOW(\n",
      "  (embedding): Embedding(13759, 100)\n",
      "  (linear1): Linear(in_features=100, out_features=13759, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define the CBOW model using PyTorch\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.linear1 = nn.Linear(embed_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        avg_embeds = embeds.mean(dim=1)\n",
    "        out = self.linear1(avg_embeds)\n",
    "        log_probs = torch.nn.functional.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "embed_size = 100\n",
    "model = CBOW(vocab_size, embed_size)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
    "\n",
    "# Prepare data for training\n",
    "class ContextTargetDataset(Dataset):\n",
    "    def __init__(self, context_target_pairs):\n",
    "        self.pairs = list(context_target_pairs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.pairs[idx]\n",
    "\n",
    "context_target_pairs = list(generate_context_word_pairs(corpus=wids[:100], window_size=window_size, vocab_size=vocab_size))\n",
    "dataset = ContextTargetDataset(context_target_pairs)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(1, 100):\n",
    "    total_loss = 0\n",
    "    for context, target in dataloader:\n",
    "        model.zero_grad()\n",
    "        log_probs = model(context)\n",
    "        loss = loss_function(log_probs, target.argmax(dim=1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch}, Loss: {total_loss}')\n",
    "\n",
    "# View model summary\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skip-gram Methods\n",
    "\n",
    "The skip-gram method is essentially an inverted version of the CBOW method. We will use the target words as input and predict the context words that project to the target word. As shown in the figure, $\\mathbf{w}_t$ is the target word used as input, the words $\\mathbf{w}_{t-2}, \\mathbf{w}_{t- 1}, \\mathbf{w}_{t+1}, \\mathbf{w}_{t+2}$ are context words that need to be predicted. These words are all encoded into one-hot vectors in the space $\\mathbb{R}^{d}$ Then the one-hot vector will be projected into the space to reduce the data dimension to, for example, 200 afternoon. The output is the vector $\\mathbf{e}_c$ of size 200, which is also the embedded representation of the word in the skip-gram. Finally we use a sigmoid layer to predict whether the target word $\\mathbf{w}_t$ and the background word $\\mathbf{w}_j$ ($\\mathbf{w}_j$ are randomly selected from the dictionary) have the same context?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "# Function to generate skip-grams\n",
    "def generate_skipgrams(wids, vocab_size, window_size):\n",
    "    skip_grams = []\n",
    "    for idx, word in enumerate(wids):\n",
    "        for neighbor in range(max(idx - window_size, 0), min(idx + window_size + 1, len(wids))):\n",
    "            if neighbor != idx:\n",
    "                skip_grams.append((word, wids[neighbor]))\n",
    "    return skip_grams\n",
    "\n",
    "# Example data\n",
    "wids = [1, 2, 3, 4, 5, 6]  # Example word IDs\n",
    "vocab_size = 10\n",
    "window_size = 2\n",
    "\n",
    "skip_grams = generate_skipgrams(wids[:100], vocab_size, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(1, 3)\n",
      "(2, 1)\n",
      "(2, 3)\n",
      "(2, 4)\n",
      "(3, 1)\n",
      "(3, 2)\n",
      "(3, 4)\n",
      "(3, 5)\n",
      "(4, 2)\n"
     ]
    }
   ],
   "source": [
    "# Sample skip-grams\n",
    "pairs = skip_grams[:10]\n",
    "for i in range(10):\n",
    "    print(f\"({pairs[i][0]}, {pairs[i][1]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SkipGramModel(\n",
      "  (word_embeddings): Embedding(10, 50)\n",
      "  (context_embeddings): Embedding(10, 50)\n",
      ")\n",
      "Epoch: 1, Loss: 4.751020431518555\n",
      "Epoch: 2, Loss: 4.4072651863098145\n",
      "Epoch: 3, Loss: 4.169256687164307\n",
      "Epoch: 4, Loss: 3.9775755405426025\n",
      "Epoch: 5, Loss: 3.813333749771118\n",
      "Epoch: 6, Loss: 3.6677236557006836\n",
      "Epoch: 7, Loss: 3.535808563232422\n",
      "Epoch: 8, Loss: 3.414497137069702\n",
      "Epoch: 9, Loss: 3.301711082458496\n",
      "Epoch: 10, Loss: 3.1959786415100098\n",
      "Epoch: 11, Loss: 3.09621524810791\n",
      "Epoch: 12, Loss: 3.001596450805664\n",
      "Epoch: 13, Loss: 2.9114785194396973\n",
      "Epoch: 14, Loss: 2.8253469467163086\n"
     ]
    }
   ],
   "source": [
    "# Skip-gram model in PyTorch\n",
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "        self.context_embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "    def forward(self, word_input, context_input):\n",
    "        word_embed = self.word_embeddings(word_input)\n",
    "        context_embed = self.context_embeddings(context_input)\n",
    "        score = torch.mul(word_embed, context_embed).sum(1)\n",
    "        return torch.sigmoid(score)\n",
    "\n",
    "embed_size = 50\n",
    "model = SkipGramModel(vocab_size, embed_size)\n",
    "\n",
    "print(model)\n",
    "\n",
    "# Training\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.001)\n",
    "\n",
    "# Prepare data for training\n",
    "def get_input_output_pairs(skip_grams):\n",
    "    word_input = torch.tensor([pair[0] for pair in skip_grams], dtype=torch.long)\n",
    "    context_input = torch.tensor([pair[1] for pair in skip_grams], dtype=torch.long)\n",
    "    labels = torch.tensor([1] * len(skip_grams), dtype=torch.float32)  # Assuming all pairs are valid\n",
    "    return word_input, context_input, labels\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, 15):\n",
    "    total_loss = 0\n",
    "    word_input, context_input, labels = get_input_output_pairs(skip_grams[:100])\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(word_input, context_input)\n",
    "    loss = loss_function(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    total_loss += loss.item()\n",
    "    print(f'Epoch: {epoch}, Loss: {total_loss}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the word2vec model using a neural network is so we can better understand the neural network structure and how the network works. In fact, to train the word2vec model we can go through the gensim package as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(209478, 336740)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# Training model with the first 1000 verses in the bible\n",
    "sentences = [[item.lower() for item in doc.split()] for doc in norm_bible[:1000]]\n",
    "model = Word2Vec(sentences, min_count = 1, vector_size = 150, window = 10, sg = 1, workers = 8)\n",
    "model.train(sentences, total_examples = model.corpus_count, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding vector shape:  (150,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.02651765,  0.20900695,  0.2421184 ,  0.13713393, -0.16987683],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('embedding vector shape: ', model.wv['king'].shape)\n",
    "model.wv['king'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction for Image Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the past when computational resources were limited and the “neural network renaissance” had not really returned, feature mining for image data was a complex field. People have to design manual filters to extract features such as corners, edges, horizontal, vertical, diagonal lines, etc. Algorithms such as HOG, SHIFT are commonly used methods to extract features. . The disadvantage of these methods is that the feature extractor and classifier are separated, so the model has slow training and prediction speed.\n",
    "\n",
    "The thawing period of deep learning has caused the CNN network to grow strongly. Modern CNN network architectures are becoming increasingly deeper and achieving high accuracy. These are end-to-end architectures that allow feature extractors to be attached to classifiers in a single pipeline. The filters also do not need to be initialized manually, on the contrary they are randomly generated according to assumed distributions.\n",
    "\n",
    "Thanks to the available resources of pretrained models, you don't need to figure out the architecture and train the network from scratch. Instead, a state-of-the-art network that has been trained with weights can be downloaded from published sources. Data scientists often make adjustments to adapt these networks to their needs by “decoupling” the final fully connected layers of the network, adding newly designed layers. designed for a specific task, and then train the network on new data. If your task is just to vectorize the image, you can simply remove the last layers and use the output from the previous layers as the feature vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we will not focus too much on neural network techniques. Instead, hand-created features are still very useful: for example, for the problem in the Rental Listing Inquiries - Kaggle Competition, to predict the popularity of rental listings, we can assume that the A well-lit apartment will attract more attention and create a new feature like “average pixel value”.\n",
    "\n",
    "Extract text information on images:\n",
    "\n",
    "- OCR (Optical character recognition) is a type of problem that extracts text information from images. They are highly applicable and often yield a lot of information when processing image data.\n",
    "\n",
    "For example, if there is text on an image, you can read it to extract some information through the pytesseract image text detection package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tesseract-ocr # Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "##### Just a random picture from search\n",
    "img = 'http://ohscurrent.org/wp-content/uploads/2015/09/domus-01-google.jpg'\n",
    "img = requests.get(img)\n",
    "\n",
    "img = Image.open(BytesIO(img.content))\n",
    "\n",
    "# show image\n",
    "img_arr = np.array(img)\n",
    "plt.imshow(img_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geographic Data Extraction\n",
    "\n",
    "In python we have a quite popular package for exploiting geographical information, <code>reverse_geocoder</code>. There are two main types of problems with geographic information:\n",
    "\n",
    "- geocoding: encodes a geographic coordinate from an address.\n",
    "\n",
    "- revert geocoding: from the information provided about longitude and latitude, return the address of the location and related information.\n",
    "\n",
    "Both problems can be solved through Google map or OpenStreetMap API. The following is an example of extracting geographic information from a location through longitude and latitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting reverse_geocoder\n",
      "  Downloading reverse_geocoder-1.5.1.tar.gz (2.2 MB)\n",
      "     ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.0/2.2 MB 262.6 kB/s eta 0:00:09\n",
      "     - -------------------------------------- 0.1/2.2 MB 573.4 kB/s eta 0:00:04\n",
      "     --- ------------------------------------ 0.2/2.2 MB 1.1 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 0.4/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 0.8/2.2 MB 3.0 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 1.3/2.2 MB 4.1 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 1.8/2.2 MB 5.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.2/2.2 MB 5.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.11.0 in c:\\users\\nguye\\anaconda3\\lib\\site-packages (from reverse_geocoder) (1.23.5)\n",
      "Requirement already satisfied: scipy>=0.17.1 in c:\\users\\nguye\\anaconda3\\lib\\site-packages (from reverse_geocoder) (1.11.4)\n",
      "Building wheels for collected packages: reverse_geocoder\n",
      "  Building wheel for reverse_geocoder (setup.py): started\n",
      "  Building wheel for reverse_geocoder (setup.py): finished with status 'done'\n",
      "  Created wheel for reverse_geocoder: filename=reverse_geocoder-1.5.1-py3-none-any.whl size=2268081 sha256=94e4f02dc3039b8af5a8dec7a613b3cb535a99beda150b0f63ff6c532de430c9\n",
      "  Stored in directory: c:\\users\\nguye\\appdata\\local\\pip\\cache\\wheels\\17\\3c\\41\\2bc89719586c2a5c53e9a527daa76a968a1288315c1ae2d904\n",
      "Successfully built reverse_geocoder\n",
      "Installing collected packages: reverse_geocoder\n",
      "Successfully installed reverse_geocoder-1.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install reverse_geocoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'lat': '34.0339',\n",
       "  'lon': '-118.20535',\n",
       "  'name': 'Boyle Heights',\n",
       "  'admin1': 'California',\n",
       "  'admin2': 'Los Angeles County',\n",
       "  'cc': 'US'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import reverse_geocoder as revgc\n",
    "\n",
    "# Giving the latitude and longitude\n",
    "revgc.search((34.016596, -118.226501))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, from the coordinates we can know that this apartment is located in Los Angeles, a developed place with a high standard of living. So its price is likely to be higher. From the district and district we can determine whether the apartment is located in the center or not, and the amenities around it. The above information is very important in assessing the sellability of the apartment. Although they do not appear in the original dataset, they can be extracted from geographic coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Data Extraction\n",
    "\n",
    "In forecasting, the data often has a changing state. Yesterday's status may be different from today's. Such as a person's height, weight or the market price of stocks. Therefore, time is an information that has a great influence on the target variable. From a known timeline, we can decompose information into hours of the day, days of the month, months, quarters, years, etc. There will be many interesting things to discover from this information. For example, the rules of some number series change according to seasons: Monthly temperatures change according to seasons, GDP changes according to quarterly rules, ice cream sales change according to seasons, etc. The time factor remains. helps determine the changing trend of a variable over time and combined with seasonality, it becomes an important index for estimating time series.\n",
    "\n",
    "One-hot transformation coding is an important method used to encode time-period variables. One-hot coding will transform a variable into vectors with elements of 0 or 1, where 1 represents the occurrence of the feature and 0 represents features that the variable does not have.\n",
    "\n",
    "For example: We have 1 day of the week that can fall from Monday to Sunday. Thus a one-hot encoding representation of day 2 would be a vector with the first element equal to 1 and the remaining elements equal to 0. This representation is similar to encoding text data into sparse vector.\n",
    "\n",
    "In python we can use the weekday() function to determine the order of a day of the week. The weekday() property only exists for datetime data. Therefore, we need to convert date variables that are in string format to datetime format through strftime (string format time). The string format time table can be viewed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "dataset = pd.DataFrame({'created': ['2021-08-13 00:00:00', '2021-08-12 00:00:00', '2021-08-11 00:00:00', \n",
    "                                    '2021-08-10 00:00:00', '2021-08-09 00:00:00', '2021-08-08 00:00:00', '2021-08-07 00:00:00']})\n",
    "\n",
    "def parser(x):\n",
    "    # Để biết được định dạng strftime của một chuỗi kí tự ta phải tra trong bàng string format time\n",
    "    return datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "dataset['created'] = dataset['created'].map(lambda x: parser(x))\n",
    "print(dataset['created'].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the created variable has been converted to datetime format. We can create a one-hot encoding based on the weekday() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4\n",
       "1    3\n",
       "2    2\n",
       "3    1\n",
       "4    0\n",
       "5    6\n",
       "6    5\n",
       "Name: weekday, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['weekday'] = dataset['created'].apply(lambda x: x.date().weekday())\n",
    "dataset['weekday']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a variable that returns the status of whether a day is a weekend by checking whether weekday() falls in [5, 6], which is a weekend day or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "5    1\n",
       "6    1\n",
       "Name: is_weekend, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['is_weekend'] = dataset['created'].apply(lambda x: 1 if x.date().weekday() in [5, 6] else 0)\n",
    "dataset['is_weekend']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some problems the data may be time dependent. For example, a credit card repayment schedule will fall on the statement period on a specific day of the month. When working with time series data, we should pay attention to the list of special days of the year such as Lunar New Year holiday, National Day, International Labor Day, etc. Because these days will often have large fluctuations in data. business materials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data from website, blog, log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large website systems will track user sessions. Tracked information includes device information, event type, customer ID, etc. From the customer ID, they can link to the user database to know information about gender, age, account, and transaction behavior. ,... In some cases a customer can change the access device, so in most cases we cannot map the session with the Customer ID on local data. However, the information stored in Cookies about the user (also known as the user agent) also provides us with many things. For example: Access devices, browsers, operating systems, etc. From mobile devices, we can also estimate the user's income level: If you use IPhone High income people, using Xiaomi phones, are likely to be middle and low income people, etc. To classify information about users, we can use the <code>user_agents</code> package in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting user_agents\n",
      "  Downloading user_agents-2.2.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting ua-parser>=0.10.0 (from user_agents)\n",
      "  Downloading ua_parser-0.18.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Downloading user_agents-2.2.0-py3-none-any.whl (9.6 kB)\n",
      "Downloading ua_parser-0.18.0-py2.py3-none-any.whl (38 kB)\n",
      "Installing collected packages: ua-parser, user_agents\n",
      "Successfully installed ua-parser-0.18.0 user_agents-2.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install user_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is a bot?  False\n",
      "Is mobile?  False\n",
      "Is PC?  True\n",
      "OS Family:  Ubuntu\n",
      "OS Version:  ()\n",
      "Browser Family:  Chromium\n",
      "Browser Version:  (56, 0, 2924)\n"
     ]
    }
   ],
   "source": [
    "import user_agents\n",
    "# Assume there is a user agent as below\n",
    "ua = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/56.0.2924.76 Chrome/56.0.2924.76 Safari/537.36'\n",
    "# Parser user agent information\n",
    "ua = user_agents.parse(ua)\n",
    "# Exploit user attributes\n",
    "print('Is a bot? ', ua.is_bot)\n",
    "print('Is mobile? ', ua.is_mobile)\n",
    "print('Is PC? ',ua.is_pc)\n",
    "print('OS Family: ',ua.os.family)\n",
    "print('OS Version: ',ua.os.version)\n",
    "print('Browser Family: ',ua.browser.family)\n",
    "print('Browser Version: ',ua.browser.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
